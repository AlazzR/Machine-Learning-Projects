{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixtures of Factor Analyzers\n",
    "## Multivariate Normal Distribution\n",
    "### Joint Distribution\n",
    "### Conditional Distribution is Normal when Jointly Distributed RVs have a Normal distributed \n",
    "### Marginal Distribution is Normal when Jointly Distributed RVs have a Normal distributed \n",
    "## Factor Analysis Model\n",
    "### Assumption about Continuous Latent Variable \n",
    "### Incomplete Log-Likelihood\n",
    "### Jensen's Inequality and the Complete Log-Likelihood\n",
    "## Mixtures of Factor Analyzers\n",
    "### Assumption about the discrete and continuous latent Variable\n",
    "### Incomplete Log-Likelihood\n",
    "### Complete Log-Likelihood and Jensen's Inequality\n",
    "### Alteranting Expectation Conditional Maximization Algorithm (AECM)\n",
    "#### First E-step for discrete latent variable\n",
    "#### Maximization w.r.t Mixing paramter and mean and covariance matrix of the distribution in each cluster\n",
    "#### Second E-step for continuous latent variable\n",
    "#### Maximization w.r.t parameters of the marginal distribution \n",
    "#### Convergence Test\n",
    "#### Initialization Options\n",
    "### Predicting to which cluster an observation belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To have better initialization of the algorithm\n",
    "class kmeans(object):\n",
    "\n",
    "    def __init__(self, X_train, k):\n",
    "        self.K = k\n",
    "        self.m = X_train.shape[0]\n",
    "        self.n = X_train.shape[1]\n",
    "        self.X_train = X_train\n",
    "        choices = numpy.random.choice(np.arange(0, self.m), self.K, replace=False)\n",
    "        self.centers = [X_train[choices[i], :].reshape(-1, 1) for i in range(0, self.K)]# initalize the clusters centers to be one of the observations\n",
    "        self.clusters_assignments = np.zeros((self.m, 1))#Just to give it the necessary shape\n",
    "    \n",
    "    def reassign_clusters_centers(self):\n",
    "        for k in range(0, self.K):\n",
    "            temp = np.zeros((self.n, 1))\n",
    "            clusters = list(map(lambda i: True if i == k else False, self.clusters_assignments))\n",
    "            for i in range(0, self.m):\n",
    "                if clusters[i] == True:\n",
    "                    temp += clusters[i] * self.X_train[i, :].reshape(-1, 1)#clusters contained in {0, 1}\n",
    "                else:\n",
    "                    pass\n",
    "            #print(np.sum(clusters))\n",
    "            self.centers[k] = temp/np.sum(clusters)\n",
    "\n",
    "    def distortiuon_function(self):\n",
    "        temp = 0\n",
    "        for i in range(0, self.m):\n",
    "            for k in range(0, self.K):\n",
    "                if self.clusters_assignments[i] == k:\n",
    "                    temp += np.linalg.norm(self.X_train[i, :].reshape(-1, 1) - self.centers[k].reshape(-1, 1))**2\n",
    "                    break#They willn't be assigned to more than one cluster in tandem\n",
    "        return temp\n",
    "\n",
    "    def assign_to_clusters(self, x):\n",
    "        temp = []\n",
    "        for k in range(0, self.K):\n",
    "            temp.append(np.linalg.norm(x.reshape(-1, 1) - self.centers[k].reshape(-1, 1))**2)#We will use L2-norm for dissimilarity measure\n",
    "        return np.argmin(temp)#return the cluster number\n",
    "\n",
    "    def E_step(self):\n",
    "        for i in range(0, self.m):\n",
    "            self.clusters_assignments[i] = self.assign_to_clusters(X_train[i, :].reshape(-1, 1))\n",
    "    \n",
    "    def fit(self, max_iterations, eps=1e-5):\n",
    "        self.E_step()#To initialize the clusters assignments\n",
    "        past = 10\n",
    "        future = 0\n",
    "        count = 0\n",
    "        while(abs(past - future) > eps):#I will care for only lack of progress because k-means will always be able to minimize the distortion functions\n",
    "            print(f\"count:{count}, max_iterations{max_iterations}, past:{past}, future:{future}\")\n",
    "            count += 1\n",
    "            past = self.distortiuon_function()\n",
    "            self.reassign_clusters_centers()#The M step\n",
    "            self.E_step()\n",
    "            future = self.distortiuon_function()\n",
    "\n",
    "        return self.centers, self.clusters_assignments\n",
    "\n",
    "    def prediction_dataset(self, X):\n",
    "        predictions = []\n",
    "        for i in range(0, X.shape[0]):\n",
    "            predictions.append(self.assign_to_clusters(X[i, :].reshape(-1, 1)))\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, x):\n",
    "        return  self.assign_to_clusters(x.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixtures_Factor_Analyzers_Model(object):\n",
    "\n",
    "    def __init__(self, X_train, G, sizeOfFactorLoadingMatrix, initialization_type=\"random\", max_iterations=100):\n",
    "        if G != len(sizeOfFactorLoadingMatrix):\n",
    "            raise Exception(\"The number of Clusters doesn't match the number of factor loading matrix\")\n",
    "        \n",
    "        self.m = X_train.shape[0]\n",
    "        self.n = X_train.shape[1]\n",
    "        self.X_train = X_train\n",
    "        self.G = G#number of clusters\n",
    "        self.loadingSize = sizeOfFactorLoadingMatrix\n",
    "        #Just Temporary values\n",
    "        self.mixing_parameter = np.zeros((self.G, 1))\n",
    "        self.loading_matrices = []\n",
    "        self.uncertainity = []\n",
    "        self.soft_discrete_latent = np.zeros((self.m, self.G))\n",
    "        self.mean_within_clusters = []\n",
    "        self.covariance_within_clusters =[]\n",
    "        self.B_parameter_within_clusters = []       \n",
    "        \n",
    "        if initialization_type == \"random\":\n",
    "            for g in range(0, self.G):\n",
    "                self.mixing_parameter[g] = abs(np.random.randn())\n",
    "                self.loading_matrices.append((30*np.random.randn()+43) * np.dot(np.eye(self.n), np.ones((self.n, self.loadingSize[g])) ))#size of dxk\n",
    "                self.uncertainity.append((34*np.random.randn()+43) * np.eye(self.n))\n",
    "                a = np.random.randn(self.n, self.n)\n",
    "                self.covariance_within_clusters.append(np.dot(a, a.T))#Creating a random covariance matrix\n",
    "                a = np.random.randn(self.n).reshape(-1, 1)\n",
    "                self.mean_within_clusters.append(a)\n",
    "                self.B_parameter_within_clusters.append(self.computing_B(self.loading_matrices[g], self.uncertainity[g]))\n",
    "            \n",
    "            self.mixing_parameter = self.mixing_parameter/np.sum(self.mixing_parameter)#To insure that they sum to 1\n",
    "            \n",
    "            for i in range(0, self.m):\n",
    "                temp = abs(np.random.randn(self.G))\n",
    "                self.soft_discrete_latent[i, :] = temp/np.sum(temp)\n",
    "        \n",
    "        else:\n",
    "            model = kmeans(self.X_train, self.G)\n",
    "            centers, clusters  = model.fit(max_iterations=max_iterations)\n",
    "            print(\"Finished the initialization by the kmeans\")\n",
    "            for k in range(0, self.G):\n",
    "                cluster_separated = list(map(lambda i: True if i == k else False, clusters))\n",
    "\n",
    "                self.mixing_parameter[k] = np.sum(cluster_separated)/self.m\n",
    "                self.mean_within_clusters.append(centers[k].reshape(-1, 1))\n",
    "                #THe following is done to create a list for each component\n",
    "                self.loading_matrices.append((30*np.random.randn()+43) * np.dot(np.eye(self.n), np.ones((self.n, self.loadingSize[k])) ))#size of dxk\n",
    "                self.uncertainity.append((34*np.random.randn()+43) * np.eye(self.n))#Initial Value\n",
    "                self.B_parameter_within_clusters.append(self.computing_B(self.loading_matrices[k], self.uncertainity[k]))#Initial Value\n",
    "                a = np.random.randn(self.n, self.n)\n",
    "                self.covariance_within_clusters.append(np.dot(a, a.T))#Creating a random covariance matrix\n",
    "                \n",
    "                self.soft_discrete_latent[:, k] = np.array(list(cluster_separated))\n",
    "            \n",
    "            #for i in range(0, self.m):\n",
    "                #temp = abs(np.random.randn(self.G))\n",
    "                #self.soft_discrete_latent[i, :] = temp/np.sum(temp) \n",
    "                \n",
    "            #Calaculating the covariance matrix matrix within each cluster, loading factors and uncertainity matrix within each cluster\n",
    "            temp = np.zeros((self.n, self.n))\n",
    "            for g in range(0, self.G):\n",
    "                self.covariance_within_clusters[g] = self.computing_S(g)\n",
    "                evalue, evectors = np.linalg.eig(self.covariance_within_clusters[g])\n",
    "                self.loading_matrices[g] = np.multiply(np.sqrt(evalue[0:self.loadingSize[g]].reshape(1, -1)), evectors[:, 0:self.loadingSize[g]])\n",
    "                self.uncertainity[g] = abs(np.eye(self.n) *( self.covariance_within_clusters[g] - np.dot( np.sqrt(evalue.reshape(1, -1)) * evectors, (np.sqrt(evalue.reshape(1, -1)) * evectors)).T))\n",
    "                #print(self.uncertainity[g])\n",
    "                self.B_parameter_within_clusters[g] = self.computing_B(self.loading_matrices[g], self.uncertainity[g])#Initial Value\n",
    "\n",
    "            \n",
    "            self.computing_soft_latent()\n",
    "        \n",
    "\n",
    "    def computing_inv_cov_x(self, lambd, psi):\n",
    "        #Using Woodbury Identity\n",
    "        inv_psi = np.linalg.inv(psi)\n",
    "        t1 = np.linalg.inv( np.add(np.eye(lambd.shape[1]), np.dot(lambd.T, np.dot(inv_psi, lambd))) )\n",
    "        t2 = np.dot(lambd, np.dot(t1, lambd.T))\n",
    "        t3 = np.dot(inv_psi, np.dot(t2, inv_psi))\n",
    "        assert(t3.shape == inv_psi.shape)\n",
    "\n",
    "        return np.subtract(inv_psi, t3)\n",
    "    \n",
    "    def computing_B(self, lambd, psi):\n",
    "        return np.dot(lambd.T, self.computing_inv_cov_x(lambd, psi))\n",
    "    \n",
    "    def computing_S(self, g):\n",
    "        temp = np.zeros((self.n, self.n))\n",
    "        for i in range(0, self.m):\n",
    "            temp += self.soft_discrete_latent[i, g] * np.dot(self.X_train[i, :].reshape(-1, 1) - self.mean_within_clusters[g].reshape(-1, 1), (self.X_train[i, :].reshape(-1, 1) - self.mean_within_clusters[g].reshape(-1, 1)).T)\n",
    "        return temp/np.sum(self.soft_discrete_latent[:, g])\n",
    "    \n",
    "    def computing_mean(self, g):\n",
    "        return np.sum(np.multiply(self.soft_discrete_latent[:, g].reshape(-1, 1), self.X_train), axis=0).reshape(-1, 1)/np.sum(self.soft_discrete_latent[:, g])\n",
    "    \n",
    "    def computing_lambda(self, g):\n",
    "        theta = np.add(np.eye(self.loadingSize[g]), \n",
    "        np.subtract(np.dot(self.B_parameter_within_clusters[g], np.dot(self.covariance_within_clusters[g],self.B_parameter_within_clusters[g].T)) , \n",
    "        np.dot(self.B_parameter_within_clusters[g], self.loading_matrices[g]) ) )\n",
    "        return np.dot(self.covariance_within_clusters[g], np.dot(self.B_parameter_within_clusters[g].T, np.linalg.inv(theta) ))\n",
    "\n",
    "    def computing_psi(self, g):\n",
    "        temp = np.dot(self.loading_matrices[g], np.dot(self.B_parameter_within_clusters[g], self.covariance_within_clusters[g]))\n",
    "        return np.subtract(self.covariance_within_clusters[g], temp)\n",
    "    \n",
    "    def computing_multivariate(self, g, x):\n",
    "        #print(self.mean_within_clusters[g].reshape(1, -1).shape)\n",
    "        #print(x.shape)\n",
    "        return scipy.stats.multivariate_normal.pdf(x.ravel(), mean=self.mean_within_clusters[g].ravel(), cov=np.dot(self.loading_matrices[g], self.loading_matrices[g].T) + self.uncertainity[g])\n",
    "\n",
    "    def computing_soft_latent(self):\n",
    "        temp = np.zeros((self.m, self.G))\n",
    "        for i in range(0, self.m):\n",
    "            denominator = 0 \n",
    "            for k in range(0, self.G):\n",
    "                denominator = denominator + ( self.computing_multivariate(k, self.X_train[i, :].reshape(-1, 1)) * self.mixing_parameter[k])\n",
    "            for g in range(0, self.G):\n",
    "                #print(denominator)\n",
    "                #1e-50 were added to preven division by zero when computing the log-likelihood\n",
    "                temp[i, g] = (self.computing_multivariate(g, self.X_train[i, :].reshape(-1, 1)) * self.mixing_parameter[g])/denominator\n",
    "                #print(temp[i, g])\n",
    "        self.soft_discrete_latent = temp\n",
    "    \n",
    "    def E_step(self, count=2):\n",
    "        for g in range(0, self.G):\n",
    "            self.mean_within_clusters[g] = self.computing_mean(g)\n",
    "            self.mixing_parameter[g] = (1/self.m) * np.sum(self.soft_discrete_latent[:, g])\n",
    "        \n",
    "        if count != 0:\n",
    "            self.computing_soft_latent()\n",
    "        \n",
    "        for g in range(0, self.G):\n",
    "            self.covariance_within_clusters[g] = self.computing_S(g)\n",
    "            self.B_parameter_within_clusters[g] = self.computing_B(self.loading_matrices[g], self.uncertainity[g])\n",
    "\n",
    "    def M_step(self):\n",
    "        for g in range(0, self.G):    \n",
    "            self.loading_matrices[g] = self.computing_lambda(g)\n",
    "            self.uncertainity[g] = np.eye(self.n) * self.computing_psi(g)\n",
    "        \n",
    "        self.computing_soft_latent()\n",
    "\n",
    "    def compute_log_likelihood(self):\n",
    "        temp = 0\n",
    "        for i in range(0, self.m):\n",
    "            temp2 = 0\n",
    "            for g in range(0, self.G):\n",
    "                temp2 += self.mixing_parameter[g] * self.computing_multivariate(g, self.X_train[i, :])\n",
    "            #if temp2 <=0:\n",
    "                #print(temp2)\n",
    "            #assert(temp2 > 0)\n",
    "            #print(temp2)\n",
    "            temp += np.log(temp2)\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def fit(self, max_iteration, eps=1e-2):\n",
    "        convergence_test = True\n",
    "        count = 0\n",
    "        while( (convergence_test == True) and (count != max_iteration)):\n",
    "            log_likelihood_t = self.compute_log_likelihood()            \n",
    "            self.E_step(count)#Update the soft latent values\n",
    "            temp_psi, temp_lambda = (self.uncertainity.copy(), self.loading_matrices.copy())            \n",
    "            self.M_step()#Update the parameters of the conditional distribution of x given z and u\n",
    "            self.computing_soft_latent()#Updating the soft values for cluster assignment\n",
    "            log_likelihood_t_future = self.compute_log_likelihood()\n",
    "            print(f\"Number of iteration:{count}, max_iteration:{max_iteration}, past:{log_likelihood_t}, future:{log_likelihood_t_future}\")\n",
    "            count = count + 1\n",
    "            if log_likelihood_t_future != log_likelihood_t_future:#The usual trick nan doesn't equal itself\n",
    "                self.uncertainity, self.loading_matrices = (temp_psi, temp_lambda)\n",
    "                print(\"Something wrong happened in the Maximization step\")\n",
    "                break\n",
    "            #print(log_likelihood_t_future[0])\n",
    "            if( (log_likelihood_t_future - log_likelihood_t) < eps and (count > 10)):\n",
    "                self.uncertainity, self.loading_matrices = (temp_psi, temp_lambda)\n",
    "                print(\"We converged to the optimal value for the log-likelihood\")\n",
    "                convergence_test =False #We reached the parameters that maximize the log-likelihood, no adancement in the log-likelihood\n",
    "            #[print(self.loading_matrices[g].shape) for g in range(0, self.G)]\n",
    "        \n",
    "        return self.uncertainity, self.loading_matrices, self.mixing_parameter\n",
    "\n",
    "    def prediction_dataset(self, X):\n",
    "        prediciton = []\n",
    "        for i in range(0, X.shape[0]):\n",
    "            prediciton.append(self.predict(X[i, :]))\n",
    "            \n",
    "        return np.array(prediciton)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        prediction = np.zeros((self.G, 1))\n",
    "        for g in range(0, self.G):\n",
    "            denominator = 0 \n",
    "            for k in range(0, self.G):\n",
    "                denominator = denominator + ( self.computing_multivariate(k, x.reshape(-1, 1)) * self.mixing_parameter[k])\n",
    "            prediction[g] = (self.computing_multivariate(g, x.reshape(-1, 1)) * self.mixing_parameter[g])/denominator\n",
    "        \n",
    "        return np.argmax(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(133, 14)\n(45, 14)\n3\n"
    }
   ],
   "source": [
    "numpy.random.seed(120)\n",
    "\n",
    "#Using IRIS and Wine Dataset\n",
    "#X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "X, y = sklearn.datasets.load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "#standard = sklearn.preprocessing.StandardScaler()\n",
    "#X_train = standard.fit_transform(X_train)\n",
    "training_data = np.c_[X_train, y_train]#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "\n",
    "#X_test = standard.transform(X_test)\n",
    "test_data = np.c_[X_test, y_test]\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)\n",
    "k = len(set(y_train))\n",
    "y_train#It needs to be labeled from 0 to k\n",
    "print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "count:0, max_iterations100, past:10, future:0\ncount:1, max_iterations100, past:10792010.491995996, future:3102217.0489981584\ncount:2, max_iterations100, past:3102217.0489981584, future:1978365.5976835634\ncount:3, max_iterations100, past:1978365.5976835634, future:1795178.7694889628\ncount:4, max_iterations100, past:1795178.7694889628, future:1702081.6684963438\ncount:5, max_iterations100, past:1702081.6684963438, future:1633691.812665524\ncount:6, max_iterations100, past:1633691.812665524, future:1616124.000250265\ncount:7, max_iterations100, past:1616124.000250265, future:1615357.3439364806\nFinished the initialization by the kmeans\nNumber of iteration:0, max_iteration:1000, past:[-2669.05191283], future:[-2300.75165462]\nNumber of iteration:1, max_iteration:1000, past:[-2300.75165462], future:[-2239.35288695]\nNumber of iteration:2, max_iteration:1000, past:[-2239.35288695], future:[-2209.44990541]\nNumber of iteration:3, max_iteration:1000, past:[-2209.44990541], future:[-2192.6340433]\nNumber of iteration:4, max_iteration:1000, past:[-2192.6340433], future:[-2180.65840935]\nNumber of iteration:5, max_iteration:1000, past:[-2180.65840935], future:[-2170.57139996]\nNumber of iteration:6, max_iteration:1000, past:[-2170.57139996], future:[-2162.96657974]\nNumber of iteration:7, max_iteration:1000, past:[-2162.96657974], future:[-2157.64503372]\nNumber of iteration:8, max_iteration:1000, past:[-2157.64503372], future:[-2153.52319084]\nNumber of iteration:9, max_iteration:1000, past:[-2153.52319084], future:[-2149.76835539]\nNumber of iteration:10, max_iteration:1000, past:[-2149.76835539], future:[-2146.99088838]\nNumber of iteration:11, max_iteration:1000, past:[-2146.99088838], future:[-2144.52064439]\nNumber of iteration:12, max_iteration:1000, past:[-2144.52064439], future:[-2142.09736903]\nNumber of iteration:13, max_iteration:1000, past:[-2142.09736903], future:[-2139.82324177]\nNumber of iteration:14, max_iteration:1000, past:[-2139.82324177], future:[-2137.72040558]\nNumber of iteration:15, max_iteration:1000, past:[-2137.72040558], future:[-2134.64354681]\nNumber of iteration:16, max_iteration:1000, past:[-2134.64354681], future:[-2131.10958006]\nNumber of iteration:17, max_iteration:1000, past:[-2131.10958006], future:[-2129.24438141]\nNumber of iteration:18, max_iteration:1000, past:[-2129.24438141], future:[-2127.92680517]\nNumber of iteration:19, max_iteration:1000, past:[-2127.92680517], future:[-2126.860058]\nNumber of iteration:20, max_iteration:1000, past:[-2126.860058], future:[-2125.85459986]\nNumber of iteration:21, max_iteration:1000, past:[-2125.85459986], future:[-2124.57375106]\nNumber of iteration:22, max_iteration:1000, past:[-2124.57375106], future:[-2123.090615]\nNumber of iteration:23, max_iteration:1000, past:[-2123.090615], future:[-2122.06468338]\nNumber of iteration:24, max_iteration:1000, past:[-2122.06468338], future:[-2121.27771709]\nNumber of iteration:25, max_iteration:1000, past:[-2121.27771709], future:[-2120.62691591]\nNumber of iteration:26, max_iteration:1000, past:[-2120.62691591], future:[-2120.06605341]\nNumber of iteration:27, max_iteration:1000, past:[-2120.06605341], future:[-2119.52918161]\nNumber of iteration:28, max_iteration:1000, past:[-2119.52918161], future:[-2118.77585707]\nNumber of iteration:29, max_iteration:1000, past:[-2118.77585707], future:[-2117.34262495]\nNumber of iteration:30, max_iteration:1000, past:[-2117.34262495], future:[-2116.32547836]\nNumber of iteration:31, max_iteration:1000, past:[-2116.32547836], future:[-2115.82655793]\nNumber of iteration:32, max_iteration:1000, past:[-2115.82655793], future:[-2115.5019871]\nNumber of iteration:33, max_iteration:1000, past:[-2115.5019871], future:[-2115.26011509]\nNumber of iteration:34, max_iteration:1000, past:[-2115.26011509], future:[-2115.06641645]\nNumber of iteration:35, max_iteration:1000, past:[-2115.06641645], future:[-2114.9047612]\nNumber of iteration:36, max_iteration:1000, past:[-2114.9047612], future:[-2114.76632385]\nNumber of iteration:37, max_iteration:1000, past:[-2114.76632385], future:[-2114.64565433]\nNumber of iteration:38, max_iteration:1000, past:[-2114.64565433], future:[-2114.53907057]\nNumber of iteration:39, max_iteration:1000, past:[-2114.53907057], future:[-2114.44392054]\nNumber of iteration:40, max_iteration:1000, past:[-2114.44392054], future:[-2114.3582083]\nNumber of iteration:41, max_iteration:1000, past:[-2114.3582083], future:[-2114.2803867]\nNumber of iteration:42, max_iteration:1000, past:[-2114.2803867], future:[-2114.20923255]\nNumber of iteration:43, max_iteration:1000, past:[-2114.20923255], future:[-2114.14376575]\nNumber of iteration:44, max_iteration:1000, past:[-2114.14376575], future:[-2114.08319345]\nNumber of iteration:45, max_iteration:1000, past:[-2114.08319345], future:[-2114.02686957]\nNumber of iteration:46, max_iteration:1000, past:[-2114.02686957], future:[-2113.97426441]\nNumber of iteration:47, max_iteration:1000, past:[-2113.97426441], future:[-2113.92494128]\nNumber of iteration:48, max_iteration:1000, past:[-2113.92494128], future:[-2113.87853819]\nNumber of iteration:49, max_iteration:1000, past:[-2113.87853819], future:[-2113.83475339]\nNumber of iteration:50, max_iteration:1000, past:[-2113.83475339], future:[-2113.79333384]\nNumber of iteration:51, max_iteration:1000, past:[-2113.79333384], future:[-2113.75406604]\nNumber of iteration:52, max_iteration:1000, past:[-2113.75406604], future:[-2113.71676862]\nNumber of iteration:53, max_iteration:1000, past:[-2113.71676862], future:[-2113.68128633]\nNumber of iteration:54, max_iteration:1000, past:[-2113.68128633], future:[-2113.64748528]\nNumber of iteration:55, max_iteration:1000, past:[-2113.64748528], future:[-2113.61524897]\nNumber of iteration:56, max_iteration:1000, past:[-2113.61524897], future:[-2113.58447512]\nNumber of iteration:57, max_iteration:1000, past:[-2113.58447512], future:[-2113.55507305]\nNumber of iteration:58, max_iteration:1000, past:[-2113.55507305], future:[-2113.52696157]\nNumber of iteration:59, max_iteration:1000, past:[-2113.52696157], future:[-2113.50006729]\nNumber of iteration:60, max_iteration:1000, past:[-2113.50006729], future:[-2113.47432317]\nNumber of iteration:61, max_iteration:1000, past:[-2113.47432317], future:[-2113.44966751]\nNumber of iteration:62, max_iteration:1000, past:[-2113.44966751], future:[-2113.42604299]\nNumber of iteration:63, max_iteration:1000, past:[-2113.42604299], future:[-2113.40339604]\nNumber of iteration:64, max_iteration:1000, past:[-2113.40339604], future:[-2113.38167631]\nNumber of iteration:65, max_iteration:1000, past:[-2113.38167631], future:[-2113.36083625]\nNumber of iteration:66, max_iteration:1000, past:[-2113.36083625], future:[-2113.34083084]\nNumber of iteration:67, max_iteration:1000, past:[-2113.34083084], future:[-2113.32161737]\nNumber of iteration:68, max_iteration:1000, past:[-2113.32161737], future:[-2113.30315528]\nNumber of iteration:69, max_iteration:1000, past:[-2113.30315528], future:[-2113.28540603]\nNumber of iteration:70, max_iteration:1000, past:[-2113.28540603], future:[-2113.268333]\nNumber of iteration:71, max_iteration:1000, past:[-2113.268333], future:[-2113.25190145]\nNumber of iteration:72, max_iteration:1000, past:[-2113.25190145], future:[-2113.23607843]\nNumber of iteration:73, max_iteration:1000, past:[-2113.23607843], future:[-2113.22083273]\nNumber of iteration:74, max_iteration:1000, past:[-2113.22083273], future:[-2113.20613482]\nNumber of iteration:75, max_iteration:1000, past:[-2113.20613482], future:[-2113.19195678]\nNumber of iteration:76, max_iteration:1000, past:[-2113.19195678], future:[-2113.17827225]\nNumber of iteration:77, max_iteration:1000, past:[-2113.17827225], future:[-2113.16505637]\nNumber of iteration:78, max_iteration:1000, past:[-2113.16505637], future:[-2113.1522857]\nNumber of iteration:79, max_iteration:1000, past:[-2113.1522857], future:[-2113.13993814]\nNumber of iteration:80, max_iteration:1000, past:[-2113.13993814], future:[-2113.12799291]\nNumber of iteration:81, max_iteration:1000, past:[-2113.12799291], future:[-2113.11643042]\nNumber of iteration:82, max_iteration:1000, past:[-2113.11643042], future:[-2113.10523227]\nNumber of iteration:83, max_iteration:1000, past:[-2113.10523227], future:[-2113.09438109]\nNumber of iteration:84, max_iteration:1000, past:[-2113.09438109], future:[-2113.08386057]\nNumber of iteration:85, max_iteration:1000, past:[-2113.08386057], future:[-2113.07365536]\nNumber of iteration:86, max_iteration:1000, past:[-2113.07365536], future:[-2113.06375097]\nWe converged to the optimal value for the log-likelihood\n"
    }
   ],
   "source": [
    "#Randomly Initialized\n",
    "#Best resulted are obtained by initializing the model by kmeans\n",
    "#u = [3, 3, 3] #for iris\n",
    "u = [9, 9, 9] #Lets try projecting it to a lower space\n",
    "model = Mixtures_Factor_Analyzers_Model(X_train, k, u, \"kmeans\")\n",
    "uncertainity, loading_matrices, mixing_parameter = model.fit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the training set\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[44,  0,  0],\n       [ 2, 45,  6],\n       [ 0,  6, 30]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 194
    }
   ],
   "source": [
    "pred = model.prediction_dataset(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "#print(sklearn.metrics.confusion_matrix(y_train, pred))\n",
    "c = sklearn.metrics.confusion_matrix(y_train, pred)\n",
    "#c = c[:, list(np.argmax(c, axis=1))]#ordering the cluster to where it shows the highest number of matching with the true labels\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the test set\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[15,  0,  0],\n       [ 2, 15,  1],\n       [ 0,  1, 11]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 195
    }
   ],
   "source": [
    "pred = model.prediction_dataset(X_test)\n",
    "print(\"Performance on the test set\")\n",
    "#print(sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "c = sklearn.metrics.confusion_matrix(y_test, pred)\n",
    "#c = c[:, list(np.argmax(c, axis=1))]#ordering the cluster to where it shows the highest number of matching with the true labels\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3\ncount:0, max_iterations100, past:10, future:0\ncount:1, max_iterations100, past:134.96000000000004, future:96.15928353392638\ncount:2, max_iterations100, past:96.15928353392638, future:61.83372617547311\ncount:3, max_iterations100, past:61.83372617547311, future:60.34434195402301\ncount:4, max_iterations100, past:60.34434195402301, future:60.19166954022988\nFinished the initialization by the kmeans\nNumber of iteration:0, max_iteration:1000, past:[-209.17174333], future:[-170.74044787]\nNumber of iteration:1, max_iteration:1000, past:[-170.74044787], future:[-159.17591952]\nNumber of iteration:2, max_iteration:1000, past:[-159.17591952], future:[-151.34666362]\nNumber of iteration:3, max_iteration:1000, past:[-151.34666362], future:[-146.05020933]\nNumber of iteration:4, max_iteration:1000, past:[-146.05020933], future:[-142.20354696]\nNumber of iteration:5, max_iteration:1000, past:[-142.20354696], future:[-139.71990125]\nNumber of iteration:6, max_iteration:1000, past:[-139.71990125], future:[-138.17345807]\nNumber of iteration:7, max_iteration:1000, past:[-138.17345807], future:[-136.98907976]\nNumber of iteration:8, max_iteration:1000, past:[-136.98907976], future:[-135.95654601]\nNumber of iteration:9, max_iteration:1000, past:[-135.95654601], future:[-135.06414916]\nNumber of iteration:10, max_iteration:1000, past:[-135.06414916], future:[-134.27707103]\nNumber of iteration:11, max_iteration:1000, past:[-134.27707103], future:[-133.53102002]\nNumber of iteration:12, max_iteration:1000, past:[-133.53102002], future:[-132.79168564]\nNumber of iteration:13, max_iteration:1000, past:[-132.79168564], future:[-132.07890511]\nNumber of iteration:14, max_iteration:1000, past:[-132.07890511], future:[-131.42774019]\nNumber of iteration:15, max_iteration:1000, past:[-131.42774019], future:[-130.84210616]\nNumber of iteration:16, max_iteration:1000, past:[-130.84210616], future:[-130.31522884]\nNumber of iteration:17, max_iteration:1000, past:[-130.31522884], future:[-129.85571587]\nNumber of iteration:18, max_iteration:1000, past:[-129.85571587], future:[-129.4795637]\nNumber of iteration:19, max_iteration:1000, past:[-129.4795637], future:[-129.18941674]\nNumber of iteration:20, max_iteration:1000, past:[-129.18941674], future:[-128.97125391]\nNumber of iteration:21, max_iteration:1000, past:[-128.97125391], future:[-128.80606294]\nNumber of iteration:22, max_iteration:1000, past:[-128.80606294], future:[-128.67822688]\nNumber of iteration:23, max_iteration:1000, past:[-128.67822688], future:[-128.57694999]\nNumber of iteration:24, max_iteration:1000, past:[-128.57694999], future:[-128.49503668]\nNumber of iteration:25, max_iteration:1000, past:[-128.49503668], future:[-128.42759749]\nNumber of iteration:26, max_iteration:1000, past:[-128.42759749], future:[-128.37119939]\nNumber of iteration:27, max_iteration:1000, past:[-128.37119939], future:[-128.32335986]\nNumber of iteration:28, max_iteration:1000, past:[-128.32335986], future:[-128.28224208]\nNumber of iteration:29, max_iteration:1000, past:[-128.28224208], future:[-128.24646309]\nNumber of iteration:30, max_iteration:1000, past:[-128.24646309], future:[-128.21496751]\nNumber of iteration:31, max_iteration:1000, past:[-128.21496751], future:[-128.18694136]\nNumber of iteration:32, max_iteration:1000, past:[-128.18694136], future:[-128.16175139]\nNumber of iteration:33, max_iteration:1000, past:[-128.16175139], future:[-128.13890153]\nNumber of iteration:34, max_iteration:1000, past:[-128.13890153], future:[-128.11800084]\nNumber of iteration:35, max_iteration:1000, past:[-128.11800084], future:[-128.09873966]\nNumber of iteration:36, max_iteration:1000, past:[-128.09873966], future:[-128.08087156]\nNumber of iteration:37, max_iteration:1000, past:[-128.08087156], future:[-128.06419953]\nNumber of iteration:38, max_iteration:1000, past:[-128.06419953], future:[-128.04856539]\nNumber of iteration:39, max_iteration:1000, past:[-128.04856539], future:[-128.03384149]\nNumber of iteration:40, max_iteration:1000, past:[-128.03384149], future:[-128.01992425]\nNumber of iteration:41, max_iteration:1000, past:[-128.01992425], future:[-128.00672914]\nNumber of iteration:42, max_iteration:1000, past:[-128.00672914], future:[-127.99418664]\nNumber of iteration:43, max_iteration:1000, past:[-127.99418664], future:[-127.98223913]\nNumber of iteration:44, max_iteration:1000, past:[-127.98223913], future:[-127.97083837]\nNumber of iteration:45, max_iteration:1000, past:[-127.97083837], future:[-127.9599436]\nNumber of iteration:46, max_iteration:1000, past:[-127.9599436], future:[-127.94951992]\nNumber of iteration:47, max_iteration:1000, past:[-127.94951992], future:[-127.93953713]\nWe converged to the optimal value for the log-likelihood\nPerformance on the training set\n[[35  0  0]\n [ 0 36  3]\n [ 0  0 38]]\nPerformance on the test set\n[[15  0  0]\n [ 0 10  1]\n [ 0  0 12]]\n"
    }
   ],
   "source": [
    "numpy.random.seed(120)\n",
    "\n",
    "#Using IRIS  \n",
    "X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "k = len(set(y_train))\n",
    "y_train#It needs to be labeled from 0 to k\n",
    "print(k)\n",
    "#Best resulted are obtained by initializing the model by kmeans\n",
    "u = [2, 2, 2] #Lets try projecting it to a lower space\n",
    "model = Mixtures_Factor_Analyzers_Model(X_train, k, u, \"kmeans\")\n",
    "uncertainity, loading_matrices, mixing_parameter = model.fit(1000)\n",
    "pred = model.prediction_dataset(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "#print(sklearn.metrics.confusion_matrix(y_train, pred))\n",
    "c = sklearn.metrics.confusion_matrix(y_train, pred)\n",
    "c = c[:, list(np.argmax(c, axis=1))]#ordering the cluster to where it shows the highest number of matching with the true labels\n",
    "print(c)\n",
    "pred = model.prediction_dataset(X_test)\n",
    "print(\"Performance on the test set\")\n",
    "#print(sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "c = sklearn.metrics.confusion_matrix(y_test, pred)\n",
    "c = c[:, list(np.argmax(c, axis=1))]#ordering the cluster to where it shows the highest number of matching with the true labels\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4.55572931513341e-67"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "scipy.stats.multivariate_normal.pdf(np.array([10, 10, 10]), mean=[0, 0, 0], cov=np.eye(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 2, Chapter 9 and Chapter 12 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 13: (https://www.youtube.com/watch?v=LBtuYU-HfUg)\n",
    "* Andrew Ng, Lec 14: (https://www.youtube.com/watch?v=ey2PE5xi9-A)\n",
    "* Chapter 3 from McNicholas, P.D. (2016). Mixture Model-Based Classification. Boca Raton: Chapman &\n",
    "Hall/CRC Press.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}