{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtures of Gaussians\n",
    "### Assumptions about the Dustribution of the Latent Variable\n",
    "### Jensen's Inequality\n",
    "### Log-likelihood for the Incomplete and Complete Data\n",
    "### Expectation Step\n",
    "### Maximization Step\n",
    "#### * With Respect to the Mean of the Distribution within Each Cluster\n",
    "#### * With Respect to the Covariance Matrix of the Distribution within Each Cluster\n",
    "#### * With Respect to the parameters of the Latent Variable\n",
    "### Convergence Test\n",
    "### Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(112, 5)\n(38, 5)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 0, 2, 1, 1, 0, 0, 1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1,\n       2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0,\n       0, 1, 1, 0, 2, 0, 0, 1, 1, 2, 1, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 1,\n       2, 0, 2, 2, 0, 1, 1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1,\n       2, 2, 0, 1, 2, 2, 0, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 0,\n       1, 2])"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "#X, y = sklearn.datasets.load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "X_train = standard.fit_transform(X_train)\n",
    "training_data = np.c_[X_train, y_train]#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "\n",
    "X_test = standard.transform(X_test)\n",
    "test_data = np.c_[X_test, y_test]\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)\n",
    "k = len(set(y_train))\n",
    "y_train#It needs to be labeled from 0 to k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To have better initialization of the algorithm\n",
    "class kmeans(object):\n",
    "\n",
    "    def __init__(self, X_train, k):\n",
    "        self.K = k\n",
    "        self.m = X_train.shape[0]\n",
    "        self.n = X_train.shape[1]\n",
    "        self.X_train = X_train\n",
    "        choices = numpy.random.choice(np.arange(0, self.m), self.K, replace=False)\n",
    "        self.centers = [X_train[choices[i], :].reshape(-1, 1) for i in range(0, self.K)]# initalize the clusters centers to be one of the observations\n",
    "        self.clusters_assignments = np.zeros((self.m, 1))#Just to give it the necessary shape\n",
    "    \n",
    "    def reassign_clusters_centers(self):\n",
    "        for k in range(0, self.K):\n",
    "            temp = np.zeros((self.n, 1))\n",
    "            clusters = list(map(lambda i: True if i == k else False, self.clusters_assignments))\n",
    "            for i in range(0, self.m):\n",
    "                if clusters[i] == True:\n",
    "                    temp += clusters[i] * self.X_train[i, :].reshape(-1, 1)#clusters contained in {0, 1}\n",
    "                else:\n",
    "                    pass\n",
    "            #print(np.sum(clusters))\n",
    "            self.centers[k] = temp/np.sum(clusters)\n",
    "\n",
    "    def distortiuon_function(self):\n",
    "        temp = 0\n",
    "        for i in range(0, self.m):\n",
    "            for k in range(0, self.K):\n",
    "                if self.clusters_assignments[i] == k:\n",
    "                    temp += np.linalg.norm(self.X_train[i, :].reshape(-1, 1) - self.centers[k].reshape(-1, 1))**2\n",
    "                    break#They willn't be assigned to more than one cluster in tandem\n",
    "        return temp\n",
    "\n",
    "    def assign_to_clusters(self, x):\n",
    "        temp = []\n",
    "        for k in range(0, self.K):\n",
    "            temp.append(np.linalg.norm(x.reshape(-1, 1) - self.centers[k].reshape(-1, 1))**2)#We will use L2-norm for dissimilarity measure\n",
    "        return np.argmin(temp)#return the cluster number\n",
    "\n",
    "    def E_step(self):\n",
    "        for i in range(0, self.m):\n",
    "            self.clusters_assignments[i] = self.assign_to_clusters(X_train[i, :].reshape(-1, 1))\n",
    "    \n",
    "    def fit(self, max_iterations, eps=1e-5):\n",
    "        self.E_step()#To initialize the clusters assignments\n",
    "        past = 10\n",
    "        future = 0\n",
    "        count = 0\n",
    "        while(abs(past - future) > eps):#I will care for only lack of progress because k-means will always be able to minimize the distortion functions\n",
    "            print(f\"count:{count}, max_iterations{max_iterations}, past:{past}, future:{future}\")\n",
    "            count += 1\n",
    "            past = self.distortiuon_function()\n",
    "            self.reassign_clusters_centers()#The M step\n",
    "            self.E_step()\n",
    "            future = self.distortiuon_function()\n",
    "\n",
    "        return self.centers, self.clusters_assignments\n",
    "\n",
    "    def prediction_dataset(self, X):\n",
    "        predictions = []\n",
    "        for i in range(0, X.shape[0]):\n",
    "            predictions.append(self.assign_to_clusters(X[i, :].reshape(-1, 1)))\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, x):\n",
    "        return  self.assign_to_clusters(x.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureofGaussian(object):\n",
    "\n",
    "    def __init__(self, X_train, k, randomly=\"random\", max_iteration =1000):\n",
    "        self.m = X_train.shape[0]\n",
    "        self.n = X_train.shape[1]\n",
    "        self.K = k\n",
    "        self.means_of_each_clusters = list(map(lambda i: np.zeros((self.n, 1)), np.arange(0, self.K)))\n",
    "        self.covariance_within_each_clusters = list(map(lambda i: np.zeros((self.n, self.n)), np.arange(0, self.K))) \n",
    "        self.parameters_of_hard_latent = [i for i in np.zeros((self.K, 1))]\n",
    "        self.parameters_of_soft_latent =np.zeros((self.m, self.K))#Posterior\n",
    "        self.X_train = X_train\n",
    "        if randomly == \"random\":\n",
    "            self.initialize_parameters_randomly()\n",
    "        else:#This is if we want to initilaize it with k-means\n",
    "            model = kmeans(self.X_train, self.K)\n",
    "            centers, clusters  = model.fit(max_iterations=max_iteration)\n",
    "            print(\"Finished the initialzation by the kmeans\")\n",
    "            for k in range(0, self.K):\n",
    "                cluster_separated = list(map(lambda i: True if i == k else False, clusters))\n",
    "                self.parameters_of_hard_latent[k] = np.sum(cluster_separated)/self.m\n",
    "                self.means_of_each_clusters[k] = (centers[k]).reshape(-1, 1)\n",
    "                #Calaculating the covariance matrix matrix within each cluster\n",
    "                temp = np.zeros((self.n, self.n))\n",
    "                for i in range(0, self.m):\n",
    "                    if cluster_separated[i] == True:\n",
    "                        temp += np.dot((self.X_train[i, :].reshape(-1, 1) - self.means_of_each_clusters[k].reshape(-1, 1)).reshape(-1, 1), (self.X_train[i, :].reshape(-1, 1) - self.means_of_each_clusters[k].reshape(-1, 1)).reshape(1, -1))\n",
    "                \n",
    "                self.covariance_within_each_clusters[k] = (1/np.sum(clusters)) * temp\n",
    "                #print(self.covariance_within_each_clusters[k])\n",
    "\n",
    "        \n",
    "    def initialize_parameters_randomly(self):\n",
    "        for k in range(0, self.K):\n",
    "            self.means_of_each_clusters[k] = (numpy.random.randn(self.n)).reshape(-1, 1)\n",
    "            self.covariance_within_each_clusters[k] = 25 * np.eye(self.n)\n",
    "            self.parameters_of_hard_latent[k] = abs(numpy.random.randn())\n",
    "\n",
    "        sums = np.sum(self.parameters_of_hard_latent)\n",
    "        self.parameters_of_hard_latent = self.parameters_of_hard_latent/sums#To ensure that the parameters of the Multinomial distribution sums to 1\n",
    "\n",
    "    def gaussian_distribution(self, chosen_class, x):\n",
    "        t1 = np.divide(1, np.linalg.det(2* np.pi * self.covariance_within_each_clusters[chosen_class]))\n",
    "        t2 = np.dot((x - self.means_of_each_clusters[chosen_class]).T, np.dot(np.linalg.pinv(self.covariance_within_each_clusters[chosen_class]), (x - self.means_of_each_clusters[chosen_class])))\n",
    "        return t1 * np.exp((-1/2) * t2)\n",
    "\n",
    "    def compute_soft_latent(self, chosen_class, x):\n",
    "        denominator = 0 \n",
    "        for k in range(0, self.K):\n",
    "            denominator = denominator + ( self.gaussian_distribution(k, x) * self.parameters_of_hard_latent[k])\n",
    "        #1e-50 were added to preven division by zero when computing the log-likelihood\n",
    "        return (self.gaussian_distribution(chosen_class, x) * self.parameters_of_hard_latent[chosen_class] + 1e-50)/denominator\n",
    "    \n",
    "    def E_step(self):\n",
    "        temp = np.zeros((1, self.K))\n",
    "        for i in range(0, self.m):\n",
    "            for k in range(0, self.K):\n",
    "                #print(self.compute_soft_latent(k, self.X_train[i, :].reshape(-1, 1)).shape)\n",
    "                temp[0, k] = self.compute_soft_latent(k, self.X_train[i, :].reshape(-1, 1)) \n",
    "            self.parameters_of_soft_latent[i, :] = temp[0, :]\n",
    "    \n",
    "    def M_step(self):\n",
    "        for k in range(0, self.K):\n",
    "            means_temp = np.zeros((self.n, 1))\n",
    "            covariance_temp = np.zeros((self.n, self.n))\n",
    "\n",
    "            for i in range(0, self.m):\n",
    "                means_temp += (self.parameters_of_soft_latent[i, k] * self.X_train[i, :]).reshape(-1, 1)\n",
    "                covariance_temp += (self.parameters_of_soft_latent[i, k] * np.multiply( (self.X_train[i, :].reshape(-1, 1) - self.means_of_each_clusters[k]), (self.X_train[i, :].reshape(-1, 1) - self.means_of_each_clusters[k]).T ) )\n",
    "\n",
    "            self.means_of_each_clusters[k] = means_temp/np.sum(self.parameters_of_soft_latent[:, k])\n",
    "            self.covariance_within_each_clusters[k] = covariance_temp/np.sum(self.parameters_of_soft_latent[:, k])\n",
    "            self.parameters_of_hard_latent[k] = np.sum(self.parameters_of_soft_latent[:, k])/self.m\n",
    "    \n",
    "    def computing_log_likelihood(self):\n",
    "        log_likelihood = 0\n",
    "        for i in range(0, self.m):\n",
    "            for k in range(0, self.K):\n",
    "                #print(self.parameters_of_soft_latent)\n",
    "                assert(self.parameters_of_soft_latent[i, k] > 0 )\n",
    "                t1 = self.gaussian_distribution(k, self.X_train[i, :].reshape(-1, 1)) * self.parameters_of_hard_latent[k]\n",
    "                #print(t1)\n",
    "                #print( self.parameters_of_hard_latent[k])\n",
    "                #print(self.parameters_of_soft_latent[i, k])\n",
    "                #print((t1/self.parameters_of_soft_latent[i, k]))\n",
    "                \n",
    "                #assert(((t1/self.parameters_of_soft_latent[i, k]) > 0) == True)\n",
    "                \n",
    "                log_likelihood += self.parameters_of_soft_latent[i, k] * np.log(np.divide( t1 , self.parameters_of_soft_latent[i, k]))\n",
    "        \n",
    "        return log_likelihood\n",
    "\n",
    "    def fit(self, max_iteration, eps=1e-5):\n",
    "        convergence_test = True\n",
    "        count = 0\n",
    "        while( (convergence_test == True) and (count != max_iteration)):\n",
    "            self.E_step()#Update the soft latent values            \n",
    "            log_likelihood_t = self.computing_log_likelihood()\n",
    "            self.M_step()#Update the parameters of the condtional distribution of x given z\n",
    "            log_likelihood_t_future = self.computing_log_likelihood()\n",
    "            #print(f\"past:{log_likelihood_t}\")\n",
    "            #print(f\"future:{log_likelihood_t_future}\")\n",
    "            print(f\"Number of iteration:{count}, max_iteration:{max_iteration}, past:{log_likelihood_t}, future:{log_likelihood_t_future}\")\n",
    "            count = count + 1\n",
    "            if( (log_likelihood_t_future - log_likelihood_t) < eps and (count > 10)):\n",
    "                print(\"We converged to the optimal value for the log-likelihood\")\n",
    "                convergence_test =False #We reached the parameters that maximize the log-likelihood, no adancement in the log-likelihood\n",
    "        \n",
    "        return self.parameters_of_hard_latent, self.parameters_of_soft_latent, self.means_of_each_clusters, self.covariance_within_each_clusters\n",
    "\n",
    "    def prediction_dataset(self, X):\n",
    "        prediciton = []\n",
    "        for i in range(0, X.shape[0]):\n",
    "            prediciton.append(self.predict(X[i, :]))\n",
    "            \n",
    "        return np.array(prediciton)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        prediction = np.zeros((self.K, 1))\n",
    "        for k in range(0, self.K):\n",
    "            prediction[k] = self.compute_soft_latent(k, x.reshape(-1, 1))\n",
    "        \n",
    "        return np.argmax(prediction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of iteration:0, max_iteration:1000, past:[[-2282.83271147]], future:[[-939.38751629]]\nNumber of iteration:1, max_iteration:1000, past:[[-911.72578855]], future:[[-517.28563538]]\nNumber of iteration:2, max_iteration:1000, past:[[-515.63779951]], future:[[-505.18031714]]\nNumber of iteration:3, max_iteration:1000, past:[[-501.20815646]], future:[[-476.51994133]]\nNumber of iteration:4, max_iteration:1000, past:[[-470.03491342]], future:[[-427.82650092]]\nNumber of iteration:5, max_iteration:1000, past:[[-419.0620125]], future:[[-369.235186]]\nNumber of iteration:6, max_iteration:1000, past:[[-362.3048864]], future:[[-339.57270936]]\nNumber of iteration:7, max_iteration:1000, past:[[-336.57401125]], future:[[-321.63435204]]\nNumber of iteration:8, max_iteration:1000, past:[[-319.87255483]], future:[[-309.64422446]]\nNumber of iteration:9, max_iteration:1000, past:[[-308.04244442]], future:[[-299.02582969]]\nNumber of iteration:10, max_iteration:1000, past:[[-297.31029576]], future:[[-287.99629557]]\nNumber of iteration:11, max_iteration:1000, past:[[-285.88645572]], future:[[-273.12087755]]\nNumber of iteration:12, max_iteration:1000, past:[[-269.42706595]], future:[[-250.25221988]]\nNumber of iteration:13, max_iteration:1000, past:[[-244.4390985]], future:[[-225.71545306]]\nNumber of iteration:14, max_iteration:1000, past:[[-218.2725096]], future:[[-180.98246532]]\nNumber of iteration:15, max_iteration:1000, past:[[-171.75497683]], future:[[-147.1836583]]\nNumber of iteration:16, max_iteration:1000, past:[[-142.2592938]], future:[[-134.74777003]]\nNumber of iteration:17, max_iteration:1000, past:[[-130.39875992]], future:[[-127.48152751]]\nNumber of iteration:18, max_iteration:1000, past:[[-124.27786538]], future:[[-120.38867126]]\nNumber of iteration:19, max_iteration:1000, past:[[-119.35835923]], future:[[-117.80352693]]\nNumber of iteration:20, max_iteration:1000, past:[[-117.50055366]], future:[[-116.58491995]]\nNumber of iteration:21, max_iteration:1000, past:[[-116.34821473]], future:[[-115.15133024]]\nNumber of iteration:22, max_iteration:1000, past:[[-114.8743556]], future:[[-113.2185484]]\nNumber of iteration:23, max_iteration:1000, past:[[-112.93203938]], future:[[-111.18813281]]\nNumber of iteration:24, max_iteration:1000, past:[[-110.97370339]], future:[[-109.67758351]]\nNumber of iteration:25, max_iteration:1000, past:[[-109.54031219]], future:[[-108.72060687]]\nNumber of iteration:26, max_iteration:1000, past:[[-108.63537078]], future:[[-108.14142044]]\nNumber of iteration:27, max_iteration:1000, past:[[-108.09107847]], future:[[-107.82083073]]\nNumber of iteration:28, max_iteration:1000, past:[[-107.79204686]], future:[[-107.6641428]]\nNumber of iteration:29, max_iteration:1000, past:[[-107.64735178]], future:[[-107.59578306]]\nNumber of iteration:30, max_iteration:1000, past:[[-107.58554914]], future:[[-107.56884754]]\nNumber of iteration:31, max_iteration:1000, past:[[-107.56236787]], future:[[-107.55964476]]\nNumber of iteration:32, max_iteration:1000, past:[[-107.55543669]], future:[[-107.55761353]]\nWe converged to the optimal value for the log-likelihood\n"
    }
   ],
   "source": [
    "#Randomly Initialized\n",
    "model = MixtureofGaussian(X_train, k, \"random\")\n",
    "parameters_of_hard_latent, parameters_of_soft_latent, means_of_each_clusters, covariance_within_each_clusters = model.fit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 0, 2, 2, 1, 1, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2,\n       0, 1, 1, 1, 2, 1, 2, 2, 1, 2, 0, 1, 0, 0, 2, 2, 0, 2, 1, 2, 0, 1,\n       1, 2, 2, 1, 0, 1, 1, 2, 2, 0, 0, 0, 0, 2, 1, 1, 2, 2, 1, 1, 1, 2,\n       0, 1, 0, 0, 1, 2, 2, 0, 2, 0, 1, 0, 2, 0, 2, 2, 2, 1, 2, 2, 1, 2,\n       2, 0, 1, 2, 0, 2, 1, 0, 1, 2, 2, 0, 2, 0, 2, 2, 0, 0, 1, 2, 0, 1,\n       2, 0], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "pred = model.prediction_dataset(X_train)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the training set\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[35,  0,  0],\n       [ 0, 38,  1],\n       [ 0, 10, 28]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "pred = model.prediction_dataset(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "#print(sklearn.metrics.confusion_matrix(y_train, pred))\n",
    "c = sklearn.metrics.confusion_matrix(y_train, pred)\n",
    "c[:, list(np.argmax(c, axis=1))]#ordering the cluster to where it shows the highest number of matching with the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the test set\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[15,  0,  0],\n       [ 2,  9,  0],\n       [ 0,  2, 10]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "source": [
    "pred = model.prediction_dataset(X_test)\n",
    "print(\"Performance on the test set\")\n",
    "#print(sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "c = sklearn.metrics.confusion_matrix(y_test, pred)\n",
    "c[:, list(np.argmax(c, axis=1))]#ordering the cluster to where it shows the highest number of matching with the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "count:0, max_iterations1000, past:10, future:0\ncount:1, max_iterations1000, past:565.374081063309, future:231.49457602906614\ncount:2, max_iterations1000, past:231.49457602906614, future:128.5139214635211\ncount:3, max_iterations1000, past:128.5139214635211, future:109.6719256474805\ncount:4, max_iterations1000, past:109.6719256474805, future:108.97247599145511\ncount:5, max_iterations1000, past:108.97247599145511, future:107.34494618026055\ncount:6, max_iterations1000, past:107.34494618026055, future:106.2528195193156\ncount:7, max_iterations1000, past:106.2528195193156, future:106.19859005755436\nFinished the initialzation by the kmeans\n[[0.07968003 0.0288702  0.04693592 0.02816197]\n [0.0288702  0.10989086 0.02636605 0.03010616]\n [0.04693592 0.02636605 0.05799119 0.04965981]\n [0.02816197 0.03010616 0.04965981 0.05964532]]\n[[0.0460743  0.068189   0.00225058 0.0033825 ]\n [0.068189   0.16917547 0.0038792  0.00594619]\n [0.00225058 0.0038792  0.00294652 0.00160053]\n [0.0033825  0.00594619 0.00160053 0.00617243]]\n[[0.13984716 0.01703554 0.05811178 0.01730659]\n [0.01703554 0.10173606 0.01860027 0.03844638]\n [0.05811178 0.01860027 0.04722681 0.03889816]\n [0.01730659 0.03844638 0.03889816 0.0796019 ]]\nNumber of iteration:0, max_iteration:1000, past:[[-inf]], future:[[-151.39110593]]\nNumber of iteration:1, max_iteration:1000, past:[[-145.85814392]], future:[[-145.52638894]]\nNumber of iteration:2, max_iteration:1000, past:[[-144.16526148]], future:[[-144.70517624]]\nNumber of iteration:3, max_iteration:1000, past:[[-143.80488389]], future:[[-144.11960859]]\nNumber of iteration:4, max_iteration:1000, past:[[-143.45544366]], future:[[-143.52292931]]\nNumber of iteration:5, max_iteration:1000, past:[[-142.9689333]], future:[[-142.93483058]]\nNumber of iteration:6, max_iteration:1000, past:[[-142.33737235]], future:[[-141.74851614]]\nNumber of iteration:7, max_iteration:1000, past:[[-140.9015986]], future:[[-138.32934124]]\nNumber of iteration:8, max_iteration:1000, past:[[-137.04023851]], future:[[-130.88494523]]\nNumber of iteration:9, max_iteration:1000, past:[[-129.70990314]], future:[[-124.69546059]]\nNumber of iteration:10, max_iteration:1000, past:[[-124.24284975]], future:[[-122.91038487]]\nNumber of iteration:11, max_iteration:1000, past:[[-122.57973627]], future:[[-121.93076753]]\nNumber of iteration:12, max_iteration:1000, past:[[-121.35755939]], future:[[-119.27421655]]\nNumber of iteration:13, max_iteration:1000, past:[[-118.08547903]], future:[[-113.15473164]]\nNumber of iteration:14, max_iteration:1000, past:[[-112.61105419]], future:[[-110.04673088]]\nNumber of iteration:15, max_iteration:1000, past:[[-109.86643735]], future:[[-108.2929724]]\nNumber of iteration:16, max_iteration:1000, past:[[-108.17566327]], future:[[-106.80761721]]\nNumber of iteration:17, max_iteration:1000, past:[[-106.71904825]], future:[[-105.44632733]]\nNumber of iteration:18, max_iteration:1000, past:[[-105.37747841]], future:[[-104.20274981]]\nNumber of iteration:19, max_iteration:1000, past:[[-104.15164916]], future:[[-103.11233144]]\nNumber of iteration:20, max_iteration:1000, past:[[-103.07775934]], future:[[-102.20860725]]\nNumber of iteration:21, max_iteration:1000, past:[[-102.18701693]], future:[[-101.49188561]]\nNumber of iteration:22, max_iteration:1000, past:[[-101.478638]], future:[[-100.93273813]]\nNumber of iteration:23, max_iteration:1000, past:[[-100.92430834]], future:[[-100.49401437]]\nNumber of iteration:24, max_iteration:1000, past:[[-100.48838649]], future:[[-100.14454971]]\nNumber of iteration:25, max_iteration:1000, past:[[-100.140657]], future:[[-99.86172067]]\nNumber of iteration:26, max_iteration:1000, past:[[-99.85896841]], future:[[-99.62969]]\nNumber of iteration:27, max_iteration:1000, past:[[-99.62771682]], future:[[-99.43726109]]\nNumber of iteration:28, max_iteration:1000, past:[[-99.43583258]], future:[[-99.27631106]]\nNumber of iteration:29, max_iteration:1000, past:[[-99.27526907]], future:[[-99.14077903]]\nNumber of iteration:30, max_iteration:1000, past:[[-99.14001432]], future:[[-99.0260281]]\nNumber of iteration:31, max_iteration:1000, past:[[-99.025464]], future:[[-98.92843691]]\nNumber of iteration:32, max_iteration:1000, past:[[-98.92801898]], future:[[-98.84513]]\nNumber of iteration:33, max_iteration:1000, past:[[-98.84481921]], future:[[-98.77379352]]\nNumber of iteration:34, max_iteration:1000, past:[[-98.77356167]], future:[[-98.71254499]]\nNumber of iteration:35, max_iteration:1000, past:[[-98.71237156]], future:[[-98.65983875]]\nNumber of iteration:36, max_iteration:1000, past:[[-98.65970871]], future:[[-98.61439565]]\nNumber of iteration:37, max_iteration:1000, past:[[-98.61429795]], future:[[-98.57514989]]\nNumber of iteration:38, max_iteration:1000, past:[[-98.57507636]], future:[[-98.54120814]]\nNumber of iteration:39, max_iteration:1000, past:[[-98.54115271]], future:[[-98.51181776]]\nNumber of iteration:40, max_iteration:1000, past:[[-98.51177593]], future:[[-98.48634176]]\nNumber of iteration:41, max_iteration:1000, past:[[-98.48631015]], future:[[-98.46423888]]\nNumber of iteration:42, max_iteration:1000, past:[[-98.46421497]], future:[[-98.44504761]]\nNumber of iteration:43, max_iteration:1000, past:[[-98.44502951]], future:[[-98.42837325]]\nNumber of iteration:44, max_iteration:1000, past:[[-98.42835954]], future:[[-98.41387735]]\nNumber of iteration:45, max_iteration:1000, past:[[-98.41386695]], future:[[-98.40126901]]\nNumber of iteration:46, max_iteration:1000, past:[[-98.40126113]], future:[[-98.39029774]]\nNumber of iteration:47, max_iteration:1000, past:[[-98.39029175]], future:[[-98.38074742]]\nNumber of iteration:48, max_iteration:1000, past:[[-98.38074287]], future:[[-98.37243134]]\nNumber of iteration:49, max_iteration:1000, past:[[-98.37242789]], future:[[-98.36518799]]\nNumber of iteration:50, max_iteration:1000, past:[[-98.36518537]], future:[[-98.35887746]]\nNumber of iteration:51, max_iteration:1000, past:[[-98.35887547]], future:[[-98.35337847]]\nNumber of iteration:52, max_iteration:1000, past:[[-98.35337696]], future:[[-98.3485858]]\nNumber of iteration:53, max_iteration:1000, past:[[-98.34858464]], future:[[-98.34440805]]\nNumber of iteration:54, max_iteration:1000, past:[[-98.34440718]], future:[[-98.34076585]]\nNumber of iteration:55, max_iteration:1000, past:[[-98.34076518]], future:[[-98.33759015]]\nNumber of iteration:56, max_iteration:1000, past:[[-98.33758965]], future:[[-98.33482093]]\nNumber of iteration:57, max_iteration:1000, past:[[-98.33482055]], future:[[-98.33240594]]\nNumber of iteration:58, max_iteration:1000, past:[[-98.33240565]], future:[[-98.33029971]]\nNumber of iteration:59, max_iteration:1000, past:[[-98.33029948]], future:[[-98.32846263]]\nNumber of iteration:60, max_iteration:1000, past:[[-98.32846246]], future:[[-98.32686022]]\nNumber of iteration:61, max_iteration:1000, past:[[-98.32686009]], future:[[-98.32546243]]\nNumber of iteration:62, max_iteration:1000, past:[[-98.32546233]], future:[[-98.32424308]]\nNumber of iteration:63, max_iteration:1000, past:[[-98.324243]], future:[[-98.32317934]]\nNumber of iteration:64, max_iteration:1000, past:[[-98.32317929]], future:[[-98.32225133]]\nNumber of iteration:65, max_iteration:1000, past:[[-98.32225129]], future:[[-98.32144171]]\nNumber of iteration:66, max_iteration:1000, past:[[-98.32144167]], future:[[-98.32073534]]\nNumber of iteration:67, max_iteration:1000, past:[[-98.32073532]], future:[[-98.32011906]]\nNumber of iteration:68, max_iteration:1000, past:[[-98.32011904]], future:[[-98.31958136]]\nNumber of iteration:69, max_iteration:1000, past:[[-98.31958135]], future:[[-98.31911221]]\nNumber of iteration:70, max_iteration:1000, past:[[-98.3191122]], future:[[-98.31870287]]\nNumber of iteration:71, max_iteration:1000, past:[[-98.31870287]], future:[[-98.31834571]]\nNumber of iteration:72, max_iteration:1000, past:[[-98.31834571]], future:[[-98.31803408]]\nNumber of iteration:73, max_iteration:1000, past:[[-98.31803407]], future:[[-98.31776216]]\nNumber of iteration:74, max_iteration:1000, past:[[-98.31776215]], future:[[-98.31752489]]\nNumber of iteration:75, max_iteration:1000, past:[[-98.31752489]], future:[[-98.31731787]]\nNumber of iteration:76, max_iteration:1000, past:[[-98.31731786]], future:[[-98.31713722]]\nNumber of iteration:77, max_iteration:1000, past:[[-98.31713722]], future:[[-98.31697959]]\nNumber of iteration:78, max_iteration:1000, past:[[-98.31697959]], future:[[-98.31684205]]\nNumber of iteration:79, max_iteration:1000, past:[[-98.31684205]], future:[[-98.31672204]]\nNumber of iteration:80, max_iteration:1000, past:[[-98.31672204]], future:[[-98.31661731]]\nNumber of iteration:81, max_iteration:1000, past:[[-98.31661731]], future:[[-98.31652593]]\nNumber of iteration:82, max_iteration:1000, past:[[-98.31652593]], future:[[-98.3164462]]\nNumber of iteration:83, max_iteration:1000, past:[[-98.3164462]], future:[[-98.31637662]]\nNumber of iteration:84, max_iteration:1000, past:[[-98.31637662]], future:[[-98.3163159]]\nNumber of iteration:85, max_iteration:1000, past:[[-98.3163159]], future:[[-98.31626293]]\nNumber of iteration:86, max_iteration:1000, past:[[-98.31626293]], future:[[-98.3162167]]\nNumber of iteration:87, max_iteration:1000, past:[[-98.3162167]], future:[[-98.31617636]]\nNumber of iteration:88, max_iteration:1000, past:[[-98.31617636]], future:[[-98.31614116]]\nNumber of iteration:89, max_iteration:1000, past:[[-98.31614116]], future:[[-98.31611044]]\nNumber of iteration:90, max_iteration:1000, past:[[-98.31611044]], future:[[-98.31608364]]\nNumber of iteration:91, max_iteration:1000, past:[[-98.31608364]], future:[[-98.31606025]]\nNumber of iteration:92, max_iteration:1000, past:[[-98.31606025]], future:[[-98.31603985]]\nNumber of iteration:93, max_iteration:1000, past:[[-98.31603985]], future:[[-98.31602204]]\nNumber of iteration:94, max_iteration:1000, past:[[-98.31602204]], future:[[-98.3160065]]\nNumber of iteration:95, max_iteration:1000, past:[[-98.3160065]], future:[[-98.31599294]]\nNumber of iteration:96, max_iteration:1000, past:[[-98.31599294]], future:[[-98.31598111]]\nNumber of iteration:97, max_iteration:1000, past:[[-98.31598111]], future:[[-98.31597078]]\nNumber of iteration:98, max_iteration:1000, past:[[-98.31597078]], future:[[-98.31596177]]\nWe converged to the optimal value for the log-likelihood\n"
    }
   ],
   "source": [
    "#K-means Initialized\n",
    "model = MixtureofGaussian(X_train, k, \"kmeans\")\n",
    "parameters_of_hard_latent, parameters_of_soft_latent, means_of_each_clusters, covariance_within_each_clusters = model.fit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the training set\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[35,  0,  0],\n       [ 0, 38,  1],\n       [ 0,  4, 34]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "source": [
    "pred = model.prediction_dataset(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "#print(sklearn.metrics.confusion_matrix(y_train, pred))\n",
    "c = sklearn.metrics.confusion_matrix(y_train, pred)\n",
    "c[:, list(np.argmax(c, axis=1))]#ordering the cluster to where it shows the highest number of matching with the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the test set\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[15,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  1, 11]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 179
    }
   ],
   "source": [
    "pred = model.prediction_dataset(X_test)\n",
    "print(\"Performance on the test set\")\n",
    "#print(sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "c = sklearn.metrics.confusion_matrix(y_test, pred)\n",
    "c[:, list(np.argmax(c, axis=1))]#ordering the cluster to where it shows the highest number of matching with the true labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 1, chapter 2 and Chapter 9 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 12: (https://www.youtube.com/watch?v=ZZGTuAkF-Hw)\n",
    "* Andrew Ng, Lec 13: (https://www.youtube.com/watch?v=LBtuYU-HfUg)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}