{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Student-t Distribution\n",
    "\n",
    "### Assuming the degrees of freedom to be known in advance\n",
    "#### Assumption about the continuous Latent Variable and its conditional Distribution, and the Discrete Latent Variable and its conditional Distribution\n",
    "#### E-step \n",
    "#### M-step\n",
    "#### Convergence Test\n",
    "\n",
    "### Assuming the degrees of freedom to be unknown in advance\n",
    "####  Assumption about the continuous Latent Variable and its conditional Distribution, and the Discrete Latent Variable and its conditional Distribution\n",
    "#### Multicycle ECM\n",
    "##### E-step\n",
    "##### First CM \n",
    "##### Second CM-step\n",
    "#### Convergence Test\n",
    "\n",
    "------------------------\n",
    "I only implemented the model with the Assumption that the degree of freedom is known.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To have better initialization of the algorithm\n",
    "class kmeans(object):\n",
    "\n",
    "    def __init__(self, X_train, k):\n",
    "        self.K = k\n",
    "        self.m = X_train.shape[0]\n",
    "        self.n = X_train.shape[1]\n",
    "        self.X_train = X_train\n",
    "        choices = numpy.random.choice(np.arange(0, self.m), self.K, replace=False)\n",
    "        self.centers = [X_train[choices[i], :].reshape(-1, 1) for i in range(0, self.K)]# initalize the clusters centers to be one of the observations\n",
    "        self.clusters_assignments = np.zeros((self.m, 1))#Just to give it the necessary shape\n",
    "    \n",
    "    def reassign_clusters_centers(self):\n",
    "        for k in range(0, self.K):\n",
    "            temp = np.zeros((self.n, 1))\n",
    "            clusters = list(map(lambda i: True if i == k else False, self.clusters_assignments))\n",
    "            for i in range(0, self.m):\n",
    "                if clusters[i] == True:\n",
    "                    temp += clusters[i] * self.X_train[i, :].reshape(-1, 1)#clusters contained in {0, 1}\n",
    "                else:\n",
    "                    pass\n",
    "            #print(np.sum(clusters))\n",
    "            self.centers[k] = temp/np.sum(clusters)\n",
    "\n",
    "    def distortiuon_function(self):\n",
    "        temp = 0\n",
    "        for i in range(0, self.m):\n",
    "            for k in range(0, self.K):\n",
    "                if self.clusters_assignments[i] == k:\n",
    "                    temp += np.linalg.norm(self.X_train[i, :].reshape(-1, 1) - self.centers[k].reshape(-1, 1))**2\n",
    "                    break#They willn't be assigned to more than one cluster in tandem\n",
    "        return temp\n",
    "\n",
    "    def assign_to_clusters(self, x):\n",
    "        temp = []\n",
    "        for k in range(0, self.K):\n",
    "            temp.append(np.linalg.norm(x.reshape(-1, 1) - self.centers[k].reshape(-1, 1))**2)#We will use L2-norm for dissimilarity measure\n",
    "        return np.argmin(temp)#return the cluster number\n",
    "\n",
    "    def E_step(self):\n",
    "        for i in range(0, self.m):\n",
    "            self.clusters_assignments[i] = self.assign_to_clusters(X_train[i, :].reshape(-1, 1))\n",
    "    \n",
    "    def fit(self, max_iterations, eps=1e-5):\n",
    "        self.E_step()#To initialize the clusters assignments\n",
    "        past = 10\n",
    "        future = 0\n",
    "        count = 0\n",
    "        while(abs(past - future) > eps):#I will care for only lack of progress because k-means will always be able to minimize the distortion functions\n",
    "            print(f\"count:{count}, max_iterations{max_iterations}, past:{past}, future:{future}\")\n",
    "            count += 1\n",
    "            past = self.distortiuon_function()\n",
    "            self.reassign_clusters_centers()#The M step\n",
    "            self.E_step()\n",
    "            future = self.distortiuon_function()\n",
    "\n",
    "        return self.centers, self.clusters_assignments\n",
    "\n",
    "    def prediction_dataset(self, X):\n",
    "        predictions = []\n",
    "        for i in range(0, X.shape[0]):\n",
    "            predictions.append(self.assign_to_clusters(X[i, :].reshape(-1, 1)))\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, x):\n",
    "        return  self.assign_to_clusters(x.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixtures_ofMultiVariate_Student_t_model(object):\n",
    "\n",
    "    def __init__(self, X_train, G, df, randomly=\"random\", max_iteration =1000):\n",
    "        self.G = G\n",
    "        if (all(int(c)>0 for c in df)) and (len(df) == self.G):\n",
    "            self.df = df#Will be a list\n",
    "        else: \n",
    "            print(\"Wrong df value because if you view gamma function as a factorial, negative value of n wouldn't exist\")\n",
    "            raise Exception(\"wrong df\")\n",
    "        self.m = X_train.shape[0]\n",
    "        self.n = X_train.shape[1]\n",
    "        self.means_of_each_clusters = list(map(lambda i: np.zeros((self.n, 1)), np.arange(0, self.G)))\n",
    "        self.covariance_within_each_clusters = list(map(lambda i: np.zeros((self.n, self.n)), np.arange(0, self.G))) \n",
    "        self.parameters_of_mixing_latent_parameter = [i for i in np.zeros((self.G, 1))]\n",
    "        self.parameters_of_mixing_soft_latent = np.zeros((self.m, self.G))#Posterior for clusters\n",
    "        self.continuous_latent_variable_soft = np.zeros((self.m, self.G))#Posterior of the conjugate prior\n",
    "\n",
    "        self.X_train = X_train\n",
    "        if randomly == \"random\":\n",
    "            self.initialize_parameters_randomly()\n",
    "\n",
    "        else:#Kmeans initialization\n",
    "            model = kmeans(self.X_train, self.G)\n",
    "            centers, clusters  = model.fit(max_iterations=max_iteration)\n",
    "            print(\"Finished the initialization by the kmeans\")\n",
    "            for k in range(0, self.G):\n",
    "                cluster_separated = list(map(lambda i: True if i == k else False, clusters))\n",
    "                self.parameters_of_mixing_latent_parameter[k] = np.sum(cluster_separated)/self.m\n",
    "                self.means_of_each_clusters[k] = (centers[k]).reshape(-1, 1)\n",
    "                #Calaculating the covariance matrix matrix within each cluster\n",
    "                temp = np.zeros((self.n, self.n))\n",
    "                for i in range(0, self.m):\n",
    "                    if cluster_separated[i] == True:\n",
    "                        temp += np.dot((self.X_train[i, :].reshape(-1, 1) - self.means_of_each_clusters[k].reshape(-1, 1)).reshape(-1, 1), (self.X_train[i, :].reshape(-1, 1) - self.means_of_each_clusters[k].reshape(-1, 1)).reshape(1, -1))\n",
    "                \n",
    "                self.covariance_within_each_clusters[k] = (1/np.sum(clusters)) * temp\n",
    "                #print(self.covariance_within_each_clusters[k])\n",
    "\n",
    "    def initialize_parameters_randomly(self):\n",
    "        for g in range(0, self.G):\n",
    "            self.means_of_each_clusters[g] = (numpy.random.randn(self.n)).reshape(-1, 1)\n",
    "            c = np.random.randn(self.n, self.n)\n",
    "            self.covariance_within_each_clusters[g] = 25 * np.dot(c, c.T)\n",
    "            self.parameters_of_mixing_latent_parameter[g] = abs(numpy.random.randn())\n",
    "\n",
    "        sums = np.sum(self.parameters_of_mixing_latent_parameter)\n",
    "        self.parameters_of_mixing_latent_parameter = self.parameters_of_mixing_latent_parameter/sums#To ensure that the parameters of the Multinomial distribution sums to 1\n",
    "\n",
    "    def Mahalanobis_distance(self, x, g):\n",
    "        return np.dot(np.transpose(x.reshape(-1, 1) - self.means_of_each_clusters[g].reshape(-1, 1)), np.dot(np.linalg.inv(self.covariance_within_each_clusters[g]), x.reshape(-1, 1) - self.means_of_each_clusters[g].reshape(-1, 1)))\n",
    "\n",
    "    def MultiVariate_Student_t_PDF(self, x, g):\n",
    "        det = np.linalg.det(self.covariance_within_each_clusters[g])\n",
    "        #print(det)\n",
    "        return (\n",
    "            (scipy.special.gamma((self.df[g] + self.n)/2)/scipy.special.gamma(self.df[g] /2) ) * (1/(np.pi * self.df[g])) * \n",
    "        (1/np.sqrt(det)) * \n",
    "        (1/(1 + (self.Mahalanobis_distance(x, g)/ self.df[g]) )**((self.df[g]+self.n)/2) )\n",
    "            )\n",
    "\n",
    "    def compute_continuous_latent(self, x, g):\n",
    "        return (self.df[g] + self.n)/(self.df[g] + self.Mahalanobis_distance(x, g))\n",
    "    \n",
    "    def compute_discrete_latent(self, x, g):\n",
    "        denominator = 0 \n",
    "        for k in range(0, self.G):\n",
    "            denominator = denominator + ( self.MultiVariate_Student_t_PDF(x, k) * self.parameters_of_mixing_latent_parameter[k])\n",
    "        #1e-50 were added to preven division by zero when computing the log-likelihood\n",
    "        return (self.MultiVariate_Student_t_PDF(x, g) * self.parameters_of_mixing_latent_parameter[g])/denominator\n",
    "    \n",
    "    def E_step(self):\n",
    "        self.continuous_latent_variable_soft = np.array(list(map(lambda x: np.array([self.compute_continuous_latent(x, g) for g in range(0, self.G)]).reshape(1, -1), self.X_train))).reshape(self.m, self.G)\n",
    "        #print(self.continuous_latent_variable_soft.shape)\n",
    "        self.parameters_of_mixing_soft_latent = np.array(list(map(lambda x: np.array([self.compute_discrete_latent(x, g) for g in range(0, self.G)]).reshape(1, -1), self.X_train))).reshape(self.m, self.G)\n",
    "    \n",
    "    def M_step(self):\n",
    "        for g in range(0, self.G):\n",
    "            self.parameters_of_mixing_latent_parameter[g] = (1/self.m) * np.sum(self.parameters_of_mixing_soft_latent[:, g])\n",
    "            #print((self.continuous_latent_variable_soft[:, g].reshape(-1, 1) * self.parameters_of_mixing_soft_latent[:, g].reshape(-1, 1)).shape)\n",
    "\n",
    "            temp = np.zeros((self.n, 1))\n",
    "            for i in range(0, self.m):\n",
    "                temp += (self.continuous_latent_variable_soft[i, g] * self.parameters_of_mixing_soft_latent[i, g] *self.X_train[i, :]).reshape(-1, 1)\n",
    "            temp = (1/np.sum(np.multiply(self.continuous_latent_variable_soft[:, g].reshape(-1, 1), self.parameters_of_mixing_soft_latent[:, g].reshape(-1, 1)) ) ) * temp\n",
    "            self.means_of_each_clusters[g] = temp\n",
    "            #self.means_of_each_clusters[g] =(1/np.sum(np.multiply(self.continuous_latent_variable_soft[:, g].reshape(-1, 1), self.parameters_of_mixing_soft_latent[:, g].reshape(-1, 1)) ) ) * np.sum(self.continuous_latent_variable_soft[:, g].reshape(-1, 1) * self.parameters_of_mixing_soft_latent[:, g].reshape(-1, 1) * self.X_train.T, axis=0).reshape(-1, 1)\n",
    "\n",
    "            for i in range(0, self.m):\n",
    "                self.covariance_within_each_clusters[g] += self.continuous_latent_variable_soft[i, g] * self.parameters_of_mixing_soft_latent[i, g] * np.dot((self.X_train[i, :].reshape(-1, 1) - self.means_of_each_clusters[g].reshape(-1, 1)),(self.X_train[i, :].reshape(-1, 1) - self.means_of_each_clusters[g].reshape(-1, 1)).T)\n",
    "            \n",
    "            self.covariance_within_each_clusters[g] /= np.sum(self.parameters_of_mixing_soft_latent[:, g])\n",
    "    \n",
    "    def compute_log_likelihood(self):\n",
    "        temp = 0\n",
    "        for i in range(0, self.m):\n",
    "            temp2 = 0\n",
    "            for g in range(0, self.G):\n",
    "                temp2 += self.parameters_of_mixing_latent_parameter[g] * self.MultiVariate_Student_t_PDF(self.X_train[i, :], g)\n",
    "            #if temp2 <=0:\n",
    "                #print(temp2)\n",
    "            #assert(temp2 > 0)\n",
    "            #print(temp2)\n",
    "            temp += np.log(temp2)\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def fit(self, max_iteration, eps=1e-3):\n",
    "        convergence_test = True\n",
    "        count = 0\n",
    "        while( (convergence_test == True) and (count != max_iteration)):\n",
    "            self.E_step()#Update the soft latent values\n",
    "            m_means, m_cov, m_mixing = (self.means_of_each_clusters.copy(), self.covariance_within_each_clusters.copy(), self.parameters_of_mixing_latent_parameter.copy())            \n",
    "            log_likelihood_t = self.compute_log_likelihood()\n",
    "            self.M_step()#Update the parameters of the conditional distribution of x given z and u\n",
    "            log_likelihood_t_future = self.compute_log_likelihood()\n",
    "            print(f\"Number of iteration:{count}, max_iteration:{max_iteration}, past:{log_likelihood_t}, future:{log_likelihood_t_future}\")\n",
    "            count = count + 1\n",
    "            if log_likelihood_t_future != log_likelihood_t_future:#The usual trick nan doesn't equal itself\n",
    "                self.means_of_each_clusters, self.covariance_within_each_clusters, self.parameters_of_mixing_latent_parameter = (m_means, m_cov, m_mixing)\n",
    "                print(\"Something wrong happened in the Maximization step\")\n",
    "                break\n",
    "            #print(log_likelihood_t_future[0])\n",
    "            if( (log_likelihood_t_future - log_likelihood_t) < eps and (count > 10)):\n",
    "                print(\"We converged to the optimal value for the log-likelihood\")\n",
    "                convergence_test =False #We reached the parameters that maximize the log-likelihood, no adancement in the log-likelihood\n",
    "        \n",
    "        return self.means_of_each_clusters, self.covariance_within_each_clusters, self.parameters_of_mixing_latent_parameter\n",
    "\n",
    "    def prediction_dataset(self, X):\n",
    "        prediciton = []\n",
    "        for i in range(0, X.shape[0]):\n",
    "            prediciton.append(self.predict(X[i, :]))\n",
    "            \n",
    "        return np.array(prediciton)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        prediction = np.zeros((self.G, 1))\n",
    "        for g in range(0, self.G):\n",
    "            prediction[g] = self.compute_discrete_latent(x.reshape(-1, 1), g)\n",
    "        \n",
    "        return np.argmax(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(133, 14)\n(45, 14)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 1, 1, 2, 0, 1, 0, 0, 2, 2, 1, 1, 0, 1, 0, 2, 1, 1, 2, 0, 0, 0,\n       2, 0, 0, 1, 2, 1, 0, 2, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 2, 1,\n       1, 1, 0, 1, 1, 1, 2, 2, 0, 1, 2, 2, 1, 1, 0, 1, 2, 2, 1, 2, 1, 1,\n       1, 0, 0, 2, 0, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2, 2,\n       1, 0, 0, 1, 2, 2, 0, 1, 2, 2, 2, 2, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0,\n       2, 1, 0, 2, 2, 0, 0, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1,\n       1])"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "numpy.random.seed(120)\n",
    "\n",
    "#Using IRIS and Wine Dataset\n",
    "#X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "X, y = sklearn.datasets.load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "#standard = sklearn.preprocessing.StandardScaler()\n",
    "#X_train = standard.fit_transform(X_train)\n",
    "training_data = np.c_[X_train, y_train]#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "\n",
    "#X_test = standard.transform(X_test)\n",
    "test_data = np.c_[X_test, y_test]\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)\n",
    "k = len(set(y_train))\n",
    "y_train#It needs to be labeled from 0 to k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3\n"
    }
   ],
   "source": [
    "print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "count:0, max_iterations10, past:10, future:0\ncount:1, max_iterations10, past:10792010.491995996, future:3102217.0489981584\ncount:2, max_iterations10, past:3102217.0489981584, future:1978365.5976835634\ncount:3, max_iterations10, past:1978365.5976835634, future:1795178.7694889628\ncount:4, max_iterations10, past:1795178.7694889628, future:1702081.6684963438\ncount:5, max_iterations10, past:1702081.6684963438, future:1633691.812665524\ncount:6, max_iterations10, past:1633691.812665524, future:1616124.000250265\ncount:7, max_iterations10, past:1616124.000250265, future:1615357.3439364806\nFinished the initialzation by the kmeans\nNumber of iteration:0, max_iteration:1000, past:[[101.20972741]], future:[[239.08935794]]\nNumber of iteration:1, max_iteration:1000, past:[[239.08935794]], future:[[305.70311323]]\nNumber of iteration:2, max_iteration:1000, past:[[305.70311323]], future:[[335.73446949]]\nNumber of iteration:3, max_iteration:1000, past:[[335.73446949]], future:[[351.79857503]]\nNumber of iteration:4, max_iteration:1000, past:[[351.79857503]], future:[[362.72717232]]\nNumber of iteration:5, max_iteration:1000, past:[[362.72717232]], future:[[372.64827371]]\nNumber of iteration:6, max_iteration:1000, past:[[372.64827371]], future:[[378.63954291]]\nNumber of iteration:7, max_iteration:1000, past:[[378.63954291]], future:[[379.84463048]]\nNumber of iteration:8, max_iteration:1000, past:[[379.84463048]], future:[[380.81663845]]\nNumber of iteration:9, max_iteration:1000, past:[[380.81663845]], future:[[381.75340491]]\nNumber of iteration:10, max_iteration:1000, past:[[381.75340491]], future:[[383.8241765]]\nNumber of iteration:11, max_iteration:1000, past:[[383.8241765]], future:[[385.52344763]]\nNumber of iteration:12, max_iteration:1000, past:[[385.52344763]], future:[[386.82629053]]\nNumber of iteration:13, max_iteration:1000, past:[[386.82629053]], future:[[388.15525239]]\nNumber of iteration:14, max_iteration:1000, past:[[388.15525239]], future:[[388.99345798]]\nNumber of iteration:15, max_iteration:1000, past:[[388.99345798]], future:[[389.37495153]]\nNumber of iteration:16, max_iteration:1000, past:[[389.37495153]], future:[[389.77971115]]\nNumber of iteration:17, max_iteration:1000, past:[[389.77971115]], future:[[390.11814427]]\nNumber of iteration:18, max_iteration:1000, past:[[390.11814427]], future:[[390.30239295]]\nNumber of iteration:19, max_iteration:1000, past:[[390.30239295]], future:[[390.62912618]]\nNumber of iteration:20, max_iteration:1000, past:[[390.62912618]], future:[[391.20270317]]\nNumber of iteration:21, max_iteration:1000, past:[[391.20270317]], future:[[391.51022222]]\nNumber of iteration:22, max_iteration:1000, past:[[391.51022222]], future:[[391.7130091]]\nNumber of iteration:23, max_iteration:1000, past:[[391.7130091]], future:[[392.33782929]]\nNumber of iteration:24, max_iteration:1000, past:[[392.33782929]], future:[[393.38055716]]\nNumber of iteration:25, max_iteration:1000, past:[[393.38055716]], future:[[393.69091901]]\nNumber of iteration:26, max_iteration:1000, past:[[393.69091901]], future:[[393.760286]]\nNumber of iteration:27, max_iteration:1000, past:[[393.760286]], future:[[393.78911554]]\nNumber of iteration:28, max_iteration:1000, past:[[393.78911554]], future:[[393.81187915]]\nNumber of iteration:29, max_iteration:1000, past:[[393.81187915]], future:[[393.84342219]]\nNumber of iteration:30, max_iteration:1000, past:[[393.84342219]], future:[[393.91227892]]\nNumber of iteration:31, max_iteration:1000, past:[[393.91227892]], future:[[394.11292202]]\nNumber of iteration:32, max_iteration:1000, past:[[394.11292202]], future:[[394.61355415]]\nNumber of iteration:33, max_iteration:1000, past:[[394.61355415]], future:[[395.08065202]]\nNumber of iteration:34, max_iteration:1000, past:[[395.08065202]], future:[[395.238157]]\nNumber of iteration:35, max_iteration:1000, past:[[395.238157]], future:[[395.33045265]]\nNumber of iteration:36, max_iteration:1000, past:[[395.33045265]], future:[[395.43187157]]\nNumber of iteration:37, max_iteration:1000, past:[[395.43187157]], future:[[395.58094227]]\nNumber of iteration:38, max_iteration:1000, past:[[395.58094227]], future:[[395.79465888]]\nNumber of iteration:39, max_iteration:1000, past:[[395.79465888]], future:[[395.996]]\nNumber of iteration:40, max_iteration:1000, past:[[395.996]], future:[[396.10369193]]\nNumber of iteration:41, max_iteration:1000, past:[[396.10369193]], future:[[396.15910542]]\nNumber of iteration:42, max_iteration:1000, past:[[396.15910542]], future:[[396.2034703]]\nNumber of iteration:43, max_iteration:1000, past:[[396.2034703]], future:[[396.2587019]]\nNumber of iteration:44, max_iteration:1000, past:[[396.2587019]], future:[[396.36867126]]\nNumber of iteration:45, max_iteration:1000, past:[[396.36867126]], future:[[396.71341602]]\nNumber of iteration:46, max_iteration:1000, past:[[396.71341602]], future:[[397.6977378]]\nNumber of iteration:47, max_iteration:1000, past:[[397.6977378]], future:[[398.37135808]]\nNumber of iteration:48, max_iteration:1000, past:[[398.37135808]], future:[[398.46675938]]\nNumber of iteration:49, max_iteration:1000, past:[[398.46675938]], future:[[398.49579547]]\nNumber of iteration:50, max_iteration:1000, past:[[398.49579547]], future:[[398.51033774]]\nNumber of iteration:51, max_iteration:1000, past:[[398.51033774]], future:[[398.51913381]]\nNumber of iteration:52, max_iteration:1000, past:[[398.51913381]], future:[[398.5248566]]\nNumber of iteration:53, max_iteration:1000, past:[[398.5248566]], future:[[398.52869965]]\nNumber of iteration:54, max_iteration:1000, past:[[398.52869965]], future:[[398.53132763]]\nNumber of iteration:55, max_iteration:1000, past:[[398.53132763]], future:[[398.53315047]]\nNumber of iteration:56, max_iteration:1000, past:[[398.53315047]], future:[[398.53443247]]\nNumber of iteration:57, max_iteration:1000, past:[[398.53443247]], future:[[398.53534753]]\nNumber of iteration:58, max_iteration:1000, past:[[398.53534753]], future:[[398.53601131]]\nNumber of iteration:59, max_iteration:1000, past:[[398.53601131]], future:[[398.53650123]]\nNumber of iteration:60, max_iteration:1000, past:[[398.53650123]], future:[[398.53686944]]\nNumber of iteration:61, max_iteration:1000, past:[[398.53686944]], future:[[398.53715129]]\nNumber of iteration:62, max_iteration:1000, past:[[398.53715129]], future:[[398.53737092]]\nNumber of iteration:63, max_iteration:1000, past:[[398.53737092]], future:[[398.53754497]]\nNumber of iteration:64, max_iteration:1000, past:[[398.53754497]], future:[[398.53768503]]\nNumber of iteration:65, max_iteration:1000, past:[[398.53768503]], future:[[398.53779929]]\nNumber of iteration:66, max_iteration:1000, past:[[398.53779929]], future:[[398.53789362]]\nNumber of iteration:67, max_iteration:1000, past:[[398.53789362]], future:[[398.53797227]]\nNumber of iteration:68, max_iteration:1000, past:[[398.53797227]], future:[[398.53803842]]\nNumber of iteration:69, max_iteration:1000, past:[[398.53803842]], future:[[398.53809444]]\nNumber of iteration:70, max_iteration:1000, past:[[398.53809444]], future:[[398.53814216]]\nNumber of iteration:71, max_iteration:1000, past:[[398.53814216]], future:[[398.53818301]]\nNumber of iteration:72, max_iteration:1000, past:[[398.53818301]], future:[[398.5382181]]\nNumber of iteration:73, max_iteration:1000, past:[[398.5382181]], future:[[398.53824836]]\nNumber of iteration:74, max_iteration:1000, past:[[398.53824836]], future:[[398.5382745]]\nNumber of iteration:75, max_iteration:1000, past:[[398.5382745]], future:[[398.53829715]]\nNumber of iteration:76, max_iteration:1000, past:[[398.53829715]], future:[[398.5383168]]\nNumber of iteration:77, max_iteration:1000, past:[[398.5383168]], future:[[398.53833388]]\nNumber of iteration:78, max_iteration:1000, past:[[398.53833388]], future:[[398.53834874]]\nNumber of iteration:79, max_iteration:1000, past:[[398.53834874]], future:[[398.53836168]]\nNumber of iteration:80, max_iteration:1000, past:[[398.53836168]], future:[[398.53837296]]\nNumber of iteration:81, max_iteration:1000, past:[[398.53837296]], future:[[398.5383828]]\nWe converged to the optimal value for the log-likelihood\n"
    }
   ],
   "source": [
    "#Randomly Initialized\n",
    "df = [10, 10, 10]\n",
    "model = Mixtures_ofMultiVariate_Student_t_model(X_train, k, df, \"kmeans\", 10)\n",
    "means_of_each_clusters, covariance_within_each_clusters, parameters_of_mixing_latent_parameter = model.fit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the training set\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[39,  0,  5],\n       [ 2, 41, 10],\n       [ 0, 17, 19]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "pred = model.prediction_dataset(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "#print(sklearn.metrics.confusion_matrix(y_train, pred))\n",
    "c = sklearn.metrics.confusion_matrix(y_train, pred)\n",
    "c = c[:, list(np.argmax(c, axis=1))]#ordering the cluster to where it shows the highest number of matching with the true labels\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the test set\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[12,  0,  0],\n       [ 2, 14, 14],\n       [ 0,  6,  6]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "pred = model.prediction_dataset(X_test)\n",
    "print(\"Performance on the test set\")\n",
    "#print(sklearn.metrics.confusion_matrix(y_test, pred))\n",
    "c = sklearn.metrics.confusion_matrix(y_test, pred)\n",
    "c = c[:, list(np.argmax(c, axis=1))]#ordering the cluster to where it shows the highest number of matching with the true labels\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating random variables from multivariate Student-t distribution \n",
    "###https://stackoverflow.com/questions/29798795/multivariate-student-t-distribution-with-python##\n",
    "def multivariatet(mu,Sigma,dof,m):\n",
    "    '''\n",
    "    Output:\n",
    "    Produce M samples of d-dimensional multivariate t distribution\n",
    "    Input:\n",
    "    mu = mean (d dimensional numpy array or scalar)\n",
    "    Sigma = scale matrix (dxd numpy array)\n",
    "    dof = degrees of freedom\n",
    "    m = # of samples to produce\n",
    "    '''\n",
    "    d = Sigma.shape[1]\n",
    "    g = np.tile(np.random.gamma(dof/2.,2./dof,m),(d,1)).T#From https://en.wikipedia.org/wiki/Multivariate_t-distribution, and the relationship between gamma distribution and chi-squared distribution\n",
    "    #Page 4 from http://users.isy.liu.se/en/rt/roth/student.pdf\n",
    "    Z = np.random.multivariate_normal(np.zeros(d),Sigma,m)\n",
    "    return mu + Z/np.sqrt(g[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10\n10\n10\n"
    }
   ],
   "source": [
    "#Testing the model with arbitrary data set\n",
    "\n",
    "n1 = 10\n",
    "df1 = 20.5\n",
    "m1 = 200\n",
    "mean1 = np.random.randn(n1).reshape(-1, 1)\n",
    "Sigma1 = np.random.randn(n1, n1) \n",
    "Sigma1 = np.dot(Sigma1, Sigma1.T)\n",
    "#Sigma1 = 20 * np.eye(n1)\n",
    "print(np.linalg.matrix_rank(Sigma1))#Need to ensure that the cov is full rank, otherwise the inverse of the covariance wouldn't exist\n",
    "X_train1 = multivariatet(mean1.reshape(1, -1), Sigma1, df1, m1)\n",
    "ytrain1 = np.array([0]*m1).reshape(-1, 1)\n",
    "#print(X_train1.shape)\n",
    "#Check its mean \n",
    "c1 = np.mean(X_train1, axis=0)\n",
    "c2 = (df1/(df1-2)) * np.cov(X_train1.T)\n",
    "#print(\"Comparing Means\")\n",
    "#[print(f\"x1:{x1}|x2:{x2}\") for x1, x2 in  zip(mean1, c1)];\n",
    "#print(\"Comparing Covariance\")\n",
    "#[print(f\"new row\\n x1:{x1}\\nx2:{x2}\") for x1, x2 in  zip(Sigma1, c2)];\n",
    "# I am going to assume the mean and covariance that would be estimated will have the following form\n",
    "mean1 = c1\n",
    "cov1 = c2\n",
    "#################################\n",
    "n2 = 10\n",
    "df2 = 7.5\n",
    "m2 = 200\n",
    "mean2 = np.random.randn(n2).reshape(-1, 1)\n",
    "Sigma2 = np.random.randn(n2, n2) \n",
    "Sigma2 = np.dot(Sigma2, Sigma2.T)\n",
    "#Sigma2 = 60 * np.eye(n2)\n",
    "print(np.linalg.matrix_rank(Sigma2))#Need to ensure that the cov is full rank, otherwise the inverse of the covariance wouldn't exist\n",
    "X_train2 = multivariatet(mean2.reshape(1, -1), Sigma2, df2, m2)\n",
    "ytrain2 = np.array([1]*m2).reshape(-1, 1)\n",
    "#print(X_train2.shape)\n",
    "#Check its mean \n",
    "c1 = np.mean(X_train2, axis=0)\n",
    "c2 = (df2/(df2-2)) * np.cov(X_train2.T)\n",
    "#print(\"Comparing Means\")\n",
    "#[print(f\"x1:{x1}|x2:{x2}\") for x1, x2 in  zip(mean2, c1)];\n",
    "#print(\"Comparing Covariance\")\n",
    "#[print(f\"new row\\n x1:{x1}\\nx2:{x2}\") for x1, x2 in  zip(Sigma2, c2)];\n",
    "# I am going to assume the mean and covariance that would be estimated will have the following form\n",
    "mean2 = c1\n",
    "cov2 = c2\n",
    "#################################\n",
    "\n",
    "n3 = 10\n",
    "df3 = 43.5\n",
    "m3 = 200\n",
    "mean3 = np.random.randn(n3).reshape(-1, 1)\n",
    "Sigma3 = np.random.randn(n3, n3) \n",
    "Sigma3 = np.dot(Sigma3, Sigma3.T)\n",
    "#Sigma3 = 99 * np.eye(n3)\n",
    "print(np.linalg.matrix_rank(Sigma3))#Need to ensure that the cov is full rank, otherwise the inverse of the covariance wouldn't exist\n",
    "X_train3 = multivariatet(mean3.reshape(1, -1), Sigma3, df3, m3)\n",
    "ytrain3 = np.array([2]*m3).reshape(-1, 1)\n",
    "#print(X_train3.shape)\n",
    "#Check its mean \n",
    "c1 = np.mean(X_train3, axis=0)\n",
    "c2 = (df3/(df3-2)) * np.cov(X_train3.T)\n",
    "#print(\"Comparing Means\")\n",
    "#[print(f\"x1:{x1}|x2:{x2}\") for x1, x2 in  zip(mean3, c1)];\n",
    "#print(\"Comparing Covariance\")\n",
    "#[print(f\"new row\\n x1:{x1}\\nx2:{x2}\") for x1, x2 in  zip(Sigma3, c2)];\n",
    "# I am going to assume the mean and covariance that would be estimated will have the following form\n",
    "mean3 = c1\n",
    "cov3 = c2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(600, 1)"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "X_train = np.vstack([X_train1, X_train1, X_train3])\n",
    "y_train = np.vstack([ytrain1, ytrain2, ytrain3])\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "count:0, max_iterations1000, past:10, future:0\ncount:1, max_iterations1000, past:76558.59948251564, future:63042.563887490694\ncount:2, max_iterations1000, past:63042.563887490694, future:62480.342083516385\ncount:3, max_iterations1000, past:62480.342083516385, future:62267.77309549749\ncount:4, max_iterations1000, past:62267.77309549749, future:62212.105536786235\ncount:5, max_iterations1000, past:62212.105536786235, future:62086.27793319166\ncount:6, max_iterations1000, past:62086.27793319166, future:61911.96962116065\ncount:7, max_iterations1000, past:61911.96962116065, future:61720.224273176274\ncount:8, max_iterations1000, past:61720.224273176274, future:61545.25346652394\ncount:9, max_iterations1000, past:61545.25346652394, future:61383.888717597416\ncount:10, max_iterations1000, past:61383.888717597416, future:61288.44921967737\ncount:11, max_iterations1000, past:61288.44921967737, future:61092.43305767626\ncount:12, max_iterations1000, past:61092.43305767626, future:61001.46022445557\ncount:13, max_iterations1000, past:61001.46022445557, future:60931.125090661946\ncount:14, max_iterations1000, past:60931.125090661946, future:60889.827291605376\ncount:15, max_iterations1000, past:60889.827291605376, future:60838.86923064609\ncount:16, max_iterations1000, past:60838.86923064609, future:60804.60793791376\ncount:17, max_iterations1000, past:60804.60793791376, future:60794.51583276698\ncount:18, max_iterations1000, past:60794.51583276698, future:60788.5091110003\ncount:19, max_iterations1000, past:60788.5091110003, future:60784.549050247384\ncount:20, max_iterations1000, past:60784.549050247384, future:60774.521064244494\ncount:21, max_iterations1000, past:60774.521064244494, future:60736.61494970592\ncount:22, max_iterations1000, past:60736.61494970592, future:60702.09127058695\ncount:23, max_iterations1000, past:60702.09127058695, future:60646.272801658175\ncount:24, max_iterations1000, past:60646.272801658175, future:60620.127888156516\ncount:25, max_iterations1000, past:60620.127888156516, future:60589.469910987376\ncount:26, max_iterations1000, past:60589.469910987376, future:60520.758614292434\ncount:27, max_iterations1000, past:60520.758614292434, future:60484.45257429926\ncount:28, max_iterations1000, past:60484.45257429926, future:60480.94483742127\nFinished the initialzation by the kmeans\nNumber of iteration:0, max_iteration:1000, past:[[-5617.17044299]], future:[[-3733.19329753]]\nNumber of iteration:1, max_iteration:1000, past:[[-3733.19329753]], future:[[-2752.38507304]]\nNumber of iteration:2, max_iteration:1000, past:[[-2752.38507304]], future:[[-2131.41077834]]\nNumber of iteration:3, max_iteration:1000, past:[[-2131.41077834]], future:[[-1659.61948624]]\nNumber of iteration:4, max_iteration:1000, past:[[-1659.61948624]], future:[[-1114.99551653]]\nNumber of iteration:5, max_iteration:1000, past:[[-1114.99551653]], future:[[-922.69398618]]\nNumber of iteration:6, max_iteration:1000, past:[[-922.69398618]], future:[[-853.11576883]]\nNumber of iteration:7, max_iteration:1000, past:[[-853.11576883]], future:[[-784.03065187]]\nNumber of iteration:8, max_iteration:1000, past:[[-784.03065187]], future:[[-714.95712509]]\nNumber of iteration:9, max_iteration:1000, past:[[-714.95712509]], future:[[-645.88143805]]\nNumber of iteration:10, max_iteration:1000, past:[[-645.88143805]], future:[[-576.80370412]]\nNumber of iteration:11, max_iteration:1000, past:[[-576.80370412]], future:[[-507.74728702]]\nNumber of iteration:12, max_iteration:1000, past:[[-507.74728702]], future:[[-438.75292625]]\nNumber of iteration:13, max_iteration:1000, past:[[-438.75292625]], future:[[-369.14436566]]\nNumber of iteration:14, max_iteration:1000, past:[[-369.14436566]], future:[[-306.43156446]]\nNumber of iteration:15, max_iteration:1000, past:[[-306.43156446]], future:[[nan]]\nSomething wrong happened in the Maximization step\nPerformance on the training set\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[200,   0,   0],\n       [200,   0,   0],\n       [200,   0,   0]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "X_train = np.vstack([X_train1, X_train1, X_train3])\n",
    "y_train = np.vstack([ytrain1, ytrain2, ytrain3])\n",
    "choices = np.random.permutation(np.arange(0, X_train.shape[0]))\n",
    "X_train = X_train[choices, :]\n",
    "y_train = y_train[choices]\n",
    "#Randomly Initialized\n",
    "df = [20.5, 7.5, 43.5]\n",
    "#df = [30, 30 , 30]\n",
    "k=3\n",
    "model = Mixtures_ofMultiVariate_Student_t_model(X_train, k, df, \"kmeans\")\n",
    "means_of_each_clusters, covariance_within_each_clusters, parameters_of_mixing_latent_parameter = model.fit(1000)\n",
    "\n",
    "pred = model.prediction_dataset(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "#print(sklearn.metrics.confusion_matrix(y_train, pred))\n",
    "c = sklearn.metrics.confusion_matrix(y_train, pred)\n",
    "#c[:, list(np.argmax(c, axis=1))]#ordering the cluster to where it shows the highest number of matching with the true labels\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparing Means\")\n",
    "[print(f\"x1:{x1}|x2:{x2}\") for x1, x2 in  zip(mean1, means_of_each_clusters[2])];\n",
    "print(\"Comparing Covariance\")\n",
    "[print(f\"new row\\n x1:{x1}\\nx2:{x2}\") for x1, x2 in  zip(cov1, covariance_within_each_clusters[0])];\n",
    "covariance_within_each_clusters[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 2, Chapter 9 and Chapter 12 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Chapter 5 from McNicholas, P.D. (2016). Mixture Model-Based Classification. Boca Raton: Chapman &\n",
    "Hall/CRC Press.\n",
    "* McLachlan, G., and  Krishnan T. (2008). The EM Algorithm and Extensions, Second Edition. New York: Wiley."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}