{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines for Classification\n",
    "### Hard Margin (SVM without regularization)\n",
    "\n",
    "### Soft Margin (SVM with regularization)\n",
    "\n",
    "### Simplified Sequential Minimal Optimization (SMO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(426, 31)\n(143, 31)\n"
    }
   ],
   "source": [
    "X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "X_train = standard.fit_transform(X_train)\n",
    "training_data = pd.DataFrame(np.c_[X_train, y_train])#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "y_train = np.array(list(map(lambda y: 1 if y == 1 else -1, y_train)))# y must be either 1 or -1 \n",
    "\n",
    "X_test = standard.transform(X_test)\n",
    "y_test = np.array(list(map(lambda y: 1 if y == 1 else -1, y_test)))# y must be either 1 or -1 \n",
    "test_data = pd.DataFrame(np.c_[X_test, y_test])\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)\n",
    "#training_data.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM2CLASS(object):\n",
    "\n",
    "    def __init__(self, X_train, y_train, C = 10, tol = 0.001, max_passes = 5, passes = 0):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.max_passes = max_passes\n",
    "        self.passes = passes\n",
    "        self.b = 0\n",
    "        self.alphas = np.zeros((self.X_train.shape[0], 1))#need to be of size nx1\n",
    "        self.coefficients = ()\n",
    "        self.kernel_type ={\"poly\": self.polynomial_kernel, \"gauss\": self.gaussian_kernel}\n",
    "        self.kernel_parameters = []\n",
    "        self.kernel_choice = \"\"\n",
    "\n",
    "    def polynomial_kernel(self, x_i, x_j, a):\n",
    "        return np.power(np.dot(x_i.T, x_j) + a[0], a[1])\n",
    "\n",
    "    def gaussian_kernel(self, x, x_star, sigma):\n",
    "        return np.exp(np.divide(-1*(np.linalg.norm(x-x_star)**2), 2*sigma**2))\n",
    "    \n",
    "    def SMO(self, kernel_choice, parameters):\n",
    "\n",
    "        choices = np.arange(0, self.y_train.shape[0])\n",
    "        count = 0\n",
    "        self.kernel_choice = kernel_choice\n",
    "        #Better to construct the Kernel matrix from the scratch for efficiency, but this method will prevent the simplified SMO from working on large datasets, like, the mnist dataset \n",
    "        if kernel_choice == \"poly\":\n",
    "            exponent = parameters[0]\n",
    "            intercept = parameters[1]\n",
    "            self.kernel_parameters = [intercept, exponent] \n",
    "            \n",
    "            K = np.zeros((self.X_train.shape[0], self.X_train.shape[0]))\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "                for j in range(0, self.X_train.shape[0]):\n",
    "                    K[i, j] = self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "            assert(np.any(np.linalg.eig(K)[0] == 0)  == False)#Test for PSD\n",
    "\n",
    "        elif kernel_choice == \"gauss\":\n",
    "            self.kernel_parameters = 2\n",
    "\n",
    "            K = np.zeros((self.X_train.shape[0], self.X_train.shape[0]))\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "                for j in range(0, self.X_train.shape[0]):\n",
    "                    K[i, j] =  self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "            assert(np.linalg.det(K) != 0)#Test for PSD\n",
    "        \n",
    "        else:\n",
    "            print(\"Wrong entry\")\n",
    "            return -1\n",
    "\n",
    "        while(self.passes <= self.max_passes):\n",
    "        #begin while\n",
    "            num_changed_alphas = 0\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "            #begin for\n",
    "                #compute kernel function for every iteration from scratch\n",
    "                #f_x_i = np.sum( list(map(lambda x, alpha, y: alpha * y * self.kernel_type[self.kernel_choice](x, self.X_train[i, :], self.kernel_parameters) , self.X_train, self.alphas, self.y_train) ) )#instead of calculating the Kernel matrix from the start, we will calculate the inner product with each iteration in order to mitigate the problem of having a large kernel matrix that will raise an exception\n",
    "                #print(f_x_i)\n",
    "                f_x_i = np.sum(self.alphas.reshape(-1, 1) * (self.y_train.reshape(-1, 1) *  K[:, i].reshape(-1, 1)).reshape(-1, 1)) \n",
    "                E_i = f_x_i + self.b - self.y_train[i]\n",
    "                #print(f_x_i)\n",
    "                #Check if we satisfy the condition for the dual problem\n",
    "                if (((self.y_train[i] * E_i) < -self.tol) and (self.alphas[i] < self.C)) or (((self.y_train[i] * E_i) > self.tol) and (self.alphas[i] > 0)):\n",
    "                #begin if\n",
    "                    j = np.random.choice( list(filter(lambda v: v == v, list(map(lambda c: c if c != i else np.nan, choices)))) ) \n",
    "                    #only nan will generate False at its equlaity, and the filter object will end up filtering out these wrong values\n",
    "                    assert( i != j)\n",
    "                    #f_x_j = np.sum( list(map(lambda x, alpha, y: alpha * y * self.kernel_type[self.kernel_choice](x, self.X_train[j, :], self.kernel_parameters) , self.X_train, self.alphas, self.y_train) ) )\n",
    "                    #print(f_x_j)\n",
    "                    f_x_j = np.sum(self.alphas.reshape(-1, 1) * (self.y_train.reshape(-1, 1) * K[:, j].reshape(-1, 1)).reshape(-1, 1)) \n",
    "                    #print((alphas.reshape(-1, 1) * (y_train.reshape(-1, 1) * K[i, :].reshape(-1, 1)).reshape(-1, 1)).shape)\n",
    "                    #print(f_x_j)\n",
    "                    E_j = f_x_j + self.b - self.y_train[j]\n",
    "\n",
    "                    alpha_i_old = self.alphas[i].copy()#Needs to copy the value because otherwise they would be pointing to the same address\n",
    "                    alpha_j_old = self.alphas[j].copy()\n",
    "\n",
    "                    #Computing L and H\n",
    "                    if(self.y_train[i] != self.y_train[j]):\n",
    "                        L = max(0, self.alphas[j] - self.alphas[i])\n",
    "                        H = min(self.C, self.C + self.alphas[j] - self.alphas[i])\n",
    "                    else:\n",
    "                        L = max(0, self.alphas[j] + self.alphas[i] - self.C)\n",
    "                        H = min(self.C, self.alphas[j] + self.alphas[i])\n",
    "                    \n",
    "                    #Checking if L=H which indicate that the alpha would certainly wouldn't change \n",
    "                    if L == H:\n",
    "                        continue\n",
    "\n",
    "                    #eta = 2 * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters) - self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[i, :], self.kernel_parameters) - self.kernel_type[self.kernel_choice](self.X_train[j, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    eta = 2 * K[i, j] - K[i, i] - K[j, j] \n",
    "\n",
    "                    #eta = 0 if the similarity between x_i and x_j is as the combination of the similarity of x_i with itself and same goes for x_j, will cause an exception to happend and this indicate we are dealing with the same x.\n",
    "                    #eta > 0 if if the similarity between x_i and x_j is higher than the combination of the similarity of x_i with itself and same goes for x_j, so, this update step would have little effect on the converging to the optimal minimum and may leads to diverging the algorithm patht to a worse path\n",
    "                    #eta < 0 there are small simialrity between x_i and x_j, so, this would help in discoverign the interaction of those observations in the feature space\n",
    "\n",
    "                    if eta >= 0:\n",
    "                        #print(\"The two vectors are too similar\")\n",
    "                        continue\n",
    "\n",
    "                    alpha_j_clip = alpha_j_old - (1/eta) * self.y_train[j] * (E_i - E_j)\n",
    "\n",
    "                    if alpha_j_clip > H:\n",
    "                        self.alphas[j] = H\n",
    "                    elif alpha_j_clip < L:\n",
    "                        self.alphas[j] = L\n",
    "                    else:\n",
    "                        self.alphas[j]  = alpha_j_clip\n",
    "                    \n",
    "                    #print(alphas[j], alpha_j_old)\n",
    "                    #Check if it is worth to update alpha_i\n",
    "                    if(abs(self.alphas[j] - alpha_j_old) < 1e-3):\n",
    "                        #print(\"No noticeable changes happened to alpha\")\n",
    "                        continue\n",
    "                    self.alphas[i] = alpha_i_old + self.y_train[i] * self.y_train[j] * (alpha_j_old - self.alphas[j])#The signs changed from  the negative sign for updating alpha_i\n",
    "\n",
    "                    ##KKT constrains convergence test\n",
    "                    #b1 = self.b - E_i - self.y_train[i]*(self.alphas[i] - alpha_i_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[i, :], self.kernel_parameters) - self.y_train[j]*(self.alphas[j] - alpha_j_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    b1 = self.b - E_i - self.y_train[i] * (self.alphas[i] - alpha_i_old) * K[i, i] - self.y_train[j] * (self.alphas[j] - alpha_j_old) * K[i, j]\n",
    "\n",
    "                    #b2 = self.b - E_j - self.y_train[i]*(self.alphas[i] - alpha_i_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters) - self.y_train[j]*(self.alphas[j] - alpha_j_old) * self.kernel_type[self.kernel_choice](self.X_train[j, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    b2 = self.b - E_j - self.y_train[i] * (self.alphas[i] - alpha_i_old) * K[i, j] - self.y_train[j] * (self.alphas[j] - alpha_j_old) * K[j, j]\n",
    "\n",
    "                    if (self.alphas[j] > 0) and (self.alphas[j] < self.C):\n",
    "                        self.b = b2\n",
    "                    elif (self.alphas[i] > 0) and (self.alphas[i] < self.C):\n",
    "                        self.b = b1\n",
    "                    else:\n",
    "                        self.b = (b1 + b2)/2\n",
    "                    \n",
    "                    num_changed_alphas  =  num_changed_alphas + 1\n",
    "                #end if\n",
    "            #end for\n",
    "            print(f\"count:{count}, passes:{self.passes}, max_passes:{self.max_passes}, b:{self.b}\")\n",
    "            count+=1\n",
    "            if(num_changed_alphas == 0):\n",
    "                self.passes = self.passes + 1\n",
    "            else:\n",
    "                self.passes = 0\n",
    "        #end while\n",
    "        #Only store in the memory the support vectors\n",
    "        support_indeces = np.argwhere(self.alphas != 0)[:, 0]\n",
    "        support_vectors = self.X_train[support_indeces, :]\n",
    "        support_alphas = self.alphas[support_indeces]\n",
    "        support_target = self.y_train[support_indeces]\n",
    "        self.importantParameters = (support_vectors, support_alphas, support_target)\n",
    "\n",
    "        return self.importantParameters\n",
    "\n",
    "    def prediction_dataset(self, X):\n",
    "        pred = list(map(lambda x: self.prediction(x), X))\n",
    "        return pred\n",
    "\n",
    "    def prediction(self, x):\n",
    "        t1 = np.sum( list(map(lambda x1, alpha, y: y * alpha * self.kernel_type[self.kernel_choice](x, x1, self.kernel_parameters), self.importantParameters[0], self.importantParameters[1], self.importantParameters[2])) )\n",
    "        pred = t1 + self.b\n",
    "        #print(pred)\n",
    "        if pred >=0:\n",
    "            return 1\n",
    "    \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x_passes:5, b:[-0.20433877]\ncount:3206, passes:1, max_passes:5, b:[-0.24635392]\ncount:3207, passes:0, max_passes:5, b:[-0.13738654]\ncount:3208, passes:0, max_passes:5, b:[-0.3165929]\ncount:3209, passes:0, max_passes:5, b:[-0.3165929]\ncount:3210, passes:1, max_passes:5, b:[-0.1693733]\ncount:3211, passes:0, max_passes:5, b:[-0.26507275]\ncount:3212, passes:0, max_passes:5, b:[-0.18503334]\ncount:3213, passes:0, max_passes:5, b:[-0.18131992]\ncount:3214, passes:0, max_passes:5, b:[-0.18131992]\ncount:3215, passes:1, max_passes:5, b:[-0.18888422]\ncount:3216, passes:0, max_passes:5, b:[-0.09446068]\ncount:3217, passes:0, max_passes:5, b:[-0.25864963]\ncount:3218, passes:0, max_passes:5, b:[-0.18851382]\ncount:3219, passes:0, max_passes:5, b:[-0.20299234]\ncount:3220, passes:0, max_passes:5, b:[-0.21216593]\ncount:3221, passes:0, max_passes:5, b:[-0.21216593]\ncount:3222, passes:1, max_passes:5, b:[-0.21216593]\ncount:3223, passes:2, max_passes:5, b:[-0.17643861]\ncount:3224, passes:0, max_passes:5, b:[-0.17643861]\ncount:3225, passes:1, max_passes:5, b:[-0.10477292]\ncount:3226, passes:0, max_passes:5, b:[-0.10477292]\ncount:3227, passes:1, max_passes:5, b:[-0.18181299]\ncount:3228, passes:0, max_passes:5, b:[-0.18181299]\ncount:3229, passes:1, max_passes:5, b:[-0.22227027]\ncount:3230, passes:0, max_passes:5, b:[-0.16229118]\ncount:3231, passes:0, max_passes:5, b:[-0.16110386]\ncount:3232, passes:0, max_passes:5, b:[-0.29812867]\ncount:3233, passes:0, max_passes:5, b:[-0.32189433]\ncount:3234, passes:0, max_passes:5, b:[-0.23446047]\ncount:3235, passes:0, max_passes:5, b:[-0.17886966]\ncount:3236, passes:0, max_passes:5, b:[-0.17886966]\ncount:3237, passes:1, max_passes:5, b:[-0.14530949]\ncount:3238, passes:0, max_passes:5, b:[-0.09537165]\ncount:3239, passes:0, max_passes:5, b:[-0.16767451]\ncount:3240, passes:0, max_passes:5, b:[-0.22587301]\ncount:3241, passes:0, max_passes:5, b:[-0.22587301]\ncount:3242, passes:1, max_passes:5, b:[-0.36324688]\ncount:3243, passes:0, max_passes:5, b:[-0.06654075]\ncount:3244, passes:0, max_passes:5, b:[-0.19900182]\ncount:3245, passes:0, max_passes:5, b:[-0.19900182]\ncount:3246, passes:1, max_passes:5, b:[-0.19900182]\ncount:3247, passes:2, max_passes:5, b:[-0.13107458]\ncount:3248, passes:0, max_passes:5, b:[-0.16555555]\ncount:3249, passes:0, max_passes:5, b:[-0.16555555]\ncount:3250, passes:1, max_passes:5, b:[-0.19270593]\ncount:3251, passes:0, max_passes:5, b:[-0.19270593]\ncount:3252, passes:1, max_passes:5, b:[-0.27496533]\ncount:3253, passes:0, max_passes:5, b:[-0.21662697]\ncount:3254, passes:0, max_passes:5, b:[-0.22603364]\ncount:3255, passes:0, max_passes:5, b:[-0.22603364]\ncount:3256, passes:1, max_passes:5, b:[-0.12625139]\ncount:3257, passes:0, max_passes:5, b:[-0.12625139]\ncount:3258, passes:1, max_passes:5, b:[-0.1401288]\ncount:3259, passes:0, max_passes:5, b:[-0.27105133]\ncount:3260, passes:0, max_passes:5, b:[-0.25733656]\ncount:3261, passes:0, max_passes:5, b:[-0.20418274]\ncount:3262, passes:0, max_passes:5, b:[-0.20418274]\ncount:3263, passes:1, max_passes:5, b:[-0.27068749]\ncount:3264, passes:0, max_passes:5, b:[-0.27068749]\ncount:3265, passes:1, max_passes:5, b:[-0.27068749]\ncount:3266, passes:2, max_passes:5, b:[-0.27068749]\ncount:3267, passes:3, max_passes:5, b:[-0.24834163]\ncount:3268, passes:0, max_passes:5, b:[-0.24834163]\ncount:3269, passes:1, max_passes:5, b:[-0.18841521]\ncount:3270, passes:0, max_passes:5, b:[-0.18841521]\ncount:3271, passes:1, max_passes:5, b:[-0.18841521]\ncount:3272, passes:2, max_passes:5, b:[-0.20216193]\ncount:3273, passes:0, max_passes:5, b:[-0.18653324]\ncount:3274, passes:0, max_passes:5, b:[-0.18653324]\ncount:3275, passes:1, max_passes:5, b:[-0.17452806]\ncount:3276, passes:0, max_passes:5, b:[-0.17022412]\ncount:3277, passes:0, max_passes:5, b:[-0.17022412]\ncount:3278, passes:1, max_passes:5, b:[-0.17022412]\ncount:3279, passes:2, max_passes:5, b:[-0.15368059]\ncount:3280, passes:0, max_passes:5, b:[-0.15368059]\ncount:3281, passes:1, max_passes:5, b:[-0.15368059]\ncount:3282, passes:2, max_passes:5, b:[-0.16423279]\ncount:3283, passes:0, max_passes:5, b:[-0.16423279]\ncount:3284, passes:1, max_passes:5, b:[-0.23341507]\ncount:3285, passes:0, max_passes:5, b:[-0.23341507]\ncount:3286, passes:1, max_passes:5, b:[-0.15538234]\ncount:3287, passes:0, max_passes:5, b:[-0.0891534]\ncount:3288, passes:0, max_passes:5, b:[-0.3393789]\ncount:3289, passes:0, max_passes:5, b:[-0.21939795]\ncount:3290, passes:0, max_passes:5, b:[-0.18851943]\ncount:3291, passes:0, max_passes:5, b:[-0.19443759]\ncount:3292, passes:0, max_passes:5, b:[-0.19443759]\ncount:3293, passes:1, max_passes:5, b:[-0.1947908]\ncount:3294, passes:0, max_passes:5, b:[-0.10912122]\ncount:3295, passes:0, max_passes:5, b:[-0.2466104]\ncount:3296, passes:0, max_passes:5, b:[-0.2466104]\ncount:3297, passes:1, max_passes:5, b:[-0.26844934]\ncount:3298, passes:0, max_passes:5, b:[-0.22108605]\ncount:3299, passes:0, max_passes:5, b:[-0.15285733]\ncount:3300, passes:0, max_passes:5, b:[-0.25587335]\ncount:3301, passes:0, max_passes:5, b:[-0.14570794]\ncount:3302, passes:0, max_passes:5, b:[-0.14570794]\ncount:3303, passes:1, max_passes:5, b:[-0.22052645]\ncount:3304, passes:0, max_passes:5, b:[-0.22052645]\ncount:3305, passes:1, max_passes:5, b:[-0.27157857]\ncount:3306, passes:0, max_passes:5, b:[-0.26921332]\ncount:3307, passes:0, max_passes:5, b:[-0.12153885]\ncount:3308, passes:0, max_passes:5, b:[-0.20786143]\ncount:3309, passes:0, max_passes:5, b:[-0.19438856]\ncount:3310, passes:0, max_passes:5, b:[-0.26992415]\ncount:3311, passes:0, max_passes:5, b:[-0.21454593]\ncount:3312, passes:0, max_passes:5, b:[-0.21454593]\ncount:3313, passes:1, max_passes:5, b:[-0.18963556]\ncount:3314, passes:0, max_passes:5, b:[-0.14331849]\ncount:3315, passes:0, max_passes:5, b:[-0.19259941]\ncount:3316, passes:0, max_passes:5, b:[-0.16424028]\ncount:3317, passes:0, max_passes:5, b:[-0.16424028]\ncount:3318, passes:1, max_passes:5, b:[-0.26168651]\ncount:3319, passes:0, max_passes:5, b:[-0.26168651]\ncount:3320, passes:1, max_passes:5, b:[-0.26168651]\ncount:3321, passes:2, max_passes:5, b:[-0.08452283]\ncount:3322, passes:0, max_passes:5, b:[-0.22158626]\ncount:3323, passes:0, max_passes:5, b:[-0.30386392]\ncount:3324, passes:0, max_passes:5, b:[-0.1753901]\ncount:3325, passes:0, max_passes:5, b:[-0.27543777]\ncount:3326, passes:0, max_passes:5, b:[-0.2529422]\ncount:3327, passes:0, max_passes:5, b:[-0.2529422]\ncount:3328, passes:1, max_passes:5, b:[-0.17536413]\ncount:3329, passes:0, max_passes:5, b:[-0.17536413]\ncount:3330, passes:1, max_passes:5, b:[-0.17536413]\ncount:3331, passes:2, max_passes:5, b:[-0.17536413]\ncount:3332, passes:3, max_passes:5, b:[-0.17536413]\ncount:3333, passes:4, max_passes:5, b:[-0.3216284]\ncount:3334, passes:0, max_passes:5, b:[-0.18878873]\ncount:3335, passes:0, max_passes:5, b:[-0.18878873]\ncount:3336, passes:1, max_passes:5, b:[-0.19460631]\ncount:3337, passes:0, max_passes:5, b:[-0.17751309]\ncount:3338, passes:0, max_passes:5, b:[-0.19852393]\ncount:3339, passes:0, max_passes:5, b:[-0.11680881]\ncount:3340, passes:0, max_passes:5, b:[-0.23551352]\ncount:3341, passes:0, max_passes:5, b:[-0.0990979]\ncount:3342, passes:0, max_passes:5, b:[-0.18863238]\ncount:3343, passes:0, max_passes:5, b:[-0.18269201]\ncount:3344, passes:0, max_passes:5, b:[-0.16395234]\ncount:3345, passes:0, max_passes:5, b:[-0.14825064]\ncount:3346, passes:0, max_passes:5, b:[-0.14825064]\ncount:3347, passes:1, max_passes:5, b:[-0.19914287]\ncount:3348, passes:0, max_passes:5, b:[-0.1949754]\ncount:3349, passes:0, max_passes:5, b:[-0.1949754]\ncount:3350, passes:1, max_passes:5, b:[-0.21132521]\ncount:3351, passes:0, max_passes:5, b:[-0.23745911]\ncount:3352, passes:0, max_passes:5, b:[-0.16139248]\ncount:3353, passes:0, max_passes:5, b:[-0.26293511]\ncount:3354, passes:0, max_passes:5, b:[-0.11119403]\ncount:3355, passes:0, max_passes:5, b:[-0.22277163]\ncount:3356, passes:0, max_passes:5, b:[-0.14347168]\ncount:3357, passes:0, max_passes:5, b:[-0.11288902]\ncount:3358, passes:0, max_passes:5, b:[-0.21402591]\ncount:3359, passes:0, max_passes:5, b:[-0.20525943]\ncount:3360, passes:0, max_passes:5, b:[-0.20525943]\ncount:3361, passes:1, max_passes:5, b:[-0.13086612]\ncount:3362, passes:0, max_passes:5, b:[-0.19048803]\ncount:3363, passes:0, max_passes:5, b:[-0.19048803]\ncount:3364, passes:1, max_passes:5, b:[-0.15773444]\ncount:3365, passes:0, max_passes:5, b:[-0.17210396]\ncount:3366, passes:0, max_passes:5, b:[-0.34313293]\ncount:3367, passes:0, max_passes:5, b:[-0.25682998]\ncount:3368, passes:0, max_passes:5, b:[-0.25682998]\ncount:3369, passes:1, max_passes:5, b:[-0.25682998]\ncount:3370, passes:2, max_passes:5, b:[-0.19553991]\ncount:3371, passes:0, max_passes:5, b:[-0.15572574]\ncount:3372, passes:0, max_passes:5, b:[-0.15572574]\ncount:3373, passes:1, max_passes:5, b:[-0.15572574]\ncount:3374, passes:2, max_passes:5, b:[-0.15572574]\ncount:3375, passes:3, max_passes:5, b:[-0.1374611]\ncount:3376, passes:0, max_passes:5, b:[-0.1374611]\ncount:3377, passes:1, max_passes:5, b:[-0.20479269]\ncount:3378, passes:0, max_passes:5, b:[-0.14186435]\ncount:3379, passes:0, max_passes:5, b:[-0.21050541]\ncount:3380, passes:0, max_passes:5, b:[-0.26627167]\ncount:3381, passes:0, max_passes:5, b:[-0.15605679]\ncount:3382, passes:0, max_passes:5, b:[-0.1184937]\ncount:3383, passes:0, max_passes:5, b:[-0.1184937]\ncount:3384, passes:1, max_passes:5, b:[-0.1184937]\ncount:3385, passes:2, max_passes:5, b:[-0.1184937]\ncount:3386, passes:3, max_passes:5, b:[-0.17123146]\ncount:3387, passes:0, max_passes:5, b:[-0.23877524]\ncount:3388, passes:0, max_passes:5, b:[-0.15891247]\ncount:3389, passes:0, max_passes:5, b:[-0.24894306]\ncount:3390, passes:0, max_passes:5, b:[-0.18017367]\ncount:3391, passes:0, max_passes:5, b:[-0.24748473]\ncount:3392, passes:0, max_passes:5, b:[-0.24748473]\ncount:3393, passes:1, max_passes:5, b:[-0.24748473]\ncount:3394, passes:2, max_passes:5, b:[-0.19147919]\ncount:3395, passes:0, max_passes:5, b:[-0.19147919]\ncount:3396, passes:1, max_passes:5, b:[-0.20099825]\ncount:3397, passes:0, max_passes:5, b:[-0.20099825]\ncount:3398, passes:1, max_passes:5, b:[-0.16397343]\ncount:3399, passes:0, max_passes:5, b:[-0.26353588]\ncount:3400, passes:0, max_passes:5, b:[-0.16360668]\ncount:3401, passes:0, max_passes:5, b:[-0.16360668]\ncount:3402, passes:1, max_passes:5, b:[-0.16360668]\ncount:3403, passes:2, max_passes:5, b:[-0.16360668]\ncount:3404, passes:3, max_passes:5, b:[-0.15238674]\ncount:3405, passes:0, max_passes:5, b:[-0.2498068]\ncount:3406, passes:0, max_passes:5, b:[-0.32199297]\ncount:3407, passes:0, max_passes:5, b:[-0.32199297]\ncount:3408, passes:1, max_passes:5, b:[-0.18264746]\ncount:3409, passes:0, max_passes:5, b:[-0.18264746]\ncount:3410, passes:1, max_passes:5, b:[-0.15156555]\ncount:3411, passes:0, max_passes:5, b:[-0.28702052]\ncount:3412, passes:0, max_passes:5, b:[-0.17175983]\ncount:3413, passes:0, max_passes:5, b:[-0.17175983]\ncount:3414, passes:1, max_passes:5, b:[-0.16742755]\ncount:3415, passes:0, max_passes:5, b:[-0.27221005]\ncount:3416, passes:0, max_passes:5, b:[-0.16541884]\ncount:3417, passes:0, max_passes:5, b:[-0.22049703]\ncount:3418, passes:0, max_passes:5, b:[-0.22049703]\ncount:3419, passes:1, max_passes:5, b:[-0.18547206]\ncount:3420, passes:0, max_passes:5, b:[-0.18547206]\ncount:3421, passes:1, max_passes:5, b:[-0.20783107]\ncount:3422, passes:0, max_passes:5, b:[-0.14164346]\ncount:3423, passes:0, max_passes:5, b:[-0.14164346]\ncount:3424, passes:1, max_passes:5, b:[-0.37134957]\ncount:3425, passes:0, max_passes:5, b:[-0.27012716]\ncount:3426, passes:0, max_passes:5, b:[-0.20920078]\ncount:3427, passes:0, max_passes:5, b:[-0.19630163]\ncount:3428, passes:0, max_passes:5, b:[-0.19630163]\ncount:3429, passes:1, max_passes:5, b:[-0.09595227]\ncount:3430, passes:0, max_passes:5, b:[-0.09595227]\ncount:3431, passes:1, max_passes:5, b:[-0.09595227]\ncount:3432, passes:2, max_passes:5, b:[-0.09595227]\ncount:3433, passes:3, max_passes:5, b:[-0.23964651]\ncount:3434, passes:0, max_passes:5, b:[-0.28404649]\ncount:3435, passes:0, max_passes:5, b:[-0.28404649]\ncount:3436, passes:1, max_passes:5, b:[-0.2685006]\ncount:3437, passes:0, max_passes:5, b:[-0.28165813]\ncount:3438, passes:0, max_passes:5, b:[-0.28165813]\ncount:3439, passes:1, max_passes:5, b:[-0.26230002]\ncount:3440, passes:0, max_passes:5, b:[-0.15156558]\ncount:3441, passes:0, max_passes:5, b:[-0.21476406]\ncount:3442, passes:0, max_passes:5, b:[-0.22688192]\ncount:3443, passes:0, max_passes:5, b:[-0.25931248]\ncount:3444, passes:0, max_passes:5, b:[-0.20312663]\ncount:3445, passes:0, max_passes:5, b:[-0.20312663]\ncount:3446, passes:1, max_passes:5, b:[-0.20570139]\ncount:3447, passes:0, max_passes:5, b:[-0.13290736]\ncount:3448, passes:0, max_passes:5, b:[-0.22269264]\ncount:3449, passes:0, max_passes:5, b:[-0.20783098]\ncount:3450, passes:0, max_passes:5, b:[-0.22797798]\ncount:3451, passes:0, max_passes:5, b:[-0.22797798]\ncount:3452, passes:1, max_passes:5, b:[-0.18925011]\ncount:3453, passes:0, max_passes:5, b:[-0.09278616]\ncount:3454, passes:0, max_passes:5, b:[-0.14971959]\ncount:3455, passes:0, max_passes:5, b:[-0.18313771]\ncount:3456, passes:0, max_passes:5, b:[-0.26821854]\ncount:3457, passes:0, max_passes:5, b:[-0.26821854]\ncount:3458, passes:1, max_passes:5, b:[-0.26821854]\ncount:3459, passes:2, max_passes:5, b:[-0.15749195]\ncount:3460, passes:0, max_passes:5, b:[-0.29180956]\ncount:3461, passes:0, max_passes:5, b:[-0.23072201]\ncount:3462, passes:0, max_passes:5, b:[-0.09576295]\ncount:3463, passes:0, max_passes:5, b:[-0.15291674]\ncount:3464, passes:0, max_passes:5, b:[-0.13873801]\ncount:3465, passes:0, max_passes:5, b:[-0.13873801]\ncount:3466, passes:1, max_passes:5, b:[-0.16794538]\ncount:3467, passes:0, max_passes:5, b:[-0.16794538]\ncount:3468, passes:1, max_passes:5, b:[-0.13024463]\ncount:3469, passes:0, max_passes:5, b:[-0.26020191]\ncount:3470, passes:0, max_passes:5, b:[-0.36700866]\ncount:3471, passes:0, max_passes:5, b:[-0.20277685]\ncount:3472, passes:0, max_passes:5, b:[-0.23519952]\ncount:3473, passes:0, max_passes:5, b:[-0.15507763]\ncount:3474, passes:0, max_passes:5, b:[-0.19309396]\ncount:3475, passes:0, max_passes:5, b:[-0.23609417]\ncount:3476, passes:0, max_passes:5, b:[-0.18108792]\ncount:3477, passes:0, max_passes:5, b:[-0.18108792]\ncount:3478, passes:1, max_passes:5, b:[-0.14543703]\ncount:3479, passes:0, max_passes:5, b:[-0.21838526]\ncount:3480, passes:0, max_passes:5, b:[-0.17560595]\ncount:3481, passes:0, max_passes:5, b:[-0.17560595]\ncount:3482, passes:1, max_passes:5, b:[-0.17560595]\ncount:3483, passes:2, max_passes:5, b:[-0.15429283]\ncount:3484, passes:0, max_passes:5, b:[-0.12414964]\ncount:3485, passes:0, max_passes:5, b:[-0.25775781]\ncount:3486, passes:0, max_passes:5, b:[-0.1645949]\ncount:3487, passes:0, max_passes:5, b:[-0.09098588]\ncount:3488, passes:0, max_passes:5, b:[-0.13649633]\ncount:3489, passes:0, max_passes:5, b:[-0.13649633]\ncount:3490, passes:1, max_passes:5, b:[-0.19100042]\ncount:3491, passes:0, max_passes:5, b:[-0.18366102]\ncount:3492, passes:0, max_passes:5, b:[-0.18366102]\ncount:3493, passes:1, max_passes:5, b:[-0.22135889]\ncount:3494, passes:0, max_passes:5, b:[-0.31169423]\ncount:3495, passes:0, max_passes:5, b:[-0.24030252]\ncount:3496, passes:0, max_passes:5, b:[-0.21579901]\ncount:3497, passes:0, max_passes:5, b:[-0.21579901]\ncount:3498, passes:1, max_passes:5, b:[-0.21579901]\ncount:3499, passes:2, max_passes:5, b:[-0.16003337]\ncount:3500, passes:0, max_passes:5, b:[-0.10572865]\ncount:3501, passes:0, max_passes:5, b:[-0.18088627]\ncount:3502, passes:0, max_passes:5, b:[-0.18088627]\ncount:3503, passes:1, max_passes:5, b:[-0.14665137]\ncount:3504, passes:0, max_passes:5, b:[-0.14665137]\ncount:3505, passes:1, max_passes:5, b:[-0.2629784]\ncount:3506, passes:0, max_passes:5, b:[-0.1962759]\ncount:3507, passes:0, max_passes:5, b:[-0.19161926]\ncount:3508, passes:0, max_passes:5, b:[-0.153932]\ncount:3509, passes:0, max_passes:5, b:[-0.153932]\ncount:3510, passes:1, max_passes:5, b:[-0.153932]\ncount:3511, passes:2, max_passes:5, b:[-0.15569445]\ncount:3512, passes:0, max_passes:5, b:[-0.15569445]\ncount:3513, passes:1, max_passes:5, b:[-0.15569445]\ncount:3514, passes:2, max_passes:5, b:[-0.15569445]\ncount:3515, passes:3, max_passes:5, b:[-0.14905509]\ncount:3516, passes:0, max_passes:5, b:[-0.21716523]\ncount:3517, passes:0, max_passes:5, b:[-0.18215714]\ncount:3518, passes:0, max_passes:5, b:[-0.18215714]\ncount:3519, passes:1, max_passes:5, b:[-0.19485309]\ncount:3520, passes:0, max_passes:5, b:[-0.07973279]\ncount:3521, passes:0, max_passes:5, b:[-0.11650512]\ncount:3522, passes:0, max_passes:5, b:[-0.08217355]\ncount:3523, passes:0, max_passes:5, b:[-0.12794191]\ncount:3524, passes:0, max_passes:5, b:[-0.12794191]\ncount:3525, passes:1, max_passes:5, b:[-0.19896955]\ncount:3526, passes:0, max_passes:5, b:[-0.10901724]\ncount:3527, passes:0, max_passes:5, b:[-0.10901724]\ncount:3528, passes:1, max_passes:5, b:[-0.17312402]\ncount:3529, passes:0, max_passes:5, b:[-0.21277006]\ncount:3530, passes:0, max_passes:5, b:[-0.18783061]\ncount:3531, passes:0, max_passes:5, b:[-0.13253474]\ncount:3532, passes:0, max_passes:5, b:[-0.17940589]\ncount:3533, passes:0, max_passes:5, b:[-0.17940589]\ncount:3534, passes:1, max_passes:5, b:[-0.16037707]\ncount:3535, passes:0, max_passes:5, b:[-0.1817748]\ncount:3536, passes:0, max_passes:5, b:[-0.1817748]\ncount:3537, passes:1, max_passes:5, b:[-0.1817748]\ncount:3538, passes:2, max_passes:5, b:[-0.1817748]\ncount:3539, passes:3, max_passes:5, b:[-0.1817748]\ncount:3540, passes:4, max_passes:5, b:[-0.1817748]\ncount:3541, passes:5, max_passes:5, b:[-0.1276834]\ncount:3542, passes:0, max_passes:5, b:[-0.12865341]\ncount:3543, passes:0, max_passes:5, b:[-0.19589294]\ncount:3544, passes:0, max_passes:5, b:[-0.19589294]\ncount:3545, passes:1, max_passes:5, b:[-0.19589294]\ncount:3546, passes:2, max_passes:5, b:[-0.19589294]\ncount:3547, passes:3, max_passes:5, b:[-0.23207549]\ncount:3548, passes:0, max_passes:5, b:[-0.23207549]\ncount:3549, passes:1, max_passes:5, b:[-0.14193858]\ncount:3550, passes:0, max_passes:5, b:[-0.20726236]\ncount:3551, passes:0, max_passes:5, b:[-0.18087464]\ncount:3552, passes:0, max_passes:5, b:[-0.18087464]\ncount:3553, passes:1, max_passes:5, b:[-0.21505631]\ncount:3554, passes:0, max_passes:5, b:[-0.192404]\ncount:3555, passes:0, max_passes:5, b:[-0.22800215]\ncount:3556, passes:0, max_passes:5, b:[-0.15613905]\ncount:3557, passes:0, max_passes:5, b:[-0.20433656]\ncount:3558, passes:0, max_passes:5, b:[-0.20433656]\ncount:3559, passes:1, max_passes:5, b:[-0.15769279]\ncount:3560, passes:0, max_passes:5, b:[-0.15769279]\ncount:3561, passes:1, max_passes:5, b:[-0.1641712]\ncount:3562, passes:0, max_passes:5, b:[-0.1641712]\ncount:3563, passes:1, max_passes:5, b:[-0.20334277]\ncount:3564, passes:0, max_passes:5, b:[-0.18366077]\ncount:3565, passes:0, max_passes:5, b:[-0.18366077]\ncount:3566, passes:1, max_passes:5, b:[-0.09242823]\ncount:3567, passes:0, max_passes:5, b:[-0.09242823]\ncount:3568, passes:1, max_passes:5, b:[-0.16011817]\ncount:3569, passes:0, max_passes:5, b:[-0.15349324]\ncount:3570, passes:0, max_passes:5, b:[-0.18379856]\ncount:3571, passes:0, max_passes:5, b:[-0.18379856]\ncount:3572, passes:1, max_passes:5, b:[-0.18379856]\ncount:3573, passes:2, max_passes:5, b:[-0.26130197]\ncount:3574, passes:0, max_passes:5, b:[-0.19559602]\ncount:3575, passes:0, max_passes:5, b:[-0.18953084]\ncount:3576, passes:0, max_passes:5, b:[-0.18953084]\ncount:3577, passes:1, max_passes:5, b:[-0.18307434]\ncount:3578, passes:0, max_passes:5, b:[-0.24355177]\ncount:3579, passes:0, max_passes:5, b:[-0.20119055]\ncount:3580, passes:0, max_passes:5, b:[-0.20119055]\ncount:3581, passes:1, max_passes:5, b:[-0.21035387]\ncount:3582, passes:0, max_passes:5, b:[-0.21035387]\ncount:3583, passes:1, max_passes:5, b:[-0.1524259]\ncount:3584, passes:0, max_passes:5, b:[-0.1524259]\ncount:3585, passes:1, max_passes:5, b:[-0.1524259]\ncount:3586, passes:2, max_passes:5, b:[-0.1524259]\ncount:3587, passes:3, max_passes:5, b:[-0.1524259]\ncount:3588, passes:4, max_passes:5, b:[-0.1524259]\ncount:3589, passes:5, max_passes:5, b:[-0.1524259]\nPerformance on the training set\n[[156   2]\n [  1 267]]\n"
    }
   ],
   "source": [
    "svm_model =SVM2CLASS(X_train, y_train, C = 10, tol = 0.001, max_passes = 5, passes = 0)\n",
    "support_vectors, support_alphas, support_target = svm_model.SMO(\"poly\", [1, 1])\n",
    "pred = svm_model.prediction_dataset(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'SVM2CLASS' object has no attribute 'prediction_dataset'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-64bf729e4343>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Performance on the training set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SVM2CLASS' object has no attribute 'prediction_dataset'"
     ]
    }
   ],
   "source": [
    "pred = svm_model.prediction_dataset(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svm_model.prediction_dataset(X_test)\n",
    "print(\"Performance on the test set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the training set\n[[156   2]\n [  1 267]]\n"
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "coef_ is only available when using a linear kernel",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b0a844013b29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\USER\\Desktop\\Python_Codes\\my_env64\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mcoef_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcoef_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             raise AttributeError('coef_ is only available when using a '\n\u001b[0m\u001b[0;32m    474\u001b[0m                                  'linear kernel')\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: coef_ is only available when using a linear kernel"
     ]
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "\n",
    "K = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    for j in range(0, X_train.shape[0]):\n",
    "        K[i, j] = polynomial_kernel(X_train[i, :], X_train[j, :], [0, 1])\n",
    "assert(np.any(np.linalg.eig(K)[0] == 0)  == False)#Test for PSD\n",
    "\n",
    "model = sklearn.svm.SVC(kernel=\"precomputed\", C=10)\n",
    "model.fit(K, y_train)\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, model.predict(K)))\n",
    "print(model.support_)\n",
    "print(model.intercept_)\n",
    "model.dual_coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 1, chapter 6 and Chapter 7 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 6: (https://www.youtube.com/watch?v=qyyJKd-zXRE)\n",
    "* Andrew Ng, Lec 7: (https://www.youtube.com/watch?v=s8B4A5ubw6c)\n",
    "* Andrew Ng, Lec 8: (https://www.youtube.com/watch?v=bUv9bfMPMb4)\n",
    "* Simplified Sequential Minimal Optimization: (https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiRlObmw5_qAhW7ShUIHSjJAbYQFjAAegQIAhAB&url=http%3A%2F%2Fcs229.stanford.edu%2Fmaterials%2Fsmo.pdf&usg=AOvVaw201bQxVZY0MmUn_gGAu5O8)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}