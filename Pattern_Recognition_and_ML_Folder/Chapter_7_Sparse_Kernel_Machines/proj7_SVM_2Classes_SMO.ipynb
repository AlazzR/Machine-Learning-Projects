{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines for Classification\n",
    "### Hard Margin (SVM without regularization)\n",
    "\n",
    "### Soft Margin (SVM with regularization)\n",
    "\n",
    "### Simplified Sequential Minimal Optimization (SMO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(426, 31)\n(143, 31)\n"
    }
   ],
   "source": [
    "X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "X_train = standard.fit_transform(X_train)\n",
    "training_data = pd.DataFrame(np.c_[X_train, y_train])#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "y_train = np.array(list(map(lambda y: 1 if y == 1 else -1, y_train)))# y must be either 1 or -1 \n",
    "\n",
    "X_test = standard.transform(X_test)\n",
    "y_test = np.array(list(map(lambda y: 1 if y == 1 else -1, y_test)))# y must be either 1 or -1 \n",
    "test_data = pd.DataFrame(np.c_[X_test, y_test])\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)\n",
    "#training_data.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM2CLASS(object):\n",
    "\n",
    "    def __init__(self, X_train, y_train, C = 10, tol = 0.001, max_passes = 5, passes = 0):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.max_passes = max_passes\n",
    "        self.passes = passes\n",
    "        self.b = 0\n",
    "        self.alphas = np.zeros((self.X_train.shape[0], 1))#need to be of size nx1\n",
    "        self.coefficients = ()\n",
    "        self.kernel_type ={\"poly\": self.polynomial_kernel, \"gauss\": self.gaussian_kernel}\n",
    "        self.kernel_parameters = []\n",
    "        self.kernel_choice = \"\"\n",
    "\n",
    "    def polynomial_kernel(self, x_i, x_j, a):\n",
    "        return np.power(np.dot(x_i.T, x_j) + a[0], a[1])\n",
    "\n",
    "    def gaussian_kernel(self, x, x_star, sigma):\n",
    "        return np.exp(np.divide(-1*(np.linalg.norm(x-x_star)**2), 2*sigma**2))\n",
    "    \n",
    "    def SMO(self, kernel_choice, parameters):\n",
    "\n",
    "        choices = np.arange(0, self.y_train.shape[0])\n",
    "        count = 0\n",
    "        self.kernel_choice = kernel_choice\n",
    "        #Better to construct the Kernel matrix from the scratch for efficiency, but this method will prevent the simplified SMO from working on large datasets, like, the mnist dataset \n",
    "        if kernel_choice == \"poly\":\n",
    "            exponent = parameters[0]\n",
    "            intercept = parameters[1]\n",
    "            self.kernel_parameters = [intercept, exponent] \n",
    "            \n",
    "            K = np.zeros((self.X_train.shape[0], self.X_train.shape[0]))\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "                for j in range(0, self.X_train.shape[0]):\n",
    "                    K[i, j] = self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "            assert(np.any(np.linalg.eig(K)[0] == 0)  == False)#Test for PSD\n",
    "\n",
    "        elif kernel_choice == \"gauss\":\n",
    "            self.kernel_parameters = 2\n",
    "\n",
    "            K = np.zeros((self.X_train.shape[0], self.X_train.shape[0]))\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "                for j in range(0, self.X_train.shape[0]):\n",
    "                    K[i, j] =  self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "            assert(np.linalg.det(K) != 0)#Test for PSD\n",
    "        \n",
    "        else:\n",
    "            print(\"Wrong entry\")\n",
    "            return -1\n",
    "\n",
    "        while(self.passes <= self.max_passes):\n",
    "        #begin while\n",
    "            num_changed_alphas = 0\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "            #begin for\n",
    "                #compute kernel function for every iteration from scratch\n",
    "                #f_x_i = np.sum( list(map(lambda x, alpha, y: alpha * y * self.kernel_type[self.kernel_choice](x, self.X_train[i, :], self.kernel_parameters) , self.X_train, self.alphas, self.y_train) ) )#instead of calculating the Kernel matrix from the start, we will calculate the inner product with each iteration in order to mitigate the problem of having a large kernel matrix that will raise an exception\n",
    "                #print(f_x_i)\n",
    "                f_x_i = np.sum(self.alphas.reshape(-1, 1) * (self.y_train.reshape(-1, 1) *  K[:, i].reshape(-1, 1)).reshape(-1, 1)) \n",
    "                E_i = f_x_i + self.b - self.y_train[i]\n",
    "                #print(f_x_i)\n",
    "                #Check if we satisfy the condition for the dual problem\n",
    "                if (((self.y_train[i] * E_i) < -self.tol) and (self.alphas[i] < self.C)) or (((self.y_train[i] * E_i) > self.tol) and (self.alphas[i] > 0)):\n",
    "                #begin if\n",
    "                    j = np.random.choice( list(filter(lambda v: v == v, list(map(lambda c: c if c != i else np.nan, choices)))) ) \n",
    "                    #only nan will generate False at its equlaity, and the filter object will end up filtering out these wrong values\n",
    "                    assert( i != j)\n",
    "                    #f_x_j = np.sum( list(map(lambda x, alpha, y: alpha * y * self.kernel_type[self.kernel_choice](x, self.X_train[j, :], self.kernel_parameters) , self.X_train, self.alphas, self.y_train) ) )\n",
    "                    #print(f_x_j)\n",
    "                    f_x_j = np.sum(self.alphas.reshape(-1, 1) * (self.y_train.reshape(-1, 1) * K[:, j].reshape(-1, 1)).reshape(-1, 1)) \n",
    "                    #print((alphas.reshape(-1, 1) * (y_train.reshape(-1, 1) * K[i, :].reshape(-1, 1)).reshape(-1, 1)).shape)\n",
    "                    #print(f_x_j)\n",
    "                    E_j = f_x_j + self.b - self.y_train[j]\n",
    "\n",
    "                    alpha_i_old = self.alphas[i].copy()#Needs to copy the value because otherwise they would be pointing to the same address\n",
    "                    alpha_j_old = self.alphas[j].copy()\n",
    "\n",
    "                    #Computing L and H\n",
    "                    if(self.y_train[i] != self.y_train[j]):\n",
    "                        L = max(0, self.alphas[j] - self.alphas[i])\n",
    "                        H = min(self.C, self.C + self.alphas[j] - self.alphas[i])\n",
    "                    else:\n",
    "                        L = max(0, self.alphas[j] + self.alphas[i] - self.C)\n",
    "                        H = min(self.C, self.alphas[j] + self.alphas[i])\n",
    "                    \n",
    "                    #Checking if L=H which indicate that the alpha would certainly wouldn't change \n",
    "                    if L == H:\n",
    "                        continue\n",
    "\n",
    "                    #eta = 2 * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters) - self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[i, :], self.kernel_parameters) - self.kernel_type[self.kernel_choice](self.X_train[j, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    eta = 2 * K[i, j] - K[i, i] - K[j, j] \n",
    "\n",
    "                    #eta = 0 if the similarity between x_i and x_j is as the combination of the similarity of x_i with itself and same goes for x_j, will cause an exception to happend and this indicate we are dealing with the same x.\n",
    "                    #eta > 0 if if the similarity between x_i and x_j is higher than the combination of the similarity of x_i with itself and same goes for x_j, so, this update step would have little effect on the converging to the optimal minimum and may leads to diverging the algorithm patht to a worse path\n",
    "                    #eta < 0 there are small simialrity between x_i and x_j, so, this would help in discoverign the interaction of those observations in the feature space\n",
    "\n",
    "                    if eta >= 0:\n",
    "                        #print(\"The two vectors are too similar\")\n",
    "                        continue\n",
    "\n",
    "                    alpha_j_clip = alpha_j_old - (1/eta) * self.y_train[j] * (E_i - E_j)\n",
    "\n",
    "                    if alpha_j_clip > H:\n",
    "                        self.alphas[j] = H\n",
    "                    elif alpha_j_clip < L:\n",
    "                        self.alphas[j] = L\n",
    "                    else:\n",
    "                        self.alphas[j]  = alpha_j_clip\n",
    "                    \n",
    "                    #print(alphas[j], alpha_j_old)\n",
    "                    #Check if it is worth to update alpha_i\n",
    "                    if(abs(self.alphas[j] - alpha_j_old) < 1e-3):\n",
    "                        #print(\"No noticeable changes happened to alpha\")\n",
    "                        continue\n",
    "                    self.alphas[i] = alpha_i_old + self.y_train[i] * self.y_train[j] * (alpha_j_old - self.alphas[j])#The signs changed from  the negative sign for updating alpha_i\n",
    "\n",
    "                    ##KKT constrains convergence test\n",
    "                    #b1 = self.b - E_i - self.y_train[i]*(self.alphas[i] - alpha_i_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[i, :], self.kernel_parameters) - self.y_train[j]*(self.alphas[j] - alpha_j_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    b1 = self.b - E_i - self.y_train[i] * (self.alphas[i] - alpha_i_old) * K[i, i] - self.y_train[j] * (self.alphas[j] - alpha_j_old) * K[i, j]\n",
    "\n",
    "                    #b2 = self.b - E_j - self.y_train[i]*(self.alphas[i] - alpha_i_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters) - self.y_train[j]*(self.alphas[j] - alpha_j_old) * self.kernel_type[self.kernel_choice](self.X_train[j, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    b2 = self.b - E_j - self.y_train[i] * (self.alphas[i] - alpha_i_old) * K[i, j] - self.y_train[j] * (self.alphas[j] - alpha_j_old) * K[j, j]\n",
    "\n",
    "                    if (self.alphas[j] > 0) and (self.alphas[j] < self.C):\n",
    "                        self.b = b2\n",
    "                    elif (self.alphas[i] > 0) and (self.alphas[i] < self.C):\n",
    "                        self.b = b1\n",
    "                    else:\n",
    "                        self.b = (b1 + b2)/2\n",
    "                    \n",
    "                    num_changed_alphas  =  num_changed_alphas + 1\n",
    "                #end if\n",
    "            #end for\n",
    "            print(f\"count:{count}, passes:{self.passes}, max_passes:{self.max_passes}, b:{self.b}\")\n",
    "            count+=1\n",
    "            if(num_changed_alphas == 0):\n",
    "                self.passes = self.passes + 1\n",
    "            else:\n",
    "                self.passes = 0\n",
    "        #end while\n",
    "        #Only store in the memory the support vectors\n",
    "        support_indeces = np.argwhere(self.alphas != 0)[:, 0]\n",
    "        support_vectors = self.X_train[support_indeces, :]\n",
    "        support_alphas = self.alphas[support_indeces]\n",
    "        support_target = self.y_train[support_indeces]\n",
    "        self.importantParameters = (support_vectors, support_alphas, support_target)\n",
    "\n",
    "        return self.importantParameters\n",
    "\n",
    "    def prediction_dataset(self, X):\n",
    "        pred = list(map(lambda x: self.prediction(x), X))\n",
    "        return pred\n",
    "\n",
    "    def prediction(self, x):\n",
    "        t1 = np.sum( list(map(lambda x1, alpha, y: y * alpha * self.kernel_type[self.kernel_choice](x, x1, self.kernel_parameters), self.importantParameters[0], self.importantParameters[1], self.importantParameters[2])) )\n",
    "        pred = t1 + self.b\n",
    "        #print(pred)\n",
    "        if pred >=0:\n",
    "            return 1\n",
    "    \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ", max_passes:5, b:[-0.15087567]\ncount:3371, passes:2, max_passes:5, b:[-0.24400035]\ncount:3372, passes:0, max_passes:5, b:[-0.24400035]\ncount:3373, passes:1, max_passes:5, b:[-0.19336467]\ncount:3374, passes:0, max_passes:5, b:[-0.19336467]\ncount:3375, passes:1, max_passes:5, b:[-0.33252898]\ncount:3376, passes:0, max_passes:5, b:[-0.14952051]\ncount:3377, passes:0, max_passes:5, b:[-0.25345214]\ncount:3378, passes:0, max_passes:5, b:[-0.25345214]\ncount:3379, passes:1, max_passes:5, b:[-0.06350016]\ncount:3380, passes:0, max_passes:5, b:[-0.11353621]\ncount:3381, passes:0, max_passes:5, b:[-0.28020908]\ncount:3382, passes:0, max_passes:5, b:[-0.28020908]\ncount:3383, passes:1, max_passes:5, b:[-0.30796845]\ncount:3384, passes:0, max_passes:5, b:[-0.3291488]\ncount:3385, passes:0, max_passes:5, b:[-0.3291488]\ncount:3386, passes:1, max_passes:5, b:[-0.25832276]\ncount:3387, passes:0, max_passes:5, b:[-0.15244854]\ncount:3388, passes:0, max_passes:5, b:[-0.15244854]\ncount:3389, passes:1, max_passes:5, b:[-0.14612324]\ncount:3390, passes:0, max_passes:5, b:[-0.11373974]\ncount:3391, passes:0, max_passes:5, b:[-0.13241741]\ncount:3392, passes:0, max_passes:5, b:[-0.13241741]\ncount:3393, passes:1, max_passes:5, b:[-0.1651656]\ncount:3394, passes:0, max_passes:5, b:[-0.1651656]\ncount:3395, passes:1, max_passes:5, b:[-0.08520501]\ncount:3396, passes:0, max_passes:5, b:[-0.20636156]\ncount:3397, passes:0, max_passes:5, b:[-0.19833725]\ncount:3398, passes:0, max_passes:5, b:[-0.25913253]\ncount:3399, passes:0, max_passes:5, b:[-0.09639574]\ncount:3400, passes:0, max_passes:5, b:[-0.24504643]\ncount:3401, passes:0, max_passes:5, b:[-0.24504643]\ncount:3402, passes:1, max_passes:5, b:[-0.16600788]\ncount:3403, passes:0, max_passes:5, b:[-0.16600788]\ncount:3404, passes:1, max_passes:5, b:[-0.44773219]\ncount:3405, passes:0, max_passes:5, b:[-0.17078883]\ncount:3406, passes:0, max_passes:5, b:[-0.14911235]\ncount:3407, passes:0, max_passes:5, b:[-0.12137529]\ncount:3408, passes:0, max_passes:5, b:[-0.27146245]\ncount:3409, passes:0, max_passes:5, b:[-0.07415973]\ncount:3410, passes:0, max_passes:5, b:[-0.20689868]\ncount:3411, passes:0, max_passes:5, b:[-0.20689868]\ncount:3412, passes:1, max_passes:5, b:[-0.22651706]\ncount:3413, passes:0, max_passes:5, b:[-0.21065549]\ncount:3414, passes:0, max_passes:5, b:[-0.11544478]\ncount:3415, passes:0, max_passes:5, b:[-0.11544478]\ncount:3416, passes:1, max_passes:5, b:[-0.22516225]\ncount:3417, passes:0, max_passes:5, b:[-0.21214755]\ncount:3418, passes:0, max_passes:5, b:[-0.17902419]\ncount:3419, passes:0, max_passes:5, b:[-0.19117511]\ncount:3420, passes:0, max_passes:5, b:[-0.18828305]\ncount:3421, passes:0, max_passes:5, b:[-0.18828305]\ncount:3422, passes:1, max_passes:5, b:[-0.26510121]\ncount:3423, passes:0, max_passes:5, b:[-0.14268518]\ncount:3424, passes:0, max_passes:5, b:[-0.13713188]\ncount:3425, passes:0, max_passes:5, b:[-0.24192628]\ncount:3426, passes:0, max_passes:5, b:[-0.16035568]\ncount:3427, passes:0, max_passes:5, b:[-0.16035568]\ncount:3428, passes:1, max_passes:5, b:[-0.16704444]\ncount:3429, passes:0, max_passes:5, b:[-0.28173088]\ncount:3430, passes:0, max_passes:5, b:[-0.18135664]\ncount:3431, passes:0, max_passes:5, b:[-0.20025022]\ncount:3432, passes:0, max_passes:5, b:[-0.17185731]\ncount:3433, passes:0, max_passes:5, b:[-0.1059912]\ncount:3434, passes:0, max_passes:5, b:[-0.2243526]\ncount:3435, passes:0, max_passes:5, b:[-0.28206545]\ncount:3436, passes:0, max_passes:5, b:[-0.19939381]\ncount:3437, passes:0, max_passes:5, b:[-0.08897635]\ncount:3438, passes:0, max_passes:5, b:[-0.08897635]\ncount:3439, passes:1, max_passes:5, b:[-0.07236349]\ncount:3440, passes:0, max_passes:5, b:[-0.07236349]\ncount:3441, passes:1, max_passes:5, b:[-0.07236349]\ncount:3442, passes:2, max_passes:5, b:[-0.113873]\ncount:3443, passes:0, max_passes:5, b:[-0.113873]\ncount:3444, passes:1, max_passes:5, b:[-0.19422762]\ncount:3445, passes:0, max_passes:5, b:[-0.09525057]\ncount:3446, passes:0, max_passes:5, b:[-0.09525057]\ncount:3447, passes:1, max_passes:5, b:[-0.29699606]\ncount:3448, passes:0, max_passes:5, b:[-0.18686737]\ncount:3449, passes:0, max_passes:5, b:[-0.18686737]\ncount:3450, passes:1, max_passes:5, b:[-0.22915532]\ncount:3451, passes:0, max_passes:5, b:[-0.22915532]\ncount:3452, passes:1, max_passes:5, b:[-0.1432702]\ncount:3453, passes:0, max_passes:5, b:[-0.17112756]\ncount:3454, passes:0, max_passes:5, b:[-0.15728198]\ncount:3455, passes:0, max_passes:5, b:[-0.14100025]\ncount:3456, passes:0, max_passes:5, b:[-0.1253606]\ncount:3457, passes:0, max_passes:5, b:[-0.13775748]\ncount:3458, passes:0, max_passes:5, b:[-0.13775748]\ncount:3459, passes:1, max_passes:5, b:[-0.20086377]\ncount:3460, passes:0, max_passes:5, b:[-0.19065694]\ncount:3461, passes:0, max_passes:5, b:[-0.34978997]\ncount:3462, passes:0, max_passes:5, b:[-0.16544037]\ncount:3463, passes:0, max_passes:5, b:[-0.16338703]\ncount:3464, passes:0, max_passes:5, b:[-0.16338703]\ncount:3465, passes:1, max_passes:5, b:[-0.16338703]\ncount:3466, passes:2, max_passes:5, b:[-0.1269117]\ncount:3467, passes:0, max_passes:5, b:[-0.1270108]\ncount:3468, passes:0, max_passes:5, b:[-0.185783]\ncount:3469, passes:0, max_passes:5, b:[-0.11953607]\ncount:3470, passes:0, max_passes:5, b:[-0.16084696]\ncount:3471, passes:0, max_passes:5, b:[-0.16116931]\ncount:3472, passes:0, max_passes:5, b:[-0.15324507]\ncount:3473, passes:0, max_passes:5, b:[-0.21092868]\ncount:3474, passes:0, max_passes:5, b:[-0.18745827]\ncount:3475, passes:0, max_passes:5, b:[-0.15832781]\ncount:3476, passes:0, max_passes:5, b:[-0.26619507]\ncount:3477, passes:0, max_passes:5, b:[-0.26619507]\ncount:3478, passes:1, max_passes:5, b:[-0.32048059]\ncount:3479, passes:0, max_passes:5, b:[-0.32048059]\ncount:3480, passes:1, max_passes:5, b:[-0.1542929]\ncount:3481, passes:0, max_passes:5, b:[-0.1542929]\ncount:3482, passes:1, max_passes:5, b:[-0.1542929]\ncount:3483, passes:2, max_passes:5, b:[-0.1687449]\ncount:3484, passes:0, max_passes:5, b:[-0.24791346]\ncount:3485, passes:0, max_passes:5, b:[-0.16898775]\ncount:3486, passes:0, max_passes:5, b:[-0.15078559]\ncount:3487, passes:0, max_passes:5, b:[-0.15078559]\ncount:3488, passes:1, max_passes:5, b:[-0.28127865]\ncount:3489, passes:0, max_passes:5, b:[-0.14918761]\ncount:3490, passes:0, max_passes:5, b:[-0.14918761]\ncount:3491, passes:1, max_passes:5, b:[-0.14918761]\ncount:3492, passes:2, max_passes:5, b:[-0.15622551]\ncount:3493, passes:0, max_passes:5, b:[-0.13385486]\ncount:3494, passes:0, max_passes:5, b:[-0.23844104]\ncount:3495, passes:0, max_passes:5, b:[-0.23844104]\ncount:3496, passes:1, max_passes:5, b:[-0.23141265]\ncount:3497, passes:0, max_passes:5, b:[-0.18849876]\ncount:3498, passes:0, max_passes:5, b:[-0.11834417]\ncount:3499, passes:0, max_passes:5, b:[-0.11834417]\ncount:3500, passes:1, max_passes:5, b:[-0.13784738]\ncount:3501, passes:0, max_passes:5, b:[-0.24805551]\ncount:3502, passes:0, max_passes:5, b:[-0.06953411]\ncount:3503, passes:0, max_passes:5, b:[-0.06953411]\ncount:3504, passes:1, max_passes:5, b:[-0.17480102]\ncount:3505, passes:0, max_passes:5, b:[-0.19955559]\ncount:3506, passes:0, max_passes:5, b:[-0.15232868]\ncount:3507, passes:0, max_passes:5, b:[-0.10418828]\ncount:3508, passes:0, max_passes:5, b:[-0.1326298]\ncount:3509, passes:0, max_passes:5, b:[-0.22865856]\ncount:3510, passes:0, max_passes:5, b:[-0.19962912]\ncount:3511, passes:0, max_passes:5, b:[-0.21884554]\ncount:3512, passes:0, max_passes:5, b:[-0.21884554]\ncount:3513, passes:1, max_passes:5, b:[-0.15861934]\ncount:3514, passes:0, max_passes:5, b:[-0.15861934]\ncount:3515, passes:1, max_passes:5, b:[-0.15861934]\ncount:3516, passes:2, max_passes:5, b:[-0.15050371]\ncount:3517, passes:0, max_passes:5, b:[-0.15050371]\ncount:3518, passes:1, max_passes:5, b:[-0.15050371]\ncount:3519, passes:2, max_passes:5, b:[-0.15050371]\ncount:3520, passes:3, max_passes:5, b:[-0.11258157]\ncount:3521, passes:0, max_passes:5, b:[-0.11258157]\ncount:3522, passes:1, max_passes:5, b:[-0.11258157]\ncount:3523, passes:2, max_passes:5, b:[-0.19278576]\ncount:3524, passes:0, max_passes:5, b:[-0.21745616]\ncount:3525, passes:0, max_passes:5, b:[-0.14629381]\ncount:3526, passes:0, max_passes:5, b:[-0.14629381]\ncount:3527, passes:1, max_passes:5, b:[-0.25946539]\ncount:3528, passes:0, max_passes:5, b:[-0.26953992]\ncount:3529, passes:0, max_passes:5, b:[-0.26953992]\ncount:3530, passes:1, max_passes:5, b:[-0.26953992]\ncount:3531, passes:2, max_passes:5, b:[-0.15339163]\ncount:3532, passes:0, max_passes:5, b:[-0.15339163]\ncount:3533, passes:1, max_passes:5, b:[-0.16783576]\ncount:3534, passes:0, max_passes:5, b:[-0.12873272]\ncount:3535, passes:0, max_passes:5, b:[-0.23728585]\ncount:3536, passes:0, max_passes:5, b:[-0.17297447]\ncount:3537, passes:0, max_passes:5, b:[-0.17297447]\ncount:3538, passes:1, max_passes:5, b:[-0.13366946]\ncount:3539, passes:0, max_passes:5, b:[-0.2634163]\ncount:3540, passes:0, max_passes:5, b:[-0.14878109]\ncount:3541, passes:0, max_passes:5, b:[-0.16955334]\ncount:3542, passes:0, max_passes:5, b:[-0.2192938]\ncount:3543, passes:0, max_passes:5, b:[-0.24495434]\ncount:3544, passes:0, max_passes:5, b:[-0.24495434]\ncount:3545, passes:1, max_passes:5, b:[-0.1698216]\ncount:3546, passes:0, max_passes:5, b:[-0.1698216]\ncount:3547, passes:1, max_passes:5, b:[-0.1653874]\ncount:3548, passes:0, max_passes:5, b:[-0.10820998]\ncount:3549, passes:0, max_passes:5, b:[-0.10198007]\ncount:3550, passes:0, max_passes:5, b:[-0.15317181]\ncount:3551, passes:0, max_passes:5, b:[-0.11114394]\ncount:3552, passes:0, max_passes:5, b:[-0.21376112]\ncount:3553, passes:0, max_passes:5, b:[-0.17684664]\ncount:3554, passes:0, max_passes:5, b:[-0.17684664]\ncount:3555, passes:1, max_passes:5, b:[-0.23226449]\ncount:3556, passes:0, max_passes:5, b:[-0.23226449]\ncount:3557, passes:1, max_passes:5, b:[-0.06750317]\ncount:3558, passes:0, max_passes:5, b:[-0.06750317]\ncount:3559, passes:1, max_passes:5, b:[-0.11252648]\ncount:3560, passes:0, max_passes:5, b:[-0.1684505]\ncount:3561, passes:0, max_passes:5, b:[-0.22051844]\ncount:3562, passes:0, max_passes:5, b:[-0.22051844]\ncount:3563, passes:1, max_passes:5, b:[-0.22051844]\ncount:3564, passes:2, max_passes:5, b:[-0.22051844]\ncount:3565, passes:3, max_passes:5, b:[-0.12592779]\ncount:3566, passes:0, max_passes:5, b:[-0.19136191]\ncount:3567, passes:0, max_passes:5, b:[-0.17548222]\ncount:3568, passes:0, max_passes:5, b:[-0.0738232]\ncount:3569, passes:0, max_passes:5, b:[-0.0738232]\ncount:3570, passes:1, max_passes:5, b:[-0.0738232]\ncount:3571, passes:2, max_passes:5, b:[-0.14935975]\ncount:3572, passes:0, max_passes:5, b:[-0.14935975]\ncount:3573, passes:1, max_passes:5, b:[-0.19357343]\ncount:3574, passes:0, max_passes:5, b:[-0.19357343]\ncount:3575, passes:1, max_passes:5, b:[-0.19357343]\ncount:3576, passes:2, max_passes:5, b:[-0.2365871]\ncount:3577, passes:0, max_passes:5, b:[-0.16818022]\ncount:3578, passes:0, max_passes:5, b:[-0.26561075]\ncount:3579, passes:0, max_passes:5, b:[-0.26561075]\ncount:3580, passes:1, max_passes:5, b:[-0.20264199]\ncount:3581, passes:0, max_passes:5, b:[-0.20264199]\ncount:3582, passes:1, max_passes:5, b:[-0.14327319]\ncount:3583, passes:0, max_passes:5, b:[-0.14327319]\ncount:3584, passes:1, max_passes:5, b:[-0.2031886]\ncount:3585, passes:0, max_passes:5, b:[-0.18601898]\ncount:3586, passes:0, max_passes:5, b:[-0.18601898]\ncount:3587, passes:1, max_passes:5, b:[-0.16431052]\ncount:3588, passes:0, max_passes:5, b:[-0.16431052]\ncount:3589, passes:1, max_passes:5, b:[-0.22650168]\ncount:3590, passes:0, max_passes:5, b:[-0.08422368]\ncount:3591, passes:0, max_passes:5, b:[-0.14485825]\ncount:3592, passes:0, max_passes:5, b:[-0.14485825]\ncount:3593, passes:1, max_passes:5, b:[-0.06460805]\ncount:3594, passes:0, max_passes:5, b:[-0.06460805]\ncount:3595, passes:1, max_passes:5, b:[-0.14078711]\ncount:3596, passes:0, max_passes:5, b:[-0.10920013]\ncount:3597, passes:0, max_passes:5, b:[-0.25700285]\ncount:3598, passes:0, max_passes:5, b:[-0.12784378]\ncount:3599, passes:0, max_passes:5, b:[-0.25974753]\ncount:3600, passes:0, max_passes:5, b:[-0.13153335]\ncount:3601, passes:0, max_passes:5, b:[-0.13153335]\ncount:3602, passes:1, max_passes:5, b:[-0.13153335]\ncount:3603, passes:2, max_passes:5, b:[-0.13153335]\ncount:3604, passes:3, max_passes:5, b:[-0.15339248]\ncount:3605, passes:0, max_passes:5, b:[-0.27310616]\ncount:3606, passes:0, max_passes:5, b:[-0.17878152]\ncount:3607, passes:0, max_passes:5, b:[-0.08331031]\ncount:3608, passes:0, max_passes:5, b:[-0.08331031]\ncount:3609, passes:1, max_passes:5, b:[-0.15971993]\ncount:3610, passes:0, max_passes:5, b:[-0.11011455]\ncount:3611, passes:0, max_passes:5, b:[-0.2809831]\ncount:3612, passes:0, max_passes:5, b:[-0.23707919]\ncount:3613, passes:0, max_passes:5, b:[-0.1466467]\ncount:3614, passes:0, max_passes:5, b:[-0.1466467]\ncount:3615, passes:1, max_passes:5, b:[-0.1466467]\ncount:3616, passes:2, max_passes:5, b:[-0.1466467]\ncount:3617, passes:3, max_passes:5, b:[-0.1466467]\ncount:3618, passes:4, max_passes:5, b:[-0.17325397]\ncount:3619, passes:0, max_passes:5, b:[-0.17325397]\ncount:3620, passes:1, max_passes:5, b:[-0.22131224]\ncount:3621, passes:0, max_passes:5, b:[-0.18448594]\ncount:3622, passes:0, max_passes:5, b:[-0.18448594]\ncount:3623, passes:1, max_passes:5, b:[-0.13943625]\ncount:3624, passes:0, max_passes:5, b:[-0.15453458]\ncount:3625, passes:0, max_passes:5, b:[-0.15453458]\ncount:3626, passes:1, max_passes:5, b:[-0.19805196]\ncount:3627, passes:0, max_passes:5, b:[-0.10636773]\ncount:3628, passes:0, max_passes:5, b:[-0.10636773]\ncount:3629, passes:1, max_passes:5, b:[-0.15177852]\ncount:3630, passes:0, max_passes:5, b:[-0.14092035]\ncount:3631, passes:0, max_passes:5, b:[-0.27870591]\ncount:3632, passes:0, max_passes:5, b:[-0.19190801]\ncount:3633, passes:0, max_passes:5, b:[-0.19190801]\ncount:3634, passes:1, max_passes:5, b:[-0.1592157]\ncount:3635, passes:0, max_passes:5, b:[-0.1592157]\ncount:3636, passes:1, max_passes:5, b:[-0.1592157]\ncount:3637, passes:2, max_passes:5, b:[-0.16412867]\ncount:3638, passes:0, max_passes:5, b:[-0.30626842]\ncount:3639, passes:0, max_passes:5, b:[-0.13205417]\ncount:3640, passes:0, max_passes:5, b:[-0.13205417]\ncount:3641, passes:1, max_passes:5, b:[-0.25665752]\ncount:3642, passes:0, max_passes:5, b:[-0.25665752]\ncount:3643, passes:1, max_passes:5, b:[-0.18263332]\ncount:3644, passes:0, max_passes:5, b:[-0.2026232]\ncount:3645, passes:0, max_passes:5, b:[-0.22547559]\ncount:3646, passes:0, max_passes:5, b:[-0.22547559]\ncount:3647, passes:1, max_passes:5, b:[-0.22547559]\ncount:3648, passes:2, max_passes:5, b:[-0.15288572]\ncount:3649, passes:0, max_passes:5, b:[-0.15288572]\ncount:3650, passes:1, max_passes:5, b:[-0.12964232]\ncount:3651, passes:0, max_passes:5, b:[-0.12964232]\ncount:3652, passes:1, max_passes:5, b:[-0.08150654]\ncount:3653, passes:0, max_passes:5, b:[-0.19478155]\ncount:3654, passes:0, max_passes:5, b:[-0.09652572]\ncount:3655, passes:0, max_passes:5, b:[-0.09522629]\ncount:3656, passes:0, max_passes:5, b:[-0.09522629]\ncount:3657, passes:1, max_passes:5, b:[-0.22300692]\ncount:3658, passes:0, max_passes:5, b:[-0.15705709]\ncount:3659, passes:0, max_passes:5, b:[-0.21198674]\ncount:3660, passes:0, max_passes:5, b:[-0.21198674]\ncount:3661, passes:1, max_passes:5, b:[-0.21198674]\ncount:3662, passes:2, max_passes:5, b:[-0.13216566]\ncount:3663, passes:0, max_passes:5, b:[-0.21948799]\ncount:3664, passes:0, max_passes:5, b:[-0.21948799]\ncount:3665, passes:1, max_passes:5, b:[-0.13490154]\ncount:3666, passes:0, max_passes:5, b:[-0.07098487]\ncount:3667, passes:0, max_passes:5, b:[-0.24177861]\ncount:3668, passes:0, max_passes:5, b:[-0.24177861]\ncount:3669, passes:1, max_passes:5, b:[-0.14570224]\ncount:3670, passes:0, max_passes:5, b:[-0.14570224]\ncount:3671, passes:1, max_passes:5, b:[-0.14570224]\ncount:3672, passes:2, max_passes:5, b:[-0.09549031]\ncount:3673, passes:0, max_passes:5, b:[-0.13255961]\ncount:3674, passes:0, max_passes:5, b:[-0.10394518]\ncount:3675, passes:0, max_passes:5, b:[-0.10394518]\ncount:3676, passes:1, max_passes:5, b:[-0.19882014]\ncount:3677, passes:0, max_passes:5, b:[-0.16678836]\ncount:3678, passes:0, max_passes:5, b:[-0.16678836]\ncount:3679, passes:1, max_passes:5, b:[-0.11117772]\ncount:3680, passes:0, max_passes:5, b:[-0.11117772]\ncount:3681, passes:1, max_passes:5, b:[-0.11117772]\ncount:3682, passes:2, max_passes:5, b:[-0.10839522]\ncount:3683, passes:0, max_passes:5, b:[-0.12301071]\ncount:3684, passes:0, max_passes:5, b:[-0.22536551]\ncount:3685, passes:0, max_passes:5, b:[-0.13933358]\ncount:3686, passes:0, max_passes:5, b:[-0.14362551]\ncount:3687, passes:0, max_passes:5, b:[-0.14362551]\ncount:3688, passes:1, max_passes:5, b:[-0.14362551]\ncount:3689, passes:2, max_passes:5, b:[-0.14239197]\ncount:3690, passes:0, max_passes:5, b:[-0.12395461]\ncount:3691, passes:0, max_passes:5, b:[-0.1111278]\ncount:3692, passes:0, max_passes:5, b:[-0.23833766]\ncount:3693, passes:0, max_passes:5, b:[-0.23833766]\ncount:3694, passes:1, max_passes:5, b:[-0.23833766]\ncount:3695, passes:2, max_passes:5, b:[-0.23833766]\ncount:3696, passes:3, max_passes:5, b:[-0.19537744]\ncount:3697, passes:0, max_passes:5, b:[-0.22228129]\ncount:3698, passes:0, max_passes:5, b:[-0.11264861]\ncount:3699, passes:0, max_passes:5, b:[-0.1495809]\ncount:3700, passes:0, max_passes:5, b:[-0.13230267]\ncount:3701, passes:0, max_passes:5, b:[-0.14070492]\ncount:3702, passes:0, max_passes:5, b:[-0.14070492]\ncount:3703, passes:1, max_passes:5, b:[-0.14070492]\ncount:3704, passes:2, max_passes:5, b:[-0.12766191]\ncount:3705, passes:0, max_passes:5, b:[-0.12766191]\ncount:3706, passes:1, max_passes:5, b:[-0.12766191]\ncount:3707, passes:2, max_passes:5, b:[-0.16381057]\ncount:3708, passes:0, max_passes:5, b:[-0.30491203]\ncount:3709, passes:0, max_passes:5, b:[-0.16719003]\ncount:3710, passes:0, max_passes:5, b:[-0.13348443]\ncount:3711, passes:0, max_passes:5, b:[-0.14857472]\ncount:3712, passes:0, max_passes:5, b:[-0.14857472]\ncount:3713, passes:1, max_passes:5, b:[-0.09702777]\ncount:3714, passes:0, max_passes:5, b:[-0.17157225]\ncount:3715, passes:0, max_passes:5, b:[-0.17157225]\ncount:3716, passes:1, max_passes:5, b:[-0.07877887]\ncount:3717, passes:0, max_passes:5, b:[-0.13181881]\ncount:3718, passes:0, max_passes:5, b:[-0.12502294]\ncount:3719, passes:0, max_passes:5, b:[-0.1893283]\ncount:3720, passes:0, max_passes:5, b:[-0.1893283]\ncount:3721, passes:1, max_passes:5, b:[-0.17695922]\ncount:3722, passes:0, max_passes:5, b:[-0.1411416]\ncount:3723, passes:0, max_passes:5, b:[-0.1411416]\ncount:3724, passes:1, max_passes:5, b:[-0.09240175]\ncount:3725, passes:0, max_passes:5, b:[-0.11025388]\ncount:3726, passes:0, max_passes:5, b:[-0.1001826]\ncount:3727, passes:0, max_passes:5, b:[-0.1562966]\ncount:3728, passes:0, max_passes:5, b:[-0.1562966]\ncount:3729, passes:1, max_passes:5, b:[-0.12827256]\ncount:3730, passes:0, max_passes:5, b:[-0.1691292]\ncount:3731, passes:0, max_passes:5, b:[-0.11851449]\ncount:3732, passes:0, max_passes:5, b:[-0.11851449]\ncount:3733, passes:1, max_passes:5, b:[-0.23392363]\ncount:3734, passes:0, max_passes:5, b:[-0.22193432]\ncount:3735, passes:0, max_passes:5, b:[-0.14471864]\ncount:3736, passes:0, max_passes:5, b:[-0.10502609]\ncount:3737, passes:0, max_passes:5, b:[-0.16943126]\ncount:3738, passes:0, max_passes:5, b:[-0.16943126]\ncount:3739, passes:1, max_passes:5, b:[-0.19974501]\ncount:3740, passes:0, max_passes:5, b:[-0.19974501]\ncount:3741, passes:1, max_passes:5, b:[-0.11038128]\ncount:3742, passes:0, max_passes:5, b:[-0.17605733]\ncount:3743, passes:0, max_passes:5, b:[-0.1893087]\ncount:3744, passes:0, max_passes:5, b:[-0.14940078]\ncount:3745, passes:0, max_passes:5, b:[-0.1683388]\ncount:3746, passes:0, max_passes:5, b:[-0.1683388]\ncount:3747, passes:1, max_passes:5, b:[-0.16666722]\ncount:3748, passes:0, max_passes:5, b:[-0.11150649]\ncount:3749, passes:0, max_passes:5, b:[-0.11150649]\ncount:3750, passes:1, max_passes:5, b:[-0.11150649]\ncount:3751, passes:2, max_passes:5, b:[-0.11150649]\ncount:3752, passes:3, max_passes:5, b:[-0.11150649]\ncount:3753, passes:4, max_passes:5, b:[-0.11150649]\ncount:3754, passes:5, max_passes:5, b:[-0.11150649]\nPerformance on the training set\n[[156   2]\n [  0 268]]\n"
    }
   ],
   "source": [
    "svm_model =SVM2CLASS(X_train, y_train, C = 10, tol = 0.001, max_passes = 5, passes = 0)\n",
    "support_vectors, support_alphas, support_target = svm_model.SMO(\"poly\", [1, 1])\n",
    "pred = svm_model.prediction_dataset(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the training set\n[[156   2]\n [  0 268]]\n"
    }
   ],
   "source": [
    "pred = svm_model.prediction_dataset(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the test set\n[[52  2]\n [ 4 85]]\n"
    }
   ],
   "source": [
    "pred = svm_model.prediction_dataset(X_test)\n",
    "print(\"Performance on the test set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the training set\n[[156   2]\n [  1 267]]\n[  5  33  91 105 126 164 168 303 325 335 365 374 384 412  38  43  53  65\n 107 115 120 134 151 250 318 353 407 418]\n[-0.06041561]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ -1.90700976,  -3.37970192,  -2.10920559, -10.        ,\n         -2.53562665, -10.        ,  -0.27063242,  -2.75357763,\n        -10.        , -10.        ,  -3.43593007,  -3.32707653,\n         -6.57192222,  -3.62688542,   5.54572154,   3.34438379,\n          6.72527011,   5.01150459,  10.        ,   2.1042078 ,\n          3.52560421,   0.27223051,   7.82312681,   0.38694323,\n          0.21750621,   9.57067784,  10.        ,   5.39039159]])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "\n",
    "def polynomial_kernel(x_i, x_j, a):\n",
    "        return np.power(np.dot(x_i.T, x_j) + a[0], a[1])\n",
    "\n",
    "K = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    for j in range(0, X_train.shape[0]):\n",
    "        K[i, j] = polynomial_kernel(X_train[i, :], X_train[j, :], [0, 1])\n",
    "assert(np.any(np.linalg.eig(K)[0] == 0)  == False)#Test for PSD\n",
    "\n",
    "model = sklearn.svm.SVC(kernel=\"precomputed\", C=10)\n",
    "model.fit(K, y_train)\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, model.predict(K)))\n",
    "print(model.support_)\n",
    "print(model.intercept_)\n",
    "model.dual_coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 1, chapter 6 and Chapter 7 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 6: (https://www.youtube.com/watch?v=qyyJKd-zXRE)\n",
    "* Andrew Ng, Lec 7: (https://www.youtube.com/watch?v=s8B4A5ubw6c)\n",
    "* Andrew Ng, Lec 8: (https://www.youtube.com/watch?v=bUv9bfMPMb4)\n",
    "* Simplified Sequential Minimal Optimization: (https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiRlObmw5_qAhW7ShUIHSjJAbYQFjAAegQIAhAB&url=http%3A%2F%2Fcs229.stanford.edu%2Fmaterials%2Fsmo.pdf&usg=AOvVaw201bQxVZY0MmUn_gGAu5O8)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}