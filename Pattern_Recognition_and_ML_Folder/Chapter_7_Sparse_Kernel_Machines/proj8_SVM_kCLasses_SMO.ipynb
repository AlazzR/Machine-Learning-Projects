{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Vector Machine for K-Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(133, 14)\n(45, 14)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 1, 1, 2, 0, 1, 0, 0, 2, 2, 1, 1, 0, 1, 0, 2, 1, 1, 2, 0, 0, 0,\n       2, 0, 0, 1, 2, 1, 0, 2, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 2, 1,\n       1, 1, 0, 1, 1, 1, 2, 2, 0, 1, 2, 2, 1, 1, 0, 1, 2, 2, 1, 2, 1, 1,\n       1, 0, 0, 2, 0, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2, 2,\n       1, 0, 0, 1, 2, 2, 0, 1, 2, 2, 2, 2, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0,\n       2, 1, 0, 2, 2, 0, 0, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1,\n       1])"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "X, y = sklearn.datasets.load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "X_train = standard.fit_transform(X_train)\n",
    "training_data = np.c_[X_train, y_train]#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "\n",
    "X_test = standard.transform(X_test)\n",
    "test_data = np.c_[X_test, y_test]\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)\n",
    "k = len(set(y_train))\n",
    "y_train#It needs to be labeled from 0 to k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM2CLASS(object):\n",
    "\n",
    "    def __init__(self, X_train, y_train, C = 10, tol = 0.001, max_passes = 5, passes = 0, classType = 0):\n",
    "        self.classType = classType\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.max_passes = max_passes\n",
    "        self.passes = passes\n",
    "        self.b = 0\n",
    "        self.alphas = np.zeros((self.X_train.shape[0], 1))#need to be of size nx1\n",
    "        self.coefficients = ()\n",
    "        self.kernel_type ={\"poly\": self.polynomial_kernel, \"gauss\": self.gaussian_kernel}\n",
    "        self.kernel_parameters = []\n",
    "        self.kernel_choice = \"\"\n",
    "\n",
    "    def polynomial_kernel(self, x_i, x_j, a):\n",
    "        return np.power(np.dot(x_i.T, x_j) + a[0], a[1])\n",
    "\n",
    "    def gaussian_kernel(self, x, x_star, sigma):\n",
    "        return np.exp(np.divide(-1*(np.linalg.norm(x-x_star)**2), 2*sigma**2))\n",
    "    \n",
    "    def SMO(self, kernel_choice, parameters):\n",
    "\n",
    "        choices = np.arange(0, self.y_train.shape[0])\n",
    "        count = 0\n",
    "        self.kernel_choice = kernel_choice\n",
    "        #Better to construct the Kernel matrix from the scratch for efficiency, but this method will prevent the simplified SMO from working on large datasets, like, the mnist dataset \n",
    "        if kernel_choice == \"poly\":\n",
    "            exponent = parameters[0]\n",
    "            intercept = parameters[1]\n",
    "            self.kernel_parameters = [intercept, exponent] \n",
    "            \n",
    "            K = np.zeros((self.X_train.shape[0], self.X_train.shape[0]))\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "                for j in range(0, self.X_train.shape[0]):\n",
    "                    K[i, j] = self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "            assert(np.any(np.linalg.eig(K)[0] == 0)  == False)#Test for PSD\n",
    "\n",
    "        elif kernel_choice == \"gauss\":\n",
    "            self.kernel_parameters = 2\n",
    "\n",
    "            K = np.zeros((self.X_train.shape[0], self.X_train.shape[0]))\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "                for j in range(0, self.X_train.shape[0]):\n",
    "                    K[i, j] =  self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "            assert(np.linalg.det(K) != 0)#Test for PSD\n",
    "        \n",
    "        else:\n",
    "            print(\"Wrong entry\")\n",
    "            return -1\n",
    "\n",
    "        while(self.passes <= self.max_passes):\n",
    "        #begin while\n",
    "            num_changed_alphas = 0\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "            #begin for\n",
    "                #compute kernel function for every iteration from scratch\n",
    "                #f_x_i = np.sum( list(map(lambda x, alpha, y: alpha * y * self.kernel_type[self.kernel_choice](x, self.X_train[i, :], self.kernel_parameters) , self.X_train, self.alphas, self.y_train) ) )#instead of calculating the Kernel matrix from the start, we will calculate the inner product with each iteration in order to mitigate the problem of having a large kernel matrix that will raise an exception\n",
    "                #print(f_x_i)\n",
    "                f_x_i = np.sum(self.alphas.reshape(-1, 1) * (self.y_train.reshape(-1, 1) *  K[:, i].reshape(-1, 1)).reshape(-1, 1)) \n",
    "                E_i = f_x_i + self.b - self.y_train[i]\n",
    "                #print(f_x_i)\n",
    "                #Check if we satisfy the condition for the dual problem\n",
    "                if (((self.y_train[i] * E_i) < -self.tol) and (self.alphas[i] < self.C)) or (((self.y_train[i] * E_i) > self.tol) and (self.alphas[i] > 0)):\n",
    "                #begin if\n",
    "                    j = np.random.choice( list(filter(lambda v: v == v, list(map(lambda c: c if c != i else np.nan, choices)))) ) \n",
    "                    #only nan will generate False at its equlaity, and the filter object will end up filtering out these wrong values\n",
    "                    assert( i != j)\n",
    "                    #f_x_j = np.sum( list(map(lambda x, alpha, y: alpha * y * self.kernel_type[self.kernel_choice](x, self.X_train[j, :], self.kernel_parameters) , self.X_train, self.alphas, self.y_train) ) )\n",
    "                    #print(f_x_j)\n",
    "                    f_x_j = np.sum(self.alphas.reshape(-1, 1) * (self.y_train.reshape(-1, 1) * K[:, j].reshape(-1, 1)).reshape(-1, 1)) \n",
    "                    #print((alphas.reshape(-1, 1) * (y_train.reshape(-1, 1) * K[i, :].reshape(-1, 1)).reshape(-1, 1)).shape)\n",
    "                    #print(f_x_j)\n",
    "                    E_j = f_x_j + self.b - self.y_train[j]\n",
    "\n",
    "                    alpha_i_old = self.alphas[i].copy()#Needs to copy the value because otherwise they would be pointing to the same address\n",
    "                    alpha_j_old = self.alphas[j].copy()\n",
    "\n",
    "                    #Computing L and H\n",
    "                    if(self.y_train[i] != self.y_train[j]):\n",
    "                        L = max(0, self.alphas[j] - self.alphas[i])\n",
    "                        H = min(self.C, self.C + self.alphas[j] - self.alphas[i])\n",
    "                    else:\n",
    "                        L = max(0, self.alphas[j] + self.alphas[i] - self.C)\n",
    "                        H = min(self.C, self.alphas[j] + self.alphas[i])\n",
    "                    \n",
    "                    #Checking if L=H which indicate that the alpha would certainly wouldn't change \n",
    "                    if L == H:\n",
    "                        continue\n",
    "\n",
    "                    #eta = 2 * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters) - self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[i, :], self.kernel_parameters) - self.kernel_type[self.kernel_choice](self.X_train[j, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    eta = 2 * K[i, j] - K[i, i] - K[j, j] \n",
    "\n",
    "                    #eta = 0 if the similarity between x_i and x_j is as the combination of the similarity of x_i with itself and same goes for x_j, will cause an exception to happend and this indicate we are dealing with the same x.\n",
    "                    #eta > 0 if if the similarity between x_i and x_j is higher than the combination of the similarity of x_i with itself and same goes for x_j, so, this update step would have little effect on the converging to the optimal minimum and may leads to diverging the algorithm patht to a worse path\n",
    "                    #eta < 0 there are small simialrity between x_i and x_j, so, this would help in discoverign the interaction of those observations in the feature space\n",
    "\n",
    "                    if eta >= 0:\n",
    "                        #print(\"The two vectors are too similar\")\n",
    "                        continue\n",
    "\n",
    "                    alpha_j_clip = alpha_j_old - (1/eta) * self.y_train[j] * (E_i - E_j)\n",
    "\n",
    "                    if alpha_j_clip > H:\n",
    "                        self.alphas[j] = H\n",
    "                    elif alpha_j_clip < L:\n",
    "                        self.alphas[j] = L\n",
    "                    else:\n",
    "                        self.alphas[j]  = alpha_j_clip\n",
    "                    \n",
    "                    #print(alphas[j], alpha_j_old)\n",
    "                    #Check if it is worth to update alpha_i\n",
    "                    if(abs(self.alphas[j] - alpha_j_old) < 1e-3):\n",
    "                        #print(\"No noticeable changes happened to alpha\")\n",
    "                        continue\n",
    "                    self.alphas[i] = alpha_i_old + self.y_train[i] * self.y_train[j] * (alpha_j_old - self.alphas[j])#The signs changed from  the negative sign for updating alpha_i\n",
    "\n",
    "                    ##KKT constrains convergence test\n",
    "                    #b1 = self.b - E_i - self.y_train[i]*(self.alphas[i] - alpha_i_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[i, :], self.kernel_parameters) - self.y_train[j]*(self.alphas[j] - alpha_j_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    b1 = self.b - E_i - self.y_train[i] * (self.alphas[i] - alpha_i_old) * K[i, i] - self.y_train[j] * (self.alphas[j] - alpha_j_old) * K[i, j]\n",
    "\n",
    "                    #b2 = self.b - E_j - self.y_train[i]*(self.alphas[i] - alpha_i_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters) - self.y_train[j]*(self.alphas[j] - alpha_j_old) * self.kernel_type[self.kernel_choice](self.X_train[j, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    b2 = self.b - E_j - self.y_train[i] * (self.alphas[i] - alpha_i_old) * K[i, j] - self.y_train[j] * (self.alphas[j] - alpha_j_old) * K[j, j]\n",
    "\n",
    "                    if (self.alphas[j] > 0) and (self.alphas[j] < self.C):\n",
    "                        self.b = b2\n",
    "                    elif (self.alphas[i] > 0) and (self.alphas[i] < self.C):\n",
    "                        self.b = b1\n",
    "                    else:\n",
    "                        self.b = (b1 + b2)/2\n",
    "                    \n",
    "                    num_changed_alphas  =  num_changed_alphas + 1\n",
    "                #end if\n",
    "            #end for\n",
    "            print(f\"class:{self.classType}, count:{count}, passes:{self.passes}, max_passes:{self.max_passes}, b:{self.b}\")\n",
    "            count+=1\n",
    "            if(num_changed_alphas == 0):\n",
    "                self.passes = self.passes + 1\n",
    "            else:\n",
    "                self.passes = 0\n",
    "        #end while\n",
    "        #Only store in the memory the support vectors\n",
    "        support_indeces = np.argwhere(self.alphas != 0)[:, 0]\n",
    "        support_vectors = self.X_train[support_indeces, :]\n",
    "        support_alphas = self.alphas[support_indeces]\n",
    "        support_target = self.y_train[support_indeces]\n",
    "        self.importantParameters = (support_vectors, support_alphas, support_target, self.b)\n",
    "\n",
    "        return self.importantParameters\n",
    "\n",
    "    def prediction_dataset(self, X):\n",
    "        pred = list(map(lambda x: self.prediction(x), X))\n",
    "        return pred\n",
    "\n",
    "    def prediction(self, x):\n",
    "        t1 = np.sum( list(map(lambda x1, alpha, y: y * alpha * self.kernel_type[self.kernel_choice](x, x1, self.kernel_parameters), self.importantParameters[0], self.importantParameters[1], self.importantParameters[2])) )\n",
    "        pred = t1 + self.b\n",
    "        #print(pred)\n",
    "        if pred >=0:\n",
    "            return 1\n",
    "    \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_kCLasses(object):\n",
    "    \n",
    "    #To simplify things I will assume that I will use the same kernel function for all of the combination of 1 vs K models\n",
    "    def __init__(self, X_train, y_train, C = 10, tol = 0.001, max_passes = 5, kernel_choice = \"poly\", parameters=[1, 1], k=None):\n",
    "        assert(k != None)\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.max_passes = max_passes\n",
    "        self.kernel_choice = kernel_choice\n",
    "        self.parameters = parameters\n",
    "        self.K = k \n",
    "        self.alphas = []\n",
    "        self.b = []\n",
    "        self.coefficients = []\n",
    "        self.classes = []\n",
    "        self.models = []\n",
    "        self.X_train = X_train\n",
    "        for c in range(0, self.K):\n",
    "            temp = np.array(list(map(lambda y: 1 if y == c else -1, y_train)))\n",
    "            self.classes.append(temp)\n",
    "        \n",
    "    def fit(self):\n",
    "        for k in range(0, self.K):\n",
    "            self.models.append(SVM2CLASS(self.X_train, self.classes[k], self.C, self.tol, self.max_passes, 0, k))\n",
    "            support_vectors, support_alphas, support_target, b = self.models[k].SMO(self.kernel_choice, self.parameters)\n",
    "            self.alphas.append(support_alphas)\n",
    "            self.b.append(b)\n",
    "            self.coefficients.append((support_vectors, support_alphas, support_target, b))\n",
    "        return self.coefficients\n",
    "\n",
    "    def prediction(self, X):\n",
    "        pred = np.zeros((X.shape[0], self.K))\n",
    "        for k in range(0, self.K):\n",
    "            pred[:, k] = self.models[k].prediction_dataset(X)\n",
    "            \n",
    "        final_prediction = np.argmax(pred, axis=1)#along the rows\n",
    "        return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "class:0, count:0, passes:0, max_passes:5, b:[0.0588624]\nclass:0, count:1, passes:0, max_passes:5, b:[-0.04423419]\nclass:0, count:2, passes:0, max_passes:5, b:[0.02909278]\nclass:0, count:3, passes:0, max_passes:5, b:[-0.18266851]\nclass:0, count:4, passes:0, max_passes:5, b:[0.29532926]\nclass:0, count:5, passes:0, max_passes:5, b:[0.16962927]\nclass:0, count:6, passes:0, max_passes:5, b:[0.12942534]\nclass:0, count:7, passes:0, max_passes:5, b:[0.15209673]\nclass:0, count:8, passes:0, max_passes:5, b:[-0.4153345]\nclass:0, count:9, passes:0, max_passes:5, b:[0.19504563]\nclass:0, count:10, passes:0, max_passes:5, b:[0.09146349]\nclass:0, count:11, passes:0, max_passes:5, b:[0.21389233]\nclass:0, count:12, passes:0, max_passes:5, b:[0.12600444]\nclass:0, count:13, passes:0, max_passes:5, b:[0.35687334]\nclass:0, count:14, passes:0, max_passes:5, b:[0.1434446]\nclass:0, count:15, passes:0, max_passes:5, b:[0.13019911]\nclass:0, count:16, passes:0, max_passes:5, b:[0.20984433]\nclass:0, count:17, passes:0, max_passes:5, b:[0.21722309]\nclass:0, count:18, passes:0, max_passes:5, b:[-0.02193557]\nclass:0, count:19, passes:0, max_passes:5, b:[0.13433748]\nclass:0, count:20, passes:0, max_passes:5, b:[0.09335709]\nclass:0, count:21, passes:0, max_passes:5, b:[0.25772018]\nclass:0, count:22, passes:0, max_passes:5, b:[0.26590021]\nclass:0, count:23, passes:0, max_passes:5, b:[0.18463724]\nclass:0, count:24, passes:0, max_passes:5, b:[0.10763845]\nclass:0, count:25, passes:0, max_passes:5, b:[0.1402869]\nclass:0, count:26, passes:0, max_passes:5, b:[0.1402869]\nclass:0, count:27, passes:1, max_passes:5, b:[0.227305]\nclass:0, count:28, passes:0, max_passes:5, b:[0.227305]\nclass:0, count:29, passes:1, max_passes:5, b:[0.227305]\nclass:0, count:30, passes:2, max_passes:5, b:[0.22516198]\nclass:0, count:31, passes:0, max_passes:5, b:[0.22516198]\nclass:0, count:32, passes:1, max_passes:5, b:[0.15999658]\nclass:0, count:33, passes:0, max_passes:5, b:[0.15999658]\nclass:0, count:34, passes:1, max_passes:5, b:[0.12142966]\nclass:0, count:35, passes:0, max_passes:5, b:[0.12142966]\nclass:0, count:36, passes:1, max_passes:5, b:[0.12142966]\nclass:0, count:37, passes:2, max_passes:5, b:[0.12142966]\nclass:0, count:38, passes:3, max_passes:5, b:[0.12142966]\nclass:0, count:39, passes:4, max_passes:5, b:[0.10679206]\nclass:0, count:40, passes:0, max_passes:5, b:[0.19104096]\nclass:0, count:41, passes:0, max_passes:5, b:[0.19104096]\nclass:0, count:42, passes:1, max_passes:5, b:[0.19345546]\nclass:0, count:43, passes:0, max_passes:5, b:[0.1524864]\nclass:0, count:44, passes:0, max_passes:5, b:[0.1524864]\nclass:0, count:45, passes:1, max_passes:5, b:[0.1524864]\nclass:0, count:46, passes:2, max_passes:5, b:[0.1524864]\nclass:0, count:47, passes:3, max_passes:5, b:[0.1524864]\nclass:0, count:48, passes:4, max_passes:5, b:[0.1524896]\nclass:0, count:49, passes:0, max_passes:5, b:[0.1524896]\nclass:0, count:50, passes:1, max_passes:5, b:[0.19187314]\nclass:0, count:51, passes:0, max_passes:5, b:[0.19187314]\nclass:0, count:52, passes:1, max_passes:5, b:[0.14778021]\nclass:0, count:53, passes:0, max_passes:5, b:[0.14778021]\nclass:0, count:54, passes:1, max_passes:5, b:[0.14778021]\nclass:0, count:55, passes:2, max_passes:5, b:[0.14778021]\nclass:0, count:56, passes:3, max_passes:5, b:[0.14778021]\nclass:0, count:57, passes:4, max_passes:5, b:[0.14778021]\nclass:0, count:58, passes:5, max_passes:5, b:[0.14778021]\nclass:1, count:0, passes:0, max_passes:5, b:[0.23948126]\nclass:1, count:1, passes:0, max_passes:5, b:[0.37157455]\nclass:1, count:2, passes:0, max_passes:5, b:[-0.04550336]\nclass:1, count:3, passes:0, max_passes:5, b:[-0.0551181]\nclass:1, count:4, passes:0, max_passes:5, b:[-0.24687747]\nclass:1, count:5, passes:0, max_passes:5, b:[-0.394826]\nclass:1, count:6, passes:0, max_passes:5, b:[0.07603732]\nclass:1, count:7, passes:0, max_passes:5, b:[-0.15695806]\nclass:1, count:8, passes:0, max_passes:5, b:[-0.50429906]\nclass:1, count:9, passes:0, max_passes:5, b:[-0.04754464]\nclass:1, count:10, passes:0, max_passes:5, b:[0.17972863]\nclass:1, count:11, passes:0, max_passes:5, b:[0.07725794]\nclass:1, count:12, passes:0, max_passes:5, b:[0.19627408]\nclass:1, count:13, passes:0, max_passes:5, b:[-0.33677391]\nclass:1, count:14, passes:0, max_passes:5, b:[-0.01858209]\nclass:1, count:15, passes:0, max_passes:5, b:[-0.12598679]\nclass:1, count:16, passes:0, max_passes:5, b:[-0.28976541]\nclass:1, count:17, passes:0, max_passes:5, b:[-0.24299192]\nclass:1, count:18, passes:0, max_passes:5, b:[-0.19650225]\nclass:1, count:19, passes:0, max_passes:5, b:[-0.19650225]\nclass:1, count:20, passes:1, max_passes:5, b:[0.1469696]\nclass:1, count:21, passes:0, max_passes:5, b:[-0.03086701]\nclass:1, count:22, passes:0, max_passes:5, b:[-0.16345051]\nclass:1, count:23, passes:0, max_passes:5, b:[-0.09680546]\nclass:1, count:24, passes:0, max_passes:5, b:[-0.18748951]\nclass:1, count:25, passes:0, max_passes:5, b:[-0.12888112]\nclass:1, count:26, passes:0, max_passes:5, b:[0.10083701]\nclass:1, count:27, passes:0, max_passes:5, b:[0.09903414]\nclass:1, count:28, passes:0, max_passes:5, b:[0.03714679]\nclass:1, count:29, passes:0, max_passes:5, b:[0.03206244]\nclass:1, count:30, passes:0, max_passes:5, b:[-0.24629792]\nclass:1, count:31, passes:0, max_passes:5, b:[-0.09052166]\nclass:1, count:32, passes:0, max_passes:5, b:[-0.09052166]\nclass:1, count:33, passes:1, max_passes:5, b:[-0.09052166]\nclass:1, count:34, passes:2, max_passes:5, b:[0.0382743]\nclass:1, count:35, passes:0, max_passes:5, b:[0.0382743]\nclass:1, count:36, passes:1, max_passes:5, b:[0.01273948]\nclass:1, count:37, passes:0, max_passes:5, b:[0.01273948]\nclass:1, count:38, passes:1, max_passes:5, b:[0.02345518]\nclass:1, count:39, passes:0, max_passes:5, b:[0.08384682]\nclass:1, count:40, passes:0, max_passes:5, b:[0.069193]\nclass:1, count:41, passes:0, max_passes:5, b:[0.069193]\nclass:1, count:42, passes:1, max_passes:5, b:[0.01700452]\nclass:1, count:43, passes:0, max_passes:5, b:[0.01700452]\nclass:1, count:44, passes:1, max_passes:5, b:[0.01700452]\nclass:1, count:45, passes:2, max_passes:5, b:[0.01700452]\nclass:1, count:46, passes:3, max_passes:5, b:[0.01700452]\nclass:1, count:47, passes:4, max_passes:5, b:[0.01700452]\nclass:1, count:48, passes:5, max_passes:5, b:[0.01700452]\nclass:2, count:0, passes:0, max_passes:5, b:[-0.73568162]\nclass:2, count:1, passes:0, max_passes:5, b:[-0.87318627]\nclass:2, count:2, passes:0, max_passes:5, b:[-1.21338612]\nclass:2, count:3, passes:0, max_passes:5, b:[-0.79750799]\nclass:2, count:4, passes:0, max_passes:5, b:[-1.50481684]\nclass:2, count:5, passes:0, max_passes:5, b:[-0.97488375]\nclass:2, count:6, passes:0, max_passes:5, b:[-0.91136988]\nclass:2, count:7, passes:0, max_passes:5, b:[-1.03578428]\nclass:2, count:8, passes:0, max_passes:5, b:[-0.72418581]\nclass:2, count:9, passes:0, max_passes:5, b:[-1.17018358]\nclass:2, count:10, passes:0, max_passes:5, b:[-1.11007012]\nclass:2, count:11, passes:0, max_passes:5, b:[-1.02414596]\nclass:2, count:12, passes:0, max_passes:5, b:[-0.87251539]\nclass:2, count:13, passes:0, max_passes:5, b:[-0.87251539]\nclass:2, count:14, passes:1, max_passes:5, b:[-0.8854712]\nclass:2, count:15, passes:0, max_passes:5, b:[-0.93173472]\nclass:2, count:16, passes:0, max_passes:5, b:[-0.99152742]\nclass:2, count:17, passes:0, max_passes:5, b:[-0.87758742]\nclass:2, count:18, passes:0, max_passes:5, b:[-0.69513696]\nclass:2, count:19, passes:0, max_passes:5, b:[-1.1803195]\nclass:2, count:20, passes:0, max_passes:5, b:[-1.2681486]\nclass:2, count:21, passes:0, max_passes:5, b:[-1.24643611]\nclass:2, count:22, passes:0, max_passes:5, b:[-1.24643611]\nclass:2, count:23, passes:1, max_passes:5, b:[-0.93877092]\nclass:2, count:24, passes:0, max_passes:5, b:[-1.16775087]\nclass:2, count:25, passes:0, max_passes:5, b:[-1.04425998]\nclass:2, count:26, passes:0, max_passes:5, b:[-0.67341059]\nclass:2, count:27, passes:0, max_passes:5, b:[-0.67341059]\nclass:2, count:28, passes:1, max_passes:5, b:[-0.67341059]\nclass:2, count:29, passes:2, max_passes:5, b:[-0.67341059]\nclass:2, count:30, passes:3, max_passes:5, b:[-0.96325525]\nclass:2, count:31, passes:0, max_passes:5, b:[-0.95789901]\nclass:2, count:32, passes:0, max_passes:5, b:[-0.91457318]\nclass:2, count:33, passes:0, max_passes:5, b:[-0.91457318]\nclass:2, count:34, passes:1, max_passes:5, b:[-0.90396258]\nclass:2, count:35, passes:0, max_passes:5, b:[-0.90396258]\nclass:2, count:36, passes:1, max_passes:5, b:[-0.90396258]\nclass:2, count:37, passes:2, max_passes:5, b:[-0.68831406]\nclass:2, count:38, passes:0, max_passes:5, b:[-0.61430142]\nclass:2, count:39, passes:0, max_passes:5, b:[-0.75521572]\nclass:2, count:40, passes:0, max_passes:5, b:[-0.75521572]\nclass:2, count:41, passes:1, max_passes:5, b:[-0.75521572]\nclass:2, count:42, passes:2, max_passes:5, b:[-0.75521572]\nclass:2, count:43, passes:3, max_passes:5, b:[-0.75521572]\nclass:2, count:44, passes:4, max_passes:5, b:[-0.75521572]\nclass:2, count:45, passes:5, max_passes:5, b:[-0.75521572]\nPerformance on the training set\n[[44  0  0]\n [ 0 53  0]\n [ 0  0 36]]\n"
    }
   ],
   "source": [
    "exponent = 2\n",
    "intercept =1\n",
    "svm_model =SVM_kCLasses(X_train, y_train, C = 10, tol = 0.001, max_passes = 5, kernel_choice = \"poly\", parameters=[exponent, intercept], k=k)\n",
    "support_vectors, support_alphas, support_target = svm_model.fit()\n",
    "pred = svm_model.prediction(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the test set\n[[15  0  0]\n [ 1 17  0]\n [ 0  1 11]]\n"
    }
   ],
   "source": [
    "pred = svm_model.prediction(X_test)\n",
    "print(\"Performance on the test set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 1, chapter 6 and Chapter 7 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 6: (https://www.youtube.com/watch?v=qyyJKd-zXRE)\n",
    "* Andrew Ng, Lec 7: (https://www.youtube.com/watch?v=s8B4A5ubw6c)\n",
    "* Andrew Ng, Lec 8: (https://www.youtube.com/watch?v=bUv9bfMPMb4)\n",
    "* Simplified Sequential Minimal Optimization: (https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiRlObmw5_qAhW7ShUIHSjJAbYQFjAAegQIAhAB&url=http%3A%2F%2Fcs229.stanford.edu%2Fmaterials%2Fsmo.pdf&usg=AOvVaw201bQxVZY0MmUn_gGAu5O8)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}