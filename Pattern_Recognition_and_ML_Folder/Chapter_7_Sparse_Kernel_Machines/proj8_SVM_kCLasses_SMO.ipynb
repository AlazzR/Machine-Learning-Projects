{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Vector Machine for K-Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(112, 5)\n(38, 5)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 0, 2, 1, 1, 0, 0, 1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1,\n       2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0,\n       0, 1, 1, 0, 2, 0, 0, 1, 1, 2, 1, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 1,\n       2, 0, 2, 2, 0, 1, 1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1,\n       2, 2, 0, 1, 2, 2, 0, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 0,\n       1, 2])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "#X, y = sklearn.datasets.load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "X_train = standard.fit_transform(X_train)\n",
    "training_data = np.c_[X_train, y_train]#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "\n",
    "X_test = standard.transform(X_test)\n",
    "test_data = np.c_[X_test, y_test]\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)\n",
    "k = len(set(y_train))\n",
    "y_train#It needs to be labeled from 0 to k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM2CLASS(object):\n",
    "\n",
    "    def __init__(self, X_train, y_train, C = 10, tol = 0.001, max_passes = 5, passes = 0, classType = 0):\n",
    "        self.classType = classType\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.max_passes = max_passes\n",
    "        self.passes = passes\n",
    "        self.b = 0\n",
    "        self.alphas = np.zeros((self.X_train.shape[0], 1))#need to be of size nx1\n",
    "        self.coefficients = ()\n",
    "        self.kernel_type ={\"poly\": self.polynomial_kernel, \"gauss\": self.gaussian_kernel}\n",
    "        self.kernel_parameters = []\n",
    "        self.kernel_choice = \"\"\n",
    "\n",
    "    def polynomial_kernel(self, x_i, x_j, a):\n",
    "        return np.power(np.dot(x_i.T, x_j) + a[0], a[1])\n",
    "\n",
    "    def gaussian_kernel(self, x, x_star, sigma):\n",
    "        return np.exp(np.divide(-1*(np.linalg.norm(x-x_star)**2), 2*sigma**2))\n",
    "    \n",
    "    def SMO(self, kernel_choice, parameters):\n",
    "\n",
    "        choices = np.arange(0, self.y_train.shape[0])\n",
    "        count = 0\n",
    "        self.kernel_choice = kernel_choice\n",
    "        #Better to construct the Kernel matrix from the scratch for efficiency, but this method will prevent the simplified SMO from working on large datasets, like, the mnist dataset \n",
    "        if kernel_choice == \"poly\":\n",
    "            exponent = parameters[0]\n",
    "            intercept = parameters[1]\n",
    "            self.kernel_parameters = [intercept, exponent] \n",
    "            \n",
    "            K = np.zeros((self.X_train.shape[0], self.X_train.shape[0]))\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "                for j in range(0, self.X_train.shape[0]):\n",
    "                    K[i, j] = self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "            assert(np.any(np.linalg.eig(K)[0] == 0)  == False)#Test for PSD\n",
    "\n",
    "        elif kernel_choice == \"gauss\":\n",
    "            self.kernel_parameters = 2\n",
    "\n",
    "            K = np.zeros((self.X_train.shape[0], self.X_train.shape[0]))\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "                for j in range(0, self.X_train.shape[0]):\n",
    "                    K[i, j] =  self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "            assert(np.linalg.det(K) != 0)#Test for PSD\n",
    "        \n",
    "        else:\n",
    "            print(\"Wrong entry\")\n",
    "            return -1\n",
    "\n",
    "        while(self.passes <= self.max_passes):\n",
    "        #begin while\n",
    "            num_changed_alphas = 0\n",
    "            for i in range(0, self.X_train.shape[0]):\n",
    "            #begin for\n",
    "                #compute kernel function for every iteration from scratch\n",
    "                #f_x_i = np.sum( list(map(lambda x, alpha, y: alpha * y * self.kernel_type[self.kernel_choice](x, self.X_train[i, :], self.kernel_parameters) , self.X_train, self.alphas, self.y_train) ) )#instead of calculating the Kernel matrix from the start, we will calculate the inner product with each iteration in order to mitigate the problem of having a large kernel matrix that will raise an exception\n",
    "                #print(f_x_i)\n",
    "                f_x_i = np.sum(self.alphas.reshape(-1, 1) * (self.y_train.reshape(-1, 1) *  K[:, i].reshape(-1, 1)).reshape(-1, 1)) \n",
    "                E_i = f_x_i + self.b - self.y_train[i]\n",
    "                #print(f_x_i)\n",
    "                #Check if we satisfy the condition for the dual problem\n",
    "                if (((self.y_train[i] * E_i) < -self.tol) and (self.alphas[i] < self.C)) or (((self.y_train[i] * E_i) > self.tol) and (self.alphas[i] > 0)):\n",
    "                #begin if\n",
    "                    j = np.random.choice( list(filter(lambda v: v == v, list(map(lambda c: c if c != i else np.nan, choices)))) ) \n",
    "                    #only nan will generate False at its equlaity, and the filter object will end up filtering out these wrong values\n",
    "                    assert( i != j)\n",
    "                    #f_x_j = np.sum( list(map(lambda x, alpha, y: alpha * y * self.kernel_type[self.kernel_choice](x, self.X_train[j, :], self.kernel_parameters) , self.X_train, self.alphas, self.y_train) ) )\n",
    "                    #print(f_x_j)\n",
    "                    f_x_j = np.sum(self.alphas.reshape(-1, 1) * (self.y_train.reshape(-1, 1) * K[:, j].reshape(-1, 1)).reshape(-1, 1)) \n",
    "                    #print((alphas.reshape(-1, 1) * (y_train.reshape(-1, 1) * K[i, :].reshape(-1, 1)).reshape(-1, 1)).shape)\n",
    "                    #print(f_x_j)\n",
    "                    E_j = f_x_j + self.b - self.y_train[j]\n",
    "\n",
    "                    alpha_i_old = self.alphas[i].copy()#Needs to copy the value because otherwise they would be pointing to the same address\n",
    "                    alpha_j_old = self.alphas[j].copy()\n",
    "\n",
    "                    #Computing L and H\n",
    "                    if(self.y_train[i] != self.y_train[j]):\n",
    "                        L = max(0, self.alphas[j] - self.alphas[i])\n",
    "                        H = min(self.C, self.C + self.alphas[j] - self.alphas[i])\n",
    "                    else:\n",
    "                        L = max(0, self.alphas[j] + self.alphas[i] - self.C)\n",
    "                        H = min(self.C, self.alphas[j] + self.alphas[i])\n",
    "                    \n",
    "                    #Checking if L=H which indicate that the alpha would certainly wouldn't change \n",
    "                    if L == H:\n",
    "                        continue\n",
    "\n",
    "                    #eta = 2 * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters) - self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[i, :], self.kernel_parameters) - self.kernel_type[self.kernel_choice](self.X_train[j, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    eta = 2 * K[i, j] - K[i, i] - K[j, j] \n",
    "\n",
    "                    #eta = 0 if the similarity between x_i and x_j is as the combination of the similarity of x_i with itself and same goes for x_j, will cause an exception to happend and this indicate we are dealing with the same x.\n",
    "                    #eta > 0 if if the similarity between x_i and x_j is higher than the combination of the similarity of x_i with itself and same goes for x_j, so, this update step would have little effect on the converging to the optimal minimum and may leads to diverging the algorithm patht to a worse path\n",
    "                    #eta < 0 there are small simialrity between x_i and x_j, so, this would help in discoverign the interaction of those observations in the feature space\n",
    "\n",
    "                    if eta >= 0:\n",
    "                        #print(\"The two vectors are too similar\")\n",
    "                        continue\n",
    "\n",
    "                    alpha_j_clip = alpha_j_old - (1/eta) * self.y_train[j] * (E_i - E_j)\n",
    "\n",
    "                    if alpha_j_clip > H:\n",
    "                        self.alphas[j] = H\n",
    "                    elif alpha_j_clip < L:\n",
    "                        self.alphas[j] = L\n",
    "                    else:\n",
    "                        self.alphas[j]  = alpha_j_clip\n",
    "                    \n",
    "                    #print(alphas[j], alpha_j_old)\n",
    "                    #Check if it is worth to update alpha_i\n",
    "                    if(abs(self.alphas[j] - alpha_j_old) < 1e-3):\n",
    "                        #print(\"No noticeable changes happened to alpha\")\n",
    "                        continue\n",
    "                    self.alphas[i] = alpha_i_old + self.y_train[i] * self.y_train[j] * (alpha_j_old - self.alphas[j])#The signs changed from  the negative sign for updating alpha_i\n",
    "\n",
    "                    ##KKT constrains convergence test\n",
    "                    #b1 = self.b - E_i - self.y_train[i]*(self.alphas[i] - alpha_i_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[i, :], self.kernel_parameters) - self.y_train[j]*(self.alphas[j] - alpha_j_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    b1 = self.b - E_i - self.y_train[i] * (self.alphas[i] - alpha_i_old) * K[i, i] - self.y_train[j] * (self.alphas[j] - alpha_j_old) * K[i, j]\n",
    "\n",
    "                    #b2 = self.b - E_j - self.y_train[i]*(self.alphas[i] - alpha_i_old) * self.kernel_type[self.kernel_choice](self.X_train[i, :], self.X_train[j, :], self.kernel_parameters) - self.y_train[j]*(self.alphas[j] - alpha_j_old) * self.kernel_type[self.kernel_choice](self.X_train[j, :], self.X_train[j, :], self.kernel_parameters)\n",
    "                    b2 = self.b - E_j - self.y_train[i] * (self.alphas[i] - alpha_i_old) * K[i, j] - self.y_train[j] * (self.alphas[j] - alpha_j_old) * K[j, j]\n",
    "\n",
    "                    if (self.alphas[j] > 0) and (self.alphas[j] < self.C):\n",
    "                        self.b = b2\n",
    "                    elif (self.alphas[i] > 0) and (self.alphas[i] < self.C):\n",
    "                        self.b = b1\n",
    "                    else:\n",
    "                        self.b = (b1 + b2)/2\n",
    "                    \n",
    "                    num_changed_alphas  =  num_changed_alphas + 1\n",
    "                #end if\n",
    "            #end for\n",
    "            print(f\"class:{self.classType}, count:{count}, passes:{self.passes}, max_passes:{self.max_passes}, b:{self.b}\")\n",
    "            count+=1\n",
    "            if(num_changed_alphas == 0):\n",
    "                self.passes = self.passes + 1\n",
    "            else:\n",
    "                self.passes = 0\n",
    "        #end while\n",
    "        #Only store in the memory the support vectors\n",
    "        support_indeces = np.argwhere(self.alphas != 0)[:, 0]\n",
    "        support_vectors = self.X_train[support_indeces, :]\n",
    "        support_alphas = self.alphas[support_indeces]\n",
    "        support_target = self.y_train[support_indeces]\n",
    "        self.importantParameters = (support_vectors, support_alphas, support_target, self.b)\n",
    "\n",
    "        return self.importantParameters\n",
    "\n",
    "    def prediction_dataset(self, X):\n",
    "        pred = list(map(lambda x: self.prediction(x), X))\n",
    "        return pred\n",
    "\n",
    "    def prediction(self, x):\n",
    "        t1 = np.sum( list(map(lambda x1, alpha, y: y * alpha * self.kernel_type[self.kernel_choice](x, x1, self.kernel_parameters), self.importantParameters[0], self.importantParameters[1], self.importantParameters[2])) )\n",
    "        pred = t1 + self.b\n",
    "        #print(pred)\n",
    "        if pred >=0:\n",
    "            return 1\n",
    "    \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_kCLasses(object):\n",
    "    \n",
    "    #To simplify things I will assume that I will use the same kernel function for all of the combination of 1 vs K models\n",
    "    def __init__(self, X_train, y_train, C = 10, tol = 0.001, max_passes = 5, kernel_choice = \"poly\", parameters=[1, 1], k=None):\n",
    "        assert(k != None)\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.max_passes = max_passes\n",
    "        self.kernel_choice = kernel_choice\n",
    "        self.parameters = parameters\n",
    "        self.K = k \n",
    "        self.alphas = []\n",
    "        self.b = []\n",
    "        self.coefficients = []\n",
    "        self.classes = []\n",
    "        self.models = []\n",
    "        self.X_train = X_train\n",
    "        for c in range(0, self.K):\n",
    "            temp = np.array(list(map(lambda y: 1 if y == c else -1, y_train)))\n",
    "            self.classes.append(temp)\n",
    "        \n",
    "    def fit(self):\n",
    "        for k in range(0, self.K):\n",
    "            self.models.append(SVM2CLASS(self.X_train, self.classes[k], self.C, self.tol, self.max_passes, 0, k))\n",
    "            support_vectors, support_alphas, support_target, b = self.models[k].SMO(self.kernel_choice, self.parameters)\n",
    "            self.alphas.append(support_alphas)\n",
    "            self.b.append(b)\n",
    "            self.coefficients.append((support_vectors, support_alphas, support_target, b))\n",
    "        return self.coefficients\n",
    "\n",
    "    def prediction(self, X):\n",
    "        pred = np.zeros((X.shape[0], self.K))\n",
    "        for k in range(0, self.K):\n",
    "            pred[:, k] = self.models[k].prediction_dataset(X_train)\n",
    "            \n",
    "        final_prediction = np.argmax(pred, axis=1)#along the rows\n",
    "        return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "s:2, count:256, passes:0, max_passes:5, b:[-3.67725222]\nclass:2, count:257, passes:1, max_passes:5, b:[-3.79949908]\nclass:2, count:258, passes:0, max_passes:5, b:[-3.61079323]\nclass:2, count:259, passes:0, max_passes:5, b:[-3.61079323]\nclass:2, count:260, passes:1, max_passes:5, b:[-3.61079323]\nclass:2, count:261, passes:2, max_passes:5, b:[-3.77297795]\nclass:2, count:262, passes:0, max_passes:5, b:[-3.78057963]\nclass:2, count:263, passes:0, max_passes:5, b:[-3.78057963]\nclass:2, count:264, passes:1, max_passes:5, b:[-3.78057963]\nclass:2, count:265, passes:2, max_passes:5, b:[-3.78057963]\nclass:2, count:266, passes:3, max_passes:5, b:[-3.78057963]\nclass:2, count:267, passes:4, max_passes:5, b:[-3.78057963]\nclass:2, count:268, passes:5, max_passes:5, b:[-4.12289863]\nclass:2, count:269, passes:0, max_passes:5, b:[-3.90832183]\nclass:2, count:270, passes:0, max_passes:5, b:[-3.90832183]\nclass:2, count:271, passes:1, max_passes:5, b:[-3.90832183]\nclass:2, count:272, passes:2, max_passes:5, b:[-3.73102291]\nclass:2, count:273, passes:0, max_passes:5, b:[-3.79253363]\nclass:2, count:274, passes:0, max_passes:5, b:[-3.8685961]\nclass:2, count:275, passes:0, max_passes:5, b:[-3.8685961]\nclass:2, count:276, passes:1, max_passes:5, b:[-3.8685961]\nclass:2, count:277, passes:2, max_passes:5, b:[-3.98850919]\nclass:2, count:278, passes:0, max_passes:5, b:[-3.74357428]\nclass:2, count:279, passes:0, max_passes:5, b:[-3.84503185]\nclass:2, count:280, passes:0, max_passes:5, b:[-3.77940664]\nclass:2, count:281, passes:0, max_passes:5, b:[-4.0432067]\nclass:2, count:282, passes:0, max_passes:5, b:[-3.7899134]\nclass:2, count:283, passes:0, max_passes:5, b:[-3.7899134]\nclass:2, count:284, passes:1, max_passes:5, b:[-3.7899134]\nclass:2, count:285, passes:2, max_passes:5, b:[-3.8301056]\nclass:2, count:286, passes:0, max_passes:5, b:[-3.79734182]\nclass:2, count:287, passes:0, max_passes:5, b:[-3.92691663]\nclass:2, count:288, passes:0, max_passes:5, b:[-3.76740036]\nclass:2, count:289, passes:0, max_passes:5, b:[-3.76740036]\nclass:2, count:290, passes:1, max_passes:5, b:[-4.61531693]\nclass:2, count:291, passes:0, max_passes:5, b:[-3.89497781]\nclass:2, count:292, passes:0, max_passes:5, b:[-3.97948097]\nclass:2, count:293, passes:0, max_passes:5, b:[-3.91486525]\nclass:2, count:294, passes:0, max_passes:5, b:[-3.76077204]\nclass:2, count:295, passes:0, max_passes:5, b:[-3.76077204]\nclass:2, count:296, passes:1, max_passes:5, b:[-3.87847121]\nclass:2, count:297, passes:0, max_passes:5, b:[-3.86216758]\nclass:2, count:298, passes:0, max_passes:5, b:[-3.86216758]\nclass:2, count:299, passes:1, max_passes:5, b:[-3.86216758]\nclass:2, count:300, passes:2, max_passes:5, b:[-3.98384453]\nclass:2, count:301, passes:0, max_passes:5, b:[-3.98384453]\nclass:2, count:302, passes:1, max_passes:5, b:[-3.42809473]\nclass:2, count:303, passes:0, max_passes:5, b:[-3.42809473]\nclass:2, count:304, passes:1, max_passes:5, b:[-3.42809473]\nclass:2, count:305, passes:2, max_passes:5, b:[-3.48950116]\nclass:2, count:306, passes:0, max_passes:5, b:[-3.85037892]\nclass:2, count:307, passes:0, max_passes:5, b:[-3.92817986]\nclass:2, count:308, passes:0, max_passes:5, b:[-4.00757488]\nclass:2, count:309, passes:0, max_passes:5, b:[-3.96559742]\nclass:2, count:310, passes:0, max_passes:5, b:[-4.08911524]\nclass:2, count:311, passes:0, max_passes:5, b:[-4.08911524]\nclass:2, count:312, passes:1, max_passes:5, b:[-4.08911524]\nclass:2, count:313, passes:2, max_passes:5, b:[-4.08911524]\nclass:2, count:314, passes:3, max_passes:5, b:[-4.09924401]\nclass:2, count:315, passes:0, max_passes:5, b:[-4.09924401]\nclass:2, count:316, passes:1, max_passes:5, b:[-4.09924401]\nclass:2, count:317, passes:2, max_passes:5, b:[-4.09924401]\nclass:2, count:318, passes:3, max_passes:5, b:[-4.09924401]\nclass:2, count:319, passes:4, max_passes:5, b:[-3.9709152]\nclass:2, count:320, passes:0, max_passes:5, b:[-3.9709152]\nclass:2, count:321, passes:1, max_passes:5, b:[-3.88258086]\nclass:2, count:322, passes:0, max_passes:5, b:[-4.70808708]\nclass:2, count:323, passes:0, max_passes:5, b:[-4.20935581]\nclass:2, count:324, passes:0, max_passes:5, b:[-4.188566]\nclass:2, count:325, passes:0, max_passes:5, b:[-3.8483558]\nclass:2, count:326, passes:0, max_passes:5, b:[-3.90565756]\nclass:2, count:327, passes:0, max_passes:5, b:[-4.13842022]\nclass:2, count:328, passes:0, max_passes:5, b:[-3.88830628]\nclass:2, count:329, passes:0, max_passes:5, b:[-4.2055073]\nclass:2, count:330, passes:0, max_passes:5, b:[-4.22727297]\nclass:2, count:331, passes:0, max_passes:5, b:[-4.18437169]\nclass:2, count:332, passes:0, max_passes:5, b:[-3.9992305]\nclass:2, count:333, passes:0, max_passes:5, b:[-3.9992305]\nclass:2, count:334, passes:1, max_passes:5, b:[-4.18317863]\nclass:2, count:335, passes:0, max_passes:5, b:[-4.18317863]\nclass:2, count:336, passes:1, max_passes:5, b:[-4.15211576]\nclass:2, count:337, passes:0, max_passes:5, b:[-4.15211576]\nclass:2, count:338, passes:1, max_passes:5, b:[-4.15211576]\nclass:2, count:339, passes:2, max_passes:5, b:[-4.19558084]\nclass:2, count:340, passes:0, max_passes:5, b:[-4.19558084]\nclass:2, count:341, passes:1, max_passes:5, b:[-4.19558084]\nclass:2, count:342, passes:2, max_passes:5, b:[-4.05364478]\nclass:2, count:343, passes:0, max_passes:5, b:[-4.14830744]\nclass:2, count:344, passes:0, max_passes:5, b:[-4.28134733]\nclass:2, count:345, passes:0, max_passes:5, b:[-4.12047726]\nclass:2, count:346, passes:0, max_passes:5, b:[-4.12636361]\nclass:2, count:347, passes:0, max_passes:5, b:[-4.33702146]\nclass:2, count:348, passes:0, max_passes:5, b:[-4.33702146]\nclass:2, count:349, passes:1, max_passes:5, b:[-4.33702146]\nclass:2, count:350, passes:2, max_passes:5, b:[-4.20833583]\nclass:2, count:351, passes:0, max_passes:5, b:[-4.30150541]\nclass:2, count:352, passes:0, max_passes:5, b:[-4.19970635]\nclass:2, count:353, passes:0, max_passes:5, b:[-4.29745558]\nclass:2, count:354, passes:0, max_passes:5, b:[-4.29745558]\nclass:2, count:355, passes:1, max_passes:5, b:[-4.01833123]\nclass:2, count:356, passes:0, max_passes:5, b:[-4.15686064]\nclass:2, count:357, passes:0, max_passes:5, b:[-4.15686064]\nclass:2, count:358, passes:1, max_passes:5, b:[-4.00053248]\nclass:2, count:359, passes:0, max_passes:5, b:[-4.19185241]\nclass:2, count:360, passes:0, max_passes:5, b:[-4.18295428]\nclass:2, count:361, passes:0, max_passes:5, b:[-4.18295428]\nclass:2, count:362, passes:1, max_passes:5, b:[-4.18295428]\nclass:2, count:363, passes:2, max_passes:5, b:[-4.18295428]\nclass:2, count:364, passes:3, max_passes:5, b:[-4.18295428]\nclass:2, count:365, passes:4, max_passes:5, b:[-4.04499798]\nclass:2, count:366, passes:0, max_passes:5, b:[-4.12545281]\nclass:2, count:367, passes:0, max_passes:5, b:[-4.07196932]\nclass:2, count:368, passes:0, max_passes:5, b:[-4.17216844]\nclass:2, count:369, passes:0, max_passes:5, b:[-4.04815379]\nclass:2, count:370, passes:0, max_passes:5, b:[-4.19053868]\nclass:2, count:371, passes:0, max_passes:5, b:[-4.19053868]\nclass:2, count:372, passes:1, max_passes:5, b:[-4.1464255]\nclass:2, count:373, passes:0, max_passes:5, b:[-4.0680297]\nclass:2, count:374, passes:0, max_passes:5, b:[-4.13220586]\nclass:2, count:375, passes:0, max_passes:5, b:[-4.13220586]\nclass:2, count:376, passes:1, max_passes:5, b:[-4.13220586]\nclass:2, count:377, passes:2, max_passes:5, b:[-4.13220586]\nclass:2, count:378, passes:3, max_passes:5, b:[-4.13220586]\nclass:2, count:379, passes:4, max_passes:5, b:[-4.33441114]\nclass:2, count:380, passes:0, max_passes:5, b:[-4.33441114]\nclass:2, count:381, passes:1, max_passes:5, b:[-4.09808745]\nclass:2, count:382, passes:0, max_passes:5, b:[-4.2820258]\nclass:2, count:383, passes:0, max_passes:5, b:[-4.05771768]\nclass:2, count:384, passes:0, max_passes:5, b:[-4.05771768]\nclass:2, count:385, passes:1, max_passes:5, b:[-4.03413814]\nclass:2, count:386, passes:0, max_passes:5, b:[-4.16338409]\nclass:2, count:387, passes:0, max_passes:5, b:[-4.32429528]\nclass:2, count:388, passes:0, max_passes:5, b:[-3.6227727]\nclass:2, count:389, passes:0, max_passes:5, b:[-3.6227727]\nclass:2, count:390, passes:1, max_passes:5, b:[-4.22562864]\nclass:2, count:391, passes:0, max_passes:5, b:[-4.29880035]\nclass:2, count:392, passes:0, max_passes:5, b:[-4.29880035]\nclass:2, count:393, passes:1, max_passes:5, b:[-4.13322662]\nclass:2, count:394, passes:0, max_passes:5, b:[-4.13322662]\nclass:2, count:395, passes:1, max_passes:5, b:[-4.20789647]\nclass:2, count:396, passes:0, max_passes:5, b:[-4.20789647]\nclass:2, count:397, passes:1, max_passes:5, b:[-4.17660012]\nclass:2, count:398, passes:0, max_passes:5, b:[-3.63915314]\nclass:2, count:399, passes:0, max_passes:5, b:[-3.63915314]\nclass:2, count:400, passes:1, max_passes:5, b:[-4.3146223]\nclass:2, count:401, passes:0, max_passes:5, b:[-4.34938695]\nclass:2, count:402, passes:0, max_passes:5, b:[-4.18401578]\nclass:2, count:403, passes:0, max_passes:5, b:[-4.18401578]\nclass:2, count:404, passes:1, max_passes:5, b:[-4.18401578]\nclass:2, count:405, passes:2, max_passes:5, b:[-4.21424877]\nclass:2, count:406, passes:0, max_passes:5, b:[-4.39087193]\nclass:2, count:407, passes:0, max_passes:5, b:[-4.2151302]\nclass:2, count:408, passes:0, max_passes:5, b:[-4.37862421]\nclass:2, count:409, passes:0, max_passes:5, b:[-4.37862421]\nclass:2, count:410, passes:1, max_passes:5, b:[-4.27389339]\nclass:2, count:411, passes:0, max_passes:5, b:[-4.27389339]\nclass:2, count:412, passes:1, max_passes:5, b:[-4.37896766]\nclass:2, count:413, passes:0, max_passes:5, b:[-4.2396702]\nclass:2, count:414, passes:0, max_passes:5, b:[-4.25018693]\nclass:2, count:415, passes:0, max_passes:5, b:[-4.3807515]\nclass:2, count:416, passes:0, max_passes:5, b:[-4.27512467]\nclass:2, count:417, passes:0, max_passes:5, b:[-4.27512467]\nclass:2, count:418, passes:1, max_passes:5, b:[-4.27512467]\nclass:2, count:419, passes:2, max_passes:5, b:[-4.23117786]\nclass:2, count:420, passes:0, max_passes:5, b:[-4.4137773]\nclass:2, count:421, passes:0, max_passes:5, b:[-4.26253405]\nclass:2, count:422, passes:0, max_passes:5, b:[-4.26253405]\nclass:2, count:423, passes:1, max_passes:5, b:[-4.26253405]\nclass:2, count:424, passes:2, max_passes:5, b:[-4.425622]\nclass:2, count:425, passes:0, max_passes:5, b:[-4.425622]\nclass:2, count:426, passes:1, max_passes:5, b:[-4.48265735]\nclass:2, count:427, passes:0, max_passes:5, b:[-4.48265735]\nclass:2, count:428, passes:1, max_passes:5, b:[-4.48265735]\nclass:2, count:429, passes:2, max_passes:5, b:[-4.32605732]\nclass:2, count:430, passes:0, max_passes:5, b:[-4.32605732]\nclass:2, count:431, passes:1, max_passes:5, b:[-4.32605732]\nclass:2, count:432, passes:2, max_passes:5, b:[-4.55417754]\nclass:2, count:433, passes:0, max_passes:5, b:[-4.6979969]\nclass:2, count:434, passes:0, max_passes:5, b:[-4.32380052]\nclass:2, count:435, passes:0, max_passes:5, b:[-4.50437179]\nclass:2, count:436, passes:0, max_passes:5, b:[-4.50437179]\nclass:2, count:437, passes:1, max_passes:5, b:[-4.49869052]\nclass:2, count:438, passes:0, max_passes:5, b:[-4.44654329]\nclass:2, count:439, passes:0, max_passes:5, b:[-4.33668099]\nclass:2, count:440, passes:0, max_passes:5, b:[-4.57888691]\nclass:2, count:441, passes:0, max_passes:5, b:[-4.57888691]\nclass:2, count:442, passes:1, max_passes:5, b:[-4.23897349]\nclass:2, count:443, passes:0, max_passes:5, b:[-4.49959304]\nclass:2, count:444, passes:0, max_passes:5, b:[-4.74172722]\nclass:2, count:445, passes:0, max_passes:5, b:[-4.74172722]\nclass:2, count:446, passes:1, max_passes:5, b:[-4.33064751]\nclass:2, count:447, passes:0, max_passes:5, b:[-4.52337542]\nclass:2, count:448, passes:0, max_passes:5, b:[-4.57158368]\nclass:2, count:449, passes:0, max_passes:5, b:[-4.55316289]\nclass:2, count:450, passes:0, max_passes:5, b:[-4.55316289]\nclass:2, count:451, passes:1, max_passes:5, b:[-4.55316289]\nclass:2, count:452, passes:2, max_passes:5, b:[-4.72856818]\nclass:2, count:453, passes:0, max_passes:5, b:[-4.42582992]\nclass:2, count:454, passes:0, max_passes:5, b:[-4.42582992]\nclass:2, count:455, passes:1, max_passes:5, b:[-4.41462287]\nclass:2, count:456, passes:0, max_passes:5, b:[-4.41462287]\nclass:2, count:457, passes:1, max_passes:5, b:[-4.41462287]\nclass:2, count:458, passes:2, max_passes:5, b:[-4.43418256]\nclass:2, count:459, passes:0, max_passes:5, b:[-4.51931316]\nclass:2, count:460, passes:0, max_passes:5, b:[-4.5192753]\nclass:2, count:461, passes:0, max_passes:5, b:[-4.55607836]\nclass:2, count:462, passes:0, max_passes:5, b:[-4.44013596]\nclass:2, count:463, passes:0, max_passes:5, b:[-4.40597679]\nclass:2, count:464, passes:0, max_passes:5, b:[-4.42534763]\nclass:2, count:465, passes:0, max_passes:5, b:[-4.41804364]\nclass:2, count:466, passes:0, max_passes:5, b:[-4.41804364]\nclass:2, count:467, passes:1, max_passes:5, b:[-4.5457336]\nclass:2, count:468, passes:0, max_passes:5, b:[-4.46374309]\nclass:2, count:469, passes:0, max_passes:5, b:[-4.50612678]\nclass:2, count:470, passes:0, max_passes:5, b:[-4.42017065]\nclass:2, count:471, passes:0, max_passes:5, b:[-4.42017065]\nclass:2, count:472, passes:1, max_passes:5, b:[-4.42017065]\nclass:2, count:473, passes:2, max_passes:5, b:[-4.43677073]\nclass:2, count:474, passes:0, max_passes:5, b:[-4.43677073]\nclass:2, count:475, passes:1, max_passes:5, b:[-4.43677073]\nclass:2, count:476, passes:2, max_passes:5, b:[-4.52594248]\nclass:2, count:477, passes:0, max_passes:5, b:[-4.56184941]\nclass:2, count:478, passes:0, max_passes:5, b:[-4.56184941]\nclass:2, count:479, passes:1, max_passes:5, b:[-4.56184941]\nclass:2, count:480, passes:2, max_passes:5, b:[-4.56184941]\nclass:2, count:481, passes:3, max_passes:5, b:[-4.56184941]\nclass:2, count:482, passes:4, max_passes:5, b:[-4.28192293]\nclass:2, count:483, passes:0, max_passes:5, b:[-4.40498762]\nclass:2, count:484, passes:0, max_passes:5, b:[-4.48343836]\nclass:2, count:485, passes:0, max_passes:5, b:[-4.45280007]\nclass:2, count:486, passes:0, max_passes:5, b:[-4.43326577]\nclass:2, count:487, passes:0, max_passes:5, b:[-4.43326577]\nclass:2, count:488, passes:1, max_passes:5, b:[-4.49160659]\nclass:2, count:489, passes:0, max_passes:5, b:[-4.60593623]\nclass:2, count:490, passes:0, max_passes:5, b:[-4.44980798]\nclass:2, count:491, passes:0, max_passes:5, b:[-4.42025723]\nclass:2, count:492, passes:0, max_passes:5, b:[-4.45740917]\nclass:2, count:493, passes:0, max_passes:5, b:[-4.44313563]\nclass:2, count:494, passes:0, max_passes:5, b:[-4.44313563]\nclass:2, count:495, passes:1, max_passes:5, b:[-4.44313563]\nclass:2, count:496, passes:2, max_passes:5, b:[-4.44966027]\nclass:2, count:497, passes:0, max_passes:5, b:[-4.44352863]\nclass:2, count:498, passes:0, max_passes:5, b:[-4.44352863]\nclass:2, count:499, passes:1, max_passes:5, b:[-4.44352863]\nclass:2, count:500, passes:2, max_passes:5, b:[-4.44352863]\nclass:2, count:501, passes:3, max_passes:5, b:[-4.44352863]\nclass:2, count:502, passes:4, max_passes:5, b:[-4.71529509]\nclass:2, count:503, passes:0, max_passes:5, b:[-4.48747516]\nclass:2, count:504, passes:0, max_passes:5, b:[-4.48747516]\nclass:2, count:505, passes:1, max_passes:5, b:[-4.48747516]\nclass:2, count:506, passes:2, max_passes:5, b:[-4.43543622]\nclass:2, count:507, passes:0, max_passes:5, b:[-4.43543622]\nclass:2, count:508, passes:1, max_passes:5, b:[-4.43543622]\nclass:2, count:509, passes:2, max_passes:5, b:[-4.46427734]\nclass:2, count:510, passes:0, max_passes:5, b:[-4.46427734]\nclass:2, count:511, passes:1, max_passes:5, b:[-4.55131439]\nclass:2, count:512, passes:0, max_passes:5, b:[-4.55131439]\nclass:2, count:513, passes:1, max_passes:5, b:[-4.58790221]\nclass:2, count:514, passes:0, max_passes:5, b:[-4.54790962]\nclass:2, count:515, passes:0, max_passes:5, b:[-4.54790962]\nclass:2, count:516, passes:1, max_passes:5, b:[-4.61297554]\nclass:2, count:517, passes:0, max_passes:5, b:[-4.61297554]\nclass:2, count:518, passes:1, max_passes:5, b:[-4.4940417]\nclass:2, count:519, passes:0, max_passes:5, b:[-4.4940417]\nclass:2, count:520, passes:1, max_passes:5, b:[-4.4940417]\nclass:2, count:521, passes:2, max_passes:5, b:[-4.4940417]\nclass:2, count:522, passes:3, max_passes:5, b:[-4.4940417]\nclass:2, count:523, passes:4, max_passes:5, b:[-4.5828714]\nclass:2, count:524, passes:0, max_passes:5, b:[-4.44022314]\nclass:2, count:525, passes:0, max_passes:5, b:[-4.44022314]\nclass:2, count:526, passes:1, max_passes:5, b:[-4.44022314]\nclass:2, count:527, passes:2, max_passes:5, b:[-4.44022314]\nclass:2, count:528, passes:3, max_passes:5, b:[-4.44022314]\nclass:2, count:529, passes:4, max_passes:5, b:[-4.59971991]\nclass:2, count:530, passes:0, max_passes:5, b:[-4.58590992]\nclass:2, count:531, passes:0, max_passes:5, b:[-4.58590992]\nclass:2, count:532, passes:1, max_passes:5, b:[-4.58590992]\nclass:2, count:533, passes:2, max_passes:5, b:[-4.48273489]\nclass:2, count:534, passes:0, max_passes:5, b:[-4.5378839]\nclass:2, count:535, passes:0, max_passes:5, b:[-4.5378839]\nclass:2, count:536, passes:1, max_passes:5, b:[-4.5378839]\nclass:2, count:537, passes:2, max_passes:5, b:[-4.4804275]\nclass:2, count:538, passes:0, max_passes:5, b:[-4.4804275]\nclass:2, count:539, passes:1, max_passes:5, b:[-4.4804275]\nclass:2, count:540, passes:2, max_passes:5, b:[-4.47301313]\nclass:2, count:541, passes:0, max_passes:5, b:[-4.43339467]\nclass:2, count:542, passes:0, max_passes:5, b:[-4.43339467]\nclass:2, count:543, passes:1, max_passes:5, b:[-4.43339467]\nclass:2, count:544, passes:2, max_passes:5, b:[-4.43339467]\nclass:2, count:545, passes:3, max_passes:5, b:[-4.43339467]\nclass:2, count:546, passes:4, max_passes:5, b:[-4.43339467]\nclass:2, count:547, passes:5, max_passes:5, b:[-4.43274457]\nclass:2, count:548, passes:0, max_passes:5, b:[-4.43274457]\nclass:2, count:549, passes:1, max_passes:5, b:[-4.54363407]\nclass:2, count:550, passes:0, max_passes:5, b:[-4.54363407]\nclass:2, count:551, passes:1, max_passes:5, b:[-4.46244753]\nclass:2, count:552, passes:0, max_passes:5, b:[-4.49012457]\nclass:2, count:553, passes:0, max_passes:5, b:[-4.49012457]\nclass:2, count:554, passes:1, max_passes:5, b:[-4.44463906]\nclass:2, count:555, passes:0, max_passes:5, b:[-4.44463906]\nclass:2, count:556, passes:1, max_passes:5, b:[-4.44463906]\nclass:2, count:557, passes:2, max_passes:5, b:[-4.44463906]\nclass:2, count:558, passes:3, max_passes:5, b:[-4.44463906]\nclass:2, count:559, passes:4, max_passes:5, b:[-4.44463906]\nclass:2, count:560, passes:5, max_passes:5, b:[-4.47540234]\nclass:2, count:561, passes:0, max_passes:5, b:[-4.51702578]\nclass:2, count:562, passes:0, max_passes:5, b:[-4.68628141]\nclass:2, count:563, passes:0, max_passes:5, b:[-4.42467799]\nclass:2, count:564, passes:0, max_passes:5, b:[-4.54687812]\nclass:2, count:565, passes:0, max_passes:5, b:[-4.62698768]\nclass:2, count:566, passes:0, max_passes:5, b:[-4.52211791]\nclass:2, count:567, passes:0, max_passes:5, b:[-4.52211791]\nclass:2, count:568, passes:1, max_passes:5, b:[-4.52766696]\nclass:2, count:569, passes:0, max_passes:5, b:[-4.50400701]\nclass:2, count:570, passes:0, max_passes:5, b:[-4.51044417]\nclass:2, count:571, passes:0, max_passes:5, b:[-4.52492228]\nclass:2, count:572, passes:0, max_passes:5, b:[-4.52492228]\nclass:2, count:573, passes:1, max_passes:5, b:[-4.52492228]\nclass:2, count:574, passes:2, max_passes:5, b:[-4.52492228]\nclass:2, count:575, passes:3, max_passes:5, b:[-4.52492228]\nclass:2, count:576, passes:4, max_passes:5, b:[-4.59852942]\nclass:2, count:577, passes:0, max_passes:5, b:[-4.59852942]\nclass:2, count:578, passes:1, max_passes:5, b:[-4.50699046]\nclass:2, count:579, passes:0, max_passes:5, b:[-4.41259745]\nclass:2, count:580, passes:0, max_passes:5, b:[-4.41259745]\nclass:2, count:581, passes:1, max_passes:5, b:[-4.50890357]\nclass:2, count:582, passes:0, max_passes:5, b:[-4.56094731]\nclass:2, count:583, passes:0, max_passes:5, b:[-4.56094731]\nclass:2, count:584, passes:1, max_passes:5, b:[-4.56094731]\nclass:2, count:585, passes:2, max_passes:5, b:[-4.56094731]\nclass:2, count:586, passes:3, max_passes:5, b:[-4.56094731]\nclass:2, count:587, passes:4, max_passes:5, b:[-4.56094731]\nclass:2, count:588, passes:5, max_passes:5, b:[-4.56094731]\nPerformance on the training set\n[[35  0  0]\n [ 0 38  1]\n [ 0  1 37]]\n"
    }
   ],
   "source": [
    "exponent = 2\n",
    "intercept =1\n",
    "svm_model =SVM_kCLasses(X_train, y_train, C = 10, tol = 0.001, max_passes = 5, kernel_choice = \"poly\", parameters=[exponent, intercept], k=k)\n",
    "support_vectors, support_alphas, support_target = svm_model.fit()\n",
    "pred = svm_model.prediction(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svm_model.prediction_dataset(X_test)\n",
    "print(\"Performance on the test set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 1, chapter 6 and Chapter 7 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 6: (https://www.youtube.com/watch?v=qyyJKd-zXRE)\n",
    "* Andrew Ng, Lec 7: (https://www.youtube.com/watch?v=s8B4A5ubw6c)\n",
    "* Andrew Ng, Lec 8: (https://www.youtube.com/watch?v=bUv9bfMPMb4)\n",
    "* Simplified Sequential Minimal Optimization: (https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiRlObmw5_qAhW7ShUIHSjJAbYQFjAAegQIAhAB&url=http%3A%2F%2Fcs229.stanford.edu%2Fmaterials%2Fsmo.pdf&usg=AOvVaw201bQxVZY0MmUn_gGAu5O8)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}