{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using Kernel Trick\n",
    "### Kernel Trick\n",
    "One of the limitation of linear regression is that it is usually implemented to utilize the linear relationship between the predictors in the input space, but this tend to overlook the interaction between these predictors in the input space. One of the approaches to mitigate this probelem is by mapping the input space into the feature space($\\underline{x}\\rightarrow\\phi(\\underline{x}$). In which thie feature space tends to be a in a higher dimension than the original input space, and in this space we can apply linear regression and achieve good result by finding the parameter of the non-linear model with respect to the feature. Nevertheless, this approach suffers from that it is computationally expensive to compute the feature vector for each observation. To solve this issue we use what is known as the kernel trick and a better name for this approach would be kernel substitution, in which instead of calculating the feature vector explicitly we could calculate it implicitly using the kernel function. The kernel function is an inner product between two feature vectors which can be expressed as follows:- $k(x,x')=\\langle \\phi(x), \\phi(x')\\rangle=\\phi(x)^T\\phi(x')$. There are many kernels from the like of linear kernel($k(x,x')=x^Tx'$), stationary kernel ($k(x,x')=c(x-x')$), Gaussian kernel ($k(x, x')=exp(\\frac{-||x-x'||_2^2}{2\\sigma^2})$), and etc. One can view kernels as a similarity measure between two observation. There are many conditions to choose a kernel one of those condition are sufficient condition(Mercer kernel) in which the kernel matrix must be P(S)D, in which this can be tested by checking for the eignevalues of the kernel matrix. THe kernel matrix is the outer product of the design matrix, and the design matrix will have its rows as the feature vector of each observation. The equation of kernel marix is as follows:- $K = \\Phi\\Phi^T$in which:-\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\Phi=\n",
    "\\begin{pmatrix}\n",
    "& \\phi(\\underline{x_1})^T\\\\\n",
    "& \\phi(\\underline{x_1})^T\\\\\n",
    "&....\\\\\n",
    "& \\phi(\\underline{x_n})^T\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "$$  \n",
    "Therefore, the kernel matrix size is $R^{nxn}$ which is a symmetric matrix (or sometimes called Gram m). One of the core ideas in kernel is that you can build more complex kernels from atomic kernels, like, the linear kernel. Let's see this idea with the Gaussian kerenel.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&k(x, x')=exp(\\frac{-||x-x'||_2^2}{2\\sigma^2})=exp(\\frac{-(x-x')^(x-x')}{2\\sigma^2})=exp(-\\frac{x^Tx-x^Tx'-x'^Tx+x'^Tx'}{2\\sigma^2})\\\\\n",
    "&k(x,x')=exp(\\frac{k1(x, x')}{\\sigma^2})exp(-\\frac{k1(x', x')}{2\\sigma^2})exp(-\\frac{k1(x, x')}{2\\sigma^2});\\ where\\ is\\ k1(.,.)\\ is\\ linear\\ kernel\\\\\n",
    "&We\\ can\\ change\\ this\\ kernel\\ to\\ be\\ nonlinear\\ kernel\\ from\\ the\\ like\\ of\\ k2(x, x')=(x^Tx'+c)^M\\\\\n",
    "&Where\\ k2\\ can\\ be\\ also\\ build\\ from\\ atomic\\ kernels\\ from\\ the\\ like\\ of\\ the\\ linear\\ kernel\\\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Also, it is known that the feature vector that is used to build the Gaussian kernel is a infinte dimension, hence, this would be computationally infeasible and this is indicative of the importance of kernel trick.\n",
    "\n",
    "### Kernel Trick on Linear Regression Direct Solution\n",
    "The cost function that we usually minimize for the linear regression is the mean squared error which can be driven by maximizing the likelihood. The following cost function will be used for the regularized cost function which is equivalent of using a gaussian prior.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&\\nabla_w\\big( J(w)=\\frac{1}{2}\\sum_{n}(w^T\\phi(x_n) -t_n)^2 + \\frac{\\lambda}{2}||w||_2^2 \\big)\\\\\n",
    "&w = \\frac{-1}{\\lambda}\\sum{n}(w^T\\phi(x_n) - t_n)\\phi(x_n) = \\frac{-1}{\\lambda}\\sum{n}a_n\\phi(x_n)=\\Phi^T\\underline{a}; substituting\\ this\\ solution\\ into\\ J(w)\\\\\n",
    "&J(w) =\\frac{1}{2}(\\Phi(\\Phi^Ta) - t)^T(\\Phi(\\Phi^Ta) - t) + \\frac{\\lambda}{2}(\\Phi^Ta)^T\\\\\n",
    "&J(w)=\\frac{1}{2}a^T\\Phi\\Phi^T\\Phi^T\\Phi a-a^T\\Phi\\Phi^Tt -\\frac{1}{2}t^t+\\frac{\\lambda}{2}a^T\\Phi\\Phi^Ta;by\\ using\\ K=\\Phi\\Phi^T\\\\\n",
    "&As\\ can\\ be\\ seen\\ the\\ w\\ have\\ disappeared\\ and\\ were\\ replaced\\ by\\ a\\, so\\ we\\ are\\ maximizing\\ w.r.t\\ a\\\\\n",
    "&\\nabla_aJ(a)=\\nabla_a(\\frac{1}{2}a^TKK^Ta-a^TKt -\\frac{1}{2}t^t+\\frac{\\lambda}{2}a^TK a)\\\\\n",
    "&KK^Ta-K^Tt+\\lambda Ka=0\\rightarrow a = (KK^T+\\lambda K)^{-1}K^Tt;\\ by\\ K=K^T\\\\\n",
    "&a=(K+\\lambda I_N)^{-1}t\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "And to make prediction $y(\\phi(x_n), w) = w^T\\phi(x_n)=a^T\\Phi\\phi(x_n)=\\phi(x_n)^T\\Phi^T(K+\\lambda IN)^{-1}t=\\underline(k(x)^T(K+\\lambda I_N)^{-1}t$, where is k(x) is just the inner product of feature vector for xn and the design matrix. And this can be expressed as inner product of xn with every obsertation and this indicate that we need to store the dataset to make a prediction. So, one would think this is a severe downfall for this method but as we will see in SVM there would be few vectors to be stored which usually are called support vectors. As can be seen the prediction and the optimization were complete expressed by the kernel fucntion, hence, the name of kernel substitution.\n",
    "\n",
    "Also, as can be seen from the equation for a that minimize the cost function, have a dimension of nx1 instead of px1. So, this raise an issue that if we have a large dataset then out parameter(a) would be large, but this trade off will be really appreciated when a simple linear model with respect to the predictors doesn't perform well and explicit computation for the feature vector is computationally infeasible. Also, the kernel trick will be a signficant factor in SVM that relies on changing the space of the dataset to be linearly separable in this new space while it would be non-linearly separable in the original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics\n",
    "import sklearn.kernel_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(379, 14)\n(127, 14)\n"
    }
   ],
   "source": [
    "X, y = sklearn.datasets.load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "X_train = standard.fit_transform(X_train)\n",
    "y_train = standard.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "training_data = np.c_[X_train, y_train]#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "\n",
    "X_test = standard.transform(X_test)\n",
    "y_test = standard.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "test_data = np.c_[X_test, y_test]\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, x_star, sigma):\n",
    "    return np.exp(np.divide(-1*(np.linalg.norm(x-x_star)**2), 2*sigma**2))\n",
    "\n",
    "def estimating_a(X, y, lambd, sigma):\n",
    "    K = np.zeros((X.shape[0], X.shape[0]))\n",
    "    for i in range(0, X.shape[0]):\n",
    "        for j in range(0, X.shape[0]):\n",
    "            K[i, j] = gaussian_kernel(X[i, :], X[j, :], sigma)\n",
    "        #K[i, :] = gaussian_kernel(X[i, :], X[:, :], sigma)\n",
    "    #print(K)\n",
    "    I = np.eye(X.shape[0])\n",
    "    a = np.dot(np.linalg.inv(K + lambd * I), y)\n",
    "    return a, K\n",
    "\n",
    "def prediction(xn, X, t, K, lambd, sigma):\n",
    "    k = np.zeros((X.shape[0], 1))\n",
    "    for i in range(0, X.shape[0]):\n",
    "        k[i] = gaussian_kernel(xn, X[i, :], sigma)\n",
    "    I = np.eye(X.shape[0])\n",
    "    return (np.dot( (np.dot(k.T, np.linalg.inv( K + lambd*I ))), t))[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.04700475472406587"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "a, K = estimating_a(X_train, y_train, 0.3, 2)\n",
    "pred = []\n",
    "for x in X_train:\n",
    "    pred.append(prediction(x, X_train, y_train, K, 0.3, 2))\n",
    "\n",
    "sklearn.metrics.mean_squared_error(y_train, pred)#Should be in range of [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8148325652363119"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "\n",
    "pred = []\n",
    "for x in X_test:\n",
    "    pred.append(prediction(x, X_train, y_train, K, 0.3, 2))\n",
    "\n",
    "sklearn.metrics.mean_squared_error(y_test, pred)#The test error became large which indicative of overfitting, so, we need to either change from Gaussian kernel or make the lambda with larger value to make the parameter more sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 3, and Chapter 5 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 7: (https://www.youtube.com/watch?v=s8B4A5ubw6c)\n",
    "* Andrew Ng, Lec 8: (https://www.youtube.com/watch?v=bUv9bfMPMb4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}