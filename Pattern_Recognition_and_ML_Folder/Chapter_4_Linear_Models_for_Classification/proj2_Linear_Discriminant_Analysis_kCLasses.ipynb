{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Generative Models\n",
    "### Linear Discriminant Analysis\n",
    "As we saw in the LR, we were able to explicitly model the conditional distribution of the target class given the input features. Another approaching for modeling this distribution is by implicitly modeling it by using Bayes' theorem. This approach is known as generative learning algorithm, in which model the conditional distribution of the input features of each class. Then the posterior (conditional distribution of the target class w.r.t input features) is estimated by Bayes' theorem. And this is illustrated by the following formula:-\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&p(t=C_1|x) = \\frac{p(t=C_1,t)}{p(x)} = \\frac{p(x|t=C_1)p(C_1)}{\\sum_{i}p(x,t=C_i)},\\\\\n",
    "&=\\frac{p(x|t=C_1)p(C_1)}{\\sum_{i}p(x|t=C_i)p(C_i)}\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "One of the Generative models is the Gaussian Discriminant model(GDM) in which we try to fit to $p(x|t=C_i)$ a multi-variate distribution, and this is indicative that the features space should be made from continuous random variables. And the Gaussian distribution is fully described by its mean and covariance matrix. Therefore, GDM model can further divided into two models one is called linear discriminant analysis (LDA) model and the other one is quadratic discriminant analysis (QDA) model, the meaning of LDA will be explained in this jupyter notebook, while QDA will be explained in another jupyter notebook.\n",
    "\n",
    "### Explaining the linear part in LDA\n",
    "As was shown above, the conditional distribution of input feature space given the target class can be explained by $p(t=C_1|x)=\\frac{p(x|t=C_1)p(C_1)}{\\sum_{i}p(x|t=C_i)p(C_i)}$. By inserting a natural logarithm and an exponential before each $p(x, t=C_i)$, one can easily express $p(t=C_i|x)$ as follows:- $p(t=C_k|x) = \\frac{exp(ln(p(x|C_k)\\ p(C_k)))}{\\sum_{i}exp(ln(p(x|t=C_i)p(C_i)))} = \\frac{exp(a_k)}{\\sum_{i}exp(a_i)}$. Also, as was mentioned before $p(x|t=C_k)$ is assumed to be a multi-variate Gaussian that is expresses by the following formula:-\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "p(x|t=C_k) = \\frac{1}{(det(2\\pi \\Sigma)^{D/2}} exp(\\frac{-1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k))\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "Also, LDA will assume that the distribution of the features space in each class is sharing the same exact covariance matrix. The $\\underline{a_k}$ vector can expresses as follows:-\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&a_k = ln(p(x|C_k)p(C_k)) = ln(\\frac{1}{det(2\\pi \\Sigma)}) -\\frac{1}{2}(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k) + ln(p(C_k)), \\ where\\  is\\ \\mu\\  and\\  x\\  are\\  vectors\\\\\n",
    "&a_k = ln(\\frac{1}{det(2\\pi \\Sigma)}) -\\frac{1}{2}\\Big( x^T\\Sigma^{-1}x - x^T\\Sigma^{-1}\\mu_k-\\mu_k^{T}\\Sigma^{-1}x+\\mu_k^T\\Sigma^{-1}\\mu_k\\Big) + ln(p(C_k));\\ let\\ t=\\Sigma^{-1}\\mu_k\\ ,\\ \\Sigma^T=\\Sigma and\\ utilize\\ a^Tb=b^Ta\\\\\n",
    "&a_k= ln(\\frac{1}{det(2\\pi \\Sigma)}) -\\frac{1}{2}x^T\\Sigma^{-1}x + \\mu_k^T\\Sigma^{-1}x  -\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + ln(p(C_k))\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "And now we are equipped with the necessary tool to be able to see the linearity with respect to $\\underline{x}$, this can be derives as follows:-\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&p(C_k|x) = \\frac{exp\\Big( exp(ln(\\frac{1}{det(2\\pi \\Sigma)}) -\\frac{1}{2}x^T\\Sigma^{-1}x + \\mu_k^T\\Sigma^{-1}x  -\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + ln(p(C_k))) \\Big)}{\\sum_{i}\\Big( exp\\Big( ln(\\frac{1}{det(2\\pi \\Sigma)}) -\\frac{1}{2}x^T\\Sigma^{-1}x + \\mu_i^T\\Sigma^{-1}x  -\\frac{1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i + ln(p(C_i)) \\Big) \\Big)};\\\\\n",
    "\n",
    "& we\\ take\\ ln(\\frac{1}{det(2\\pi \\Sigma)}) -\\frac{1}{2}x^T\\Sigma^{-1}x\\ as\\ a common\\ term\\ from\\ the\\ denominator\\\\\n",
    "\n",
    "&and\\ we\\ need\\ to\\ utilize\\ exp(a+b)=exp(a)exp(b)\\\\\n",
    "\n",
    "&p(C_k|x) = \\frac{exp\\Big( \\mu_k^T\\Sigma^{-1}x  -\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + ln(p(C_k) \\Big)}{\\sum_{i}exp\\Big( \\mu_k^T\\Sigma^{-1}x  -\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + ln(p(C_k) \\Big)}\\\\\n",
    "\n",
    "&a_k(x) =\\underline{w}^T\\underline{x} + w_{k0} = (\\Sigma^{-1}\\mu_k)^Tx +\\frac{-1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + ln(p(C_k)) \n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "As can be seen from the last equation, we can express $a_k$ as a linear combination of the input feature, and from here the term linear was coined. Also, this indicate that the decision boundary would be linear similar to the case of logistic regression and softmax regression. And the relationship between LDA and logistic regression will be discussed later.\n",
    "\n",
    "### Maximum Likelihood Solution\n",
    "This method need to estimate three parameters and these parameters are:- $p(C_k)=\\phi_k$ which is proportion of observation belonging to class k, $\\mu_k$ which is mean vector of the input feature for the k-th class, and $\\Sigma$ which is share covariance matrix among all classes. Because we have k-classes we need to utilize the indicator function in which $\\textit{I}(c =k)$ in which is equal to 1 if c was equal to k, otherwise it would be zero. Therefore, the log-Likelihood can be expressed as follows:-\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&\\ell(\\mu_1, ... \\mu_k, \\Sigma, \\phi_1, ... \\phi_k) = log\\Big( p(\\underline{t}|\\underline{x}, \\phi^s, \\mu^s, \\Sigma) \\Big)\\\\\n",
    "\n",
    "&=log\\Big( \\prod_{n=1}{N}\\big( \\prod_{k=1}^{K}p(x|C_k;.)p(C_k) \\big) \\Big)\\\\\n",
    "\n",
    "&=log\\Big( \\prod_{n=1}^{N}\\big( \\prod_{k=1}^{K}(\\phi_k \\mathcal{N}(x_n;\\mu_k, \\Sigma))^{\\textit{I}(C_k = t_n)} \\big) \\Big)\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "So, we need to maximize the log-likelihood with respect to $\\phi^s$, $\\mu^s$ and $\\Sigma$ and the resulting values for those parameters that maximize the log-likelihood are:-\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&\\pi_k = \\frac{N_k}{N_1 + ... + N_K}\\\\\n",
    "&\\mu_k = \\frac{1}{N_k}\\sum_{n=1}^{N}\\textit(C_k =k) \\underline{x_n}\\\\\n",
    "&\\Sigma = \\frac{N_1}{N}\\sigma_1+ ... +\\frac{N_K}{N}\\sigma_k\\\\\n",
    "&\\Sigma_k =\\frac{1}{N_1}\\sum_{n \\in C_k}(x_n - \\mu_k)(x_n - \\mu_k)^T  \n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "### Make prediction\n",
    "After learning the parameters for the LDA model, we can make prediction by utilizing the following formula:-\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(C_k|x) = \\frac{exp\\Big( \\mu_k^T\\Sigma^{-1}x  -\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + ln(\\phi_k) \\Big)}{\\sum_{i}exp\\Big( \\mu_k^T\\Sigma^{-1}x  -\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + ln(\\phi_k) \\Big)}\n",
    "\\end{align*}\n",
    "$$\n",
    "In which we choose the class that achieved the highest posterior value.\n",
    "\n",
    "### Relationship between LDA and Logistic regression\n",
    "We will restrict LDA to deal with a 2 class dataset in order to be compared to logistic regression. The derivation of the relationship can be seen as follows:-\n",
    "\n",
    "As can be seen from above, LDA or GDA in general is equivalent to the LR when the assumption that p(X|C) is normally distributed. And if this assumption holds LDA will be faster than LR due to simple manner of estimating its parameters. But if the assumption doesn't holds and we have small data set (\"Central limit theorem), the LR would achieve better LDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(133, 14)\n(45, 14)\n"
    }
   ],
   "source": [
    "#X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "X, y = sklearn.datasets.load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "X_train = standard.fit_transform(X_train)\n",
    "training_data = np.c_[X_train, y_train]#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "\n",
    "X_test = standard.transform(X_test)\n",
    "test_data = np.c_[X_test, y_test]\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "X_train, y_train = (training_data[:, 0:-1], training_data[:, -1])\n",
    "X_test, y_test = (test_data[:, 0:-1], test_data[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_Model(object):\n",
    "\n",
    "\n",
    "    def __init__(self, X_train, y_train, k):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.Mu = [mu for mu in [np.zeros((X_train.shape[1], 1))]*k]\n",
    "        self.Sigma = [sigma for sigma in [np.zeros((X_train.shape[1], X_train.shape[1]))]*k]\n",
    "        self.Sigma_total = np.zeros((self.X_train.shape[1], self.X_train.shape[1]))\n",
    "        self.phis = [phi for phi in np.zeros((k, 1))]\n",
    "        self.m = (self.X_train).shape[0]\n",
    "        self.n = (self.X_train).shape[1]\n",
    "        self.K = k\n",
    "\n",
    "    def fit(self):\n",
    "        data = pd.DataFrame(np.c_[self.X_train, self.y_train])\n",
    "        indexs = data.columns\n",
    "        class_observations = []\n",
    "        N = []\n",
    "        for k in range(0, self.K):\n",
    "            class_observations.append(data[data[indexs[-1]] == k])\n",
    "            N.append(class_observations[k].shape[0])\n",
    "\n",
    "\n",
    "        for k in range(0, self.K):\n",
    "            temp = (class_observations[k]).to_numpy()\n",
    "            mean_temp = (np.mean(temp[:, 0:-1], axis=0)).reshape(-1, 1)\n",
    "            assert(self.Mu[k].shape == mean_temp.shape)\n",
    "            self.Mu[k] = mean_temp.copy()\n",
    "            self.Sigma[k] = np.cov((temp[:, 0:-1]).T)\n",
    "\n",
    "        for k in range(0, self.K):\n",
    "            self.phis[k] = N[k]/self.n\n",
    "            self.Sigma_total = self.Sigma_total + (N[k]/self.n) * self.Sigma[k]\n",
    "        \n",
    "        return self.phis, self.Mu, self.Sigma_total\n",
    "\n",
    "    def predict_observation(self, x):\n",
    "        prediction = np.zeros((1, self.K))\n",
    "        \n",
    "        denominator = 0\n",
    "        s_inv = np.linalg.inv(self.Sigma_total)\n",
    "\n",
    "        for k in range(0, self.K):\n",
    "            t1 = np.dot(self.Mu[k].T, np.dot(s_inv, x))\n",
    "            t2 = (-1/2)* np.dot(self.Mu[k].T, np.dot(s_inv, self.Mu[k]))\n",
    "            t3 = np.log(self.phis[k])\n",
    "            temp = np.exp( t1 + t2 + t3)\n",
    "            #print(temp.shape)\n",
    "            #assert(denominator.shape == temp.shape)\n",
    "            denominator = denominator + temp\n",
    "\n",
    "        #print(denominator)\n",
    "        #print(prediction.shape)\n",
    "        for k in range(0, self.K):\n",
    "            t1 = np.dot(self.Mu[k].T, np.dot(s_inv, x))\n",
    "            t2 = (-1/2)* np.dot(self.Mu[k].T, np.dot(s_inv, self.Mu[k]))\n",
    "            t3 = np.log(self.phis[k])\n",
    "            temp = np.exp(t1 + t2 + t3)\n",
    "            #print(temp.shape)\n",
    "            #assert(temp.shape == denominator.shape)\n",
    "            prediction[:, k] = (np.divide(temp, denominator))\n",
    "        \n",
    "        return np.argmax(prediction)\n",
    "    \n",
    "    def predict_dataset(self, X, y):\n",
    "        prediction = np.zeros((X.shape[0], 1))\n",
    "        for i in range(0, X.shape[0]):\n",
    "            prediction[i, 0] = self.predict_observation(X[i, :])\n",
    "        \n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the training set\n[[43  1  0]\n [ 0 53  0]\n [ 0  1 35]]\n"
    }
   ],
   "source": [
    "LDA = LDA_Model(X_train, y_train, k=3)\n",
    "phis, Mu, Sigma_total = LDA.fit()\n",
    "prediction = LDA.predict_dataset(X_train, y_train)\n",
    "\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, prediction))\n",
    "#print(f\"precision:{sklearn.metrics.precision_score(y_train, prediction):0.3f}, recall:{sklearn.metrics.recall_score(y_train, prediction):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the test set\n[[15  0  0]\n [ 0 18  0]\n [ 0  0 12]]\n"
    }
   ],
   "source": [
    "prediction = LDA.predict_dataset(X_test, y_test)\n",
    "print(\"Performance on the test set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 1, chapter 2 and Chapter 4 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 4: (https://www.youtube.com/watch?v=nLKOQfKLUks)\n",
    "* Andrew Ng, Lec 5: (https://www.youtube.com/watch?v=qRJ3GKMOFrE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}