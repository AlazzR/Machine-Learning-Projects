{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Generative Models\n",
    "### Quadratic Discriminant Analysis (QDA)\n",
    "QDA takes the same paradigm as the LDA but with the difference that the decision boundary will be quadratic instead of linear as the case in LDA. This result from taking away the assumption that the covariance matrix is shared among the input space in each class. I willn't derive QDA from scratch because I already discussed its building blocks in LDA section, but I will discuss how to arrive to the quadratic decision boundary. And I will discuss the process for prediction and what are the essential formulas that will be discussed in this project.\n",
    "\n",
    "As was mentioned in the LDA part, we can express the posterior of the conditional distribution of the class, as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&p(t=C_k|x) = \\frac{p(t=C_k,t)}{p(x)} = \\frac{p(x|t=C_k)p(C_k)}{\\sum_{i}p(x,t=C_i)},\\\\\n",
    "&=\\frac{p(x|t=C_k)p(C_k)}{\\sum_{i}p(x|t=C_i)p(C_i)};\\ by \\ inserting\\ exp\\ log\\ before\\ p(x|t=C_i)p(C_i)\\\\\n",
    "&p(t=C_k|x) = \\frac{exp\\Big( ln(p(x|t=C_k)p(C_k))\\Big)}{\\sum_{i}exp\\Big( ln(p(x|t=C_i)p(C_i))\\Big)} = \\frac{exp(a_k)}{\\sum_{i}exp(a_i)}\\\\\n",
    "&a_k=ln(p(x|t=C_k)p(C_k))=-ln((2\\pi)^{D/2}) - \\frac{1}{2}ln(det(\\Sigma_k)) -\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k) + ln(p(C_k));\\\\\n",
    "&-ln((2\\pi)^{D/2})\\ will\\ be\\ removed\\ because\\ it\\ is\\ a\\ common\\ factor\\ with\\ the\\ denominator\\\\\n",
    "&a_k=-\\frac{1}{2}ln(det(\\Sigma_k)) + ln(p(C_k)) -\\frac{1}{2}x^T\\Sigma_k^{-1}x +\\frac{1}{2}x^T\\Sigma_k^{-1}\\mu_k+\\frac{1}{2}\\mu_k^T\\Sigma_k^{-1}x-\\frac{1}{2}\\mu_k^T\\Sigma_k^{-1}\\mu_k;\\ let\\ e=\\Sigma_k^{-1}\\mu_k\\ and\\ we\\ use\\ a^Tb=b^Ta\\\\\n",
    "&a_k =\\frac{-1}{2}\\Big( x^T\\Sigma_k^{-1}x -2x^T\\Sigma_k^{-1}\\mu_k+\\mu_k^T\\Sigma_k^{-1}\\mu_k+ln(det(\\Sigma_k) -2ln(p(C_k))) \\Big)\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$ \n",
    "It can be seen directly from the last question the occurrence of quadratic decision boundary. The following equations are the same results that were derived in LDA, with the update for the prediction method.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&\\pi_k = \\frac{N_k}{N_1 + ... + N_K}\\\\\n",
    "&\\mu_k = \\frac{1}{N_k}\\sum_{n=1}^{N}\\textit(C_k =k) \\underline{x_n}\\\\\n",
    "&\\Sigma_k =\\frac{1}{N_1}\\sum_{n \\in C_k}(x_n - \\mu_k)(x_n - \\mu_k)^T\\\\\n",
    "&p(C_k|x) = \\frac{exp\\Big( \\frac{-1}{2}\\Big( x^T\\Sigma_k^{-1}x -2x^T\\Sigma_k^{-1}\\mu_k+\\mu_k^T\\Sigma_k^{-1}\\mu_k+ln(det(\\Sigma_k) -2ln(p(C_k))) \\Big) \\Big)}{\\sum_{i}exp\\Big( \\frac{-1}{2}\\Big( x^T\\Sigma_i^{-1}x -2x^T\\Sigma_i^{-1}\\mu_k+\\mu_i^T\\Sigma_i^{-1}\\mu_k+ln(det(\\Sigma_i) -2ln(p(C_k))) \\Big) \\Big)} \n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "QDA tends to perform better on training set than LDA due to the complex decision boundary that it use, but as is known the more complex the model becomes the more likely it would over-fit the training set. Therefore, this is a direct disadvantage of QDA but this disadvantage would disappear with the more data that it use to learn its parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(133, 14)\n(45, 14)\n"
    }
   ],
   "source": [
    "#X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "X, y = sklearn.datasets.load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "X_train = standard.fit_transform(X_train)\n",
    "training_data = np.c_[X_train, y_train]#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "\n",
    "X_test = standard.transform(X_test)\n",
    "test_data = np.c_[X_test, y_test]\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "X_train, y_train = (training_data[:, 0:-1], training_data[:, -1])\n",
    "X_test, y_test = (test_data[:, 0:-1], test_data[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA_Model(object):\n",
    "\n",
    "\n",
    "    def __init__(self, X_train, y_train, k):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.Mu = [mu for mu in [np.zeros((X_train.shape[1], 1))]*k]\n",
    "        self.Sigma = [sigma for sigma in [np.zeros((X_train.shape[1], X_train.shape[1]))]*k]\n",
    "        self.phis = [phi for phi in np.zeros((k, 1))]\n",
    "        self.m = (self.X_train).shape[0]\n",
    "        self.n = (self.X_train).shape[1]\n",
    "        self.K = k\n",
    "\n",
    "    def fit(self):\n",
    "        data = pd.DataFrame(np.c_[self.X_train, self.y_train])\n",
    "        indexs = data.columns\n",
    "        class_observations = []\n",
    "        N = []\n",
    "        for k in range(0, self.K):\n",
    "            class_observations.append(data[data[indexs[-1]] == k])\n",
    "            N.append(class_observations[k].shape[0])\n",
    "\n",
    "\n",
    "        for k in range(0, self.K):\n",
    "            temp = (class_observations[k]).to_numpy()\n",
    "            mean_temp = (np.mean(temp[:, 0:-1], axis=0)).reshape(-1, 1)\n",
    "            assert(self.Mu[k].shape == mean_temp.shape)\n",
    "            self.Mu[k] = mean_temp.copy()\n",
    "            self.Sigma[k] = np.cov((temp[:, 0:-1]).T)\n",
    "\n",
    "        for k in range(0, self.K):\n",
    "            self.phis[k] = N[k]/self.n\n",
    "        \n",
    "        return self.phis, self.Mu, self.Sigma\n",
    "\n",
    "    def predict_observation(self, x):\n",
    "        prediction = np.zeros((1, self.K))\n",
    "        \n",
    "        denominator = 0\n",
    "\n",
    "        for k in range(0, self.K):\n",
    "            s_inv = np.linalg.inv(self.Sigma[k])\n",
    "            t1 = -2*np.dot(x.T, np.dot(s_inv, self.Mu[k]))\n",
    "            t2 = np.dot(x.T, np.dot(s_inv, x))\n",
    "            t3 = np.dot(self.Mu[k].T, np.dot(s_inv, self.Mu[k]))\n",
    "            t4 = np.log(np.linalg.det(self.Sigma[k]))\n",
    "            t5 = -2 * np.log(self.phis[k])\n",
    "            temp = np.exp( (-1/2) * ( t1 + t2 + t3 + t4 + t5))\n",
    "            denominator = denominator + temp\n",
    "\n",
    "        for k in range(0, self.K):\n",
    "            s_inv = np.linalg.inv(self.Sigma[k])\n",
    "            t1 = -2*np.dot(x.T, np.dot(s_inv, self.Mu[k]))\n",
    "            t2 = np.dot(x.T, np.dot(s_inv, x))\n",
    "            t3 = np.dot(self.Mu[k].T, np.dot(s_inv, self.Mu[k]))\n",
    "            t4 = np.log(np.linalg.det(self.Sigma[k]))\n",
    "            t5 = -2 * np.log(self.phis[k])\n",
    "            temp = np.exp( (-1/2) * ( t1 + t2 + t3 + t4 + t5))\n",
    "\n",
    "            prediction[:, k] = (np.divide(temp, denominator))\n",
    "        \n",
    "        return np.argmax(prediction)\n",
    "    \n",
    "    def predict_dataset(self, X, y):\n",
    "        prediction = np.zeros((X.shape[0], 1))\n",
    "        for i in range(0, X.shape[0]):\n",
    "            prediction[i, 0] = self.predict_observation(X[i, :])\n",
    "        \n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the training set\n[[44  0  0]\n [ 1 52  0]\n [ 0  0 36]]\n"
    }
   ],
   "source": [
    "QDA = QDA_Model(X_train, y_train, k=3)\n",
    "phis, Mu, Sigma = QDA.fit()\n",
    "prediction = QDA.predict_dataset(X_train, y_train)\n",
    "\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, prediction))\n",
    "#print(f\"precision:{sklearn.metrics.precision_score(y_train, prediction):0.3f}, recall:{sklearn.metrics.recall_score(y_train, prediction):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the test set\n[[15  0  0]\n [ 0 18  0]\n [ 0  1 11]]\n"
    }
   ],
   "source": [
    "prediction = QDA.predict_dataset(X_test, y_test)\n",
    "print(\"Performance on the test set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 1, chapter 2 and Chapter 4 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 4: (https://www.youtube.com/watch?v=nLKOQfKLUks)\n",
    "* Andrew Ng, Lec 5: (https://www.youtube.com/watch?v=qRJ3GKMOFrE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}