{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Discriminative Models\n",
    "### Logistic Regression (LR)\n",
    "LR is one of the discriminative models that is constructed by directly modeling the conditional distribution of the target class. This project will proceed in discussing LR by going through generalized linear model (GLM) in which this help in reaching to the famous formula for LR. Also, LR deals with binary target in which is indicative that the conditional distribution can modeled by the Bernoulli distribution which is fully described by $\\phi$, and this distribution is as follows:-\n",
    "$$\n",
    "\\begin{align*}\n",
    "&Bernoulli(t|x;\\phi) = \\phi^{t}(1-\\phi)^{1-t}, t\\in {0, 1}; and\\ we\\ are\\ going\\ to\\ show\\ the\\ relationship\\ between\\ \\phi\\ and\\ x\\\n",
    "\\end{align*}\n",
    "$$\n",
    "GLM's represent a generalization for the ordinarily least square (OLS) models, in which the GLM's aren't restricted to the relationship of linearity between the mean value of the conditional distribution of the target value and the parameters. And that was explicitly stated by the OLS construction in which the OLS models are built on the assumption of noise that is normally distributed. The GLM's allows generalization for the distribution of the noise which leads into allowing the conditional distribution of the target to have non-linear relationship with the parameters of the model. This relationship is represented by the following:-  $E[\\underline{t}|\\underline{x};\\underline{w}] = y = f(w^T x)$, where is f(.) is a nonlinear function. And f(.) is known as the canonical response function, and the inverse of this function is known as the canonical link function. Which essentially act as a link between the linear combination of the parameters and input features with the response of the model, and in the LR this link would be the Logit function ( $\\eta = f^{-1}(E[\\underline{t}|\\underline{x};\\underline{w}])$).\n",
    "\n",
    "GLM's involve mainly the transformation of the conditional distribution of the target to the exponential family which are described with the following formula:- $p(x|\\eta) = h(x) g(\\eta) exp(\\eta^Tu(x))$. Where is u(x) is knows as sufficient statistics and the reason for this name will be shown shortly, $g(\\eta)$ represent the normalization term that ensure that the exponential distribution is a proper PDF. And its formula is as follows $g(\\eta)\\int_{-\\infty}^{\\infty}h(x)exp(\\eta^Tu(x))dx$. And the $\\eta$ parameter is the natural parameter that fully describe the exponential distribution.\n",
    "#### Bernoulli transformation to exponential\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&p(c=1|x;\\phi) = \\phi,\\ p(c=0|x;\\phi)=1-\\phi\\\\\n",
    "&p(c|x;\\phi) = \\phi^{c}(1-\\phi)^{1-c}; c \\in {0, 1}\\\\\n",
    "&p(c|x;\\phi) = exp\\Big( \\phi^{c}(1-\\phi)^{1-c}\\big(  \\big)\\Big) =\\ (1-\\phi)exp(\\ log(\\frac{\\phi}{1-\\phi} c))\\\\\n",
    "&u(x) = c,\\ \\eta^T=\\eta=log(\\frac{\\phi}{1-\\phi}),\\ \\phi = \\frac{exp(\\eta)}{1+\\exp(\\eta))} = \\frac{1}{1+exp(-\\eta)} = \\sigma(\\eta)\\\\\n",
    "&\\therefore p(c|x;\\eta) = \\sigma(-\\eta)\\ exp(\\eta\\ c) \n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "The canonical link response in this case the Logit in which is expresses by $\\eta = \\underline{w}^T\\underline{x} = log(\\frac{p(c=1|x;\\eta)}{p(c=0|x;\\eta)}) = log(\\frac{\\phi}{1-\\phi})$.\n",
    "#### Sufficient Statistics importance \n",
    "Before delving further into the topic of sufficient statistics, we need to derive a formula. As was mentioned before, the function $g(\\eta)$ is used for normalization, so, we can easily take the gradient of this formula w.r.t $\\eta$. And this is done as follows:-\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&\\nabla_\\eta (g(\\eta)\\int_{-\\infty}^{\\infty}h(x)exp(\\eta^Tu(x))dx = 1);\\ by\\ using\\ chain\\ rule\\\\\n",
    "& \\Big(\\nabla_\\eta g(\\eta)\\Big)(\\frac{g(\\eta)}{g(\\eta)}) \\int_{-\\infty}^{\\infty}h(x)exp(\\eta^Tu(x))dx +\\ g(\\eta)\\int_{-\\infty}^{\\infty}h(x)exp(\\eta^Tu(x)) u(x)dx =0\\\\\n",
    "&By\\ using\\ g(\\eta)\\int_{-\\infty}^{\\infty}h(x)exp(\\eta^Tu(x))dx = 1\\ and\\ E[g(x)]\\ = \\int_{}g(x)f_x(x)dx\\\\\n",
    "&(\\frac{-1}{g(\\eta)} \\nabla_\\eta g(\\eta)) = \\int_{} u(x)\\ h(x) g(\\eta)\\ exp(\\eta^T u(x)) dx;\\ by\\ using\\ \\nabla_x ln(x) = \\frac{1}{x}\\\\\n",
    "&-\\nabla_\\eta ln(g(\\eta)) = E[u(x)]\\\\\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "After showing the relationship between the sufficient statistics and the normalization term, we are now able to discuss the meaning of sufficient statistics. We now construct the negative log-likelihood(which is known as cross entropy) for the data and for brevity we will discuss the maximization of the likelihood with respect to the natural parameters.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&-\\nabla_\\eta ln(p(x|\\eta))= -\\Big( (\\prod_{n=1}^{N}h(x_n))\\ (g(\\eta))^N\\ exp(\\eta^T \\sum_{n=1}^{N}u(x_n))\\Big) =0\\\\\n",
    "&\\nabla_\\eta ln(g(\\eta_{ML})) = \\frac{1}{N} \\sum_{n=1}^{N}u(x_n); as\\ N \\rightarrow \\infty\\\\\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "As can be seen from the last formula, to maximize the likelihood w.r.t the natural parameters we need only the sufficient values of x, and this indicate that there is no need to store the value of x, hence, the name of sufficient statistics.\n",
    "\n",
    "#### Mean value of the conditional distribution of the target class\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "E[u(c)|x;\\eta] = \\sum_{c=0}^{1}u(c)\\ p(c|x;\\eta) = p(c=1|x;\\eta)=\\sigma(-\\eta)exp(\\eta^T) = (1-\\phi)\\ \\frac{\\phi}{1-\\phi}=\\phi\\\\\n",
    "= \\frac{exp(\\eta)}{1+exp(\\eta)}=\\ \\sigma(\\eta)\\\\\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "And this formula is known as the canonical response function, in which it connect the response(\"target\") to our nonlinear function w.r.t the parameter('w'). And this non-linear function is known as the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "X_train = standard.fit_transform(X_train)\n",
    "training_data = pd.DataFrame(np.c_[X_train, y_train])#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "#need to appends 1's for the bias term of the parameters\n",
    "training_data = pd.DataFrame(np.c_[np.ones(training_data.shape[0]), training_data])\n",
    "\n",
    "X_test = standard.transform(X_test)\n",
    "test_data = pd.DataFrame(np.c_[X_test, y_test])\n",
    "test_data = pd.DataFrame(np.c_[np.ones(test_data.shape[0]), test_data])\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)\n",
    "#training_data.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(x, w):\n",
    "    #assert(x.shape == w.shape)\n",
    "    activation = np.divide(1, np.add(1, np.exp(-1 * np.dot(x, w.reshape(-1, 1)) ) ) )\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing the likelihood\n",
    "After expressing the conditional distribution of the target variable, now, comes the part of maximizing the negative log likelihood( which is equivalent to the statement of minimizing to be 0). Each observation is assumed to be derived from i.i.d that is distributed according to the exponential representation of the Bernoulli distribution. The maximization of the cross entropy cost function would be with respect to $\\underline{w}$. And for brevity I will not discuss the derivation in details, but I need to state that it can be easily done by using chain rule from calculus.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&\\nabla_{\\underline{w}}\\ell(\\eta) = \\frac{1}{m}\\nabla_{\\underline{w}}\\ \\Big(-log\\Big( \\prod_{n=1}^{N}p(c|\\underline{x}; \\eta)Big)\\Big)\\\\\n",
    "&=\\frac{1}{m}\\nabla_{\\underline{w}}\\ \\Big(-\\sum_{n=1}^{N}c\\ log{\\phi}+ (1-c)\\ log(1-\\phi)\\Big)\\\\\n",
    "&=\\frac{1}{m}\\nabla_{\\underline{w}}\\ \\Big(-\\sum_{n=1}^{N}c\\log(\\sigma(\\eta))+(1-c) log(\\sigma(-\\eta))\\Big); where\\ is \\eta = \\underline{w}^T\\underline{x}\\\\\n",
    "&=-\\frac{1}{m}\\sum_{n=1}^{N}(c - \\phi)\\underline{x_n} =-\\frac{1}{m}\\sum_{n=1}^{N}(c - E[u(c)|\\underline{x}; \\eta])\\underline{x_n}\\\\\n",
    "&= \\frac{1}{m}\\sum_{n=1}^{N}(\\phi - c)\\underline{x_n}\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "In which the optimal value of $\\underline{w}$ can be found using gradient descent, in which I will utilize the following update method for w:- $\\underline{w}^{(t+1)} = \\underline{w}^{(t)} - \\alpha \\nabla_{\\underline{w}^{(t)}}\\ell(\\eta) = \\underline{w}^{(t)} -  \\frac{\\alpha}{m}\\sum_{n=1}^{N}(\\phi - c)\\underline{x_n} $\n",
    "\n",
    "Also, its known that maximizing the likelihood tends to over fit to the data especially if the number of observations isn't large, so, to solve this we would add the L2-regularizing(Ridge regularization) term to the cost function. And this is equivalent to maximum a posteriori (MAP) with the prior as the Gaussian distribution with mean zero and precision of $\\lambda^-1$. Therefore, the cost function which is the cross entropy and optimization for $\\underline{w}$ would be seen in the following equation.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&J(\\underline{w}) = \\frac{1}{m}(-\\ell(\\underline{w}) + \\frac{\\lambda}{2}\\underline{w}^T\\underline{w}) = \\frac{-1}{m} \\sum_{n=1}^{N} c\\ log{\\phi}+ (1-c)\\ log(1-\\phi) - \\frac{\\lambda}{2m}\\underline{w}^T\\underline{w}\\\\\n",
    "&\\underline{w}^{(t+1)} = \\underline{w}^{(t)} -\\alpha\\Big(\\frac{1}{m}\\sum_{n=1}^{N}(\\sigma(\\underline{w}^T\\underline{x_n}) - c)\\underline{x_n}\\ + \\frac{\\lambda}{2m}\\underline{w}^{(t)}\\Big)\\\\\n",
    "&=\\underline{w}^{(t)} - \\alpha\\Big(\\frac{1}{m}X^T(\\sigma(X\\underline{w}) - \\underline{y}) + \\frac{\\lambda}{m}\\underline{w}\\Big);\\ X \\in R^{m\\ x\\ n}\\\\\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(phi, decision):\n",
    "    if phi > decision:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(phi, c, w, lambd):\n",
    "    assert(phi.shape == c.shape)\n",
    "    m = phi.shape[0]\n",
    "    J = (-1/m) * (np.sum(np.add( np.multiply(c, np.log(phi)), np.multiply(1-c, np.log(1-phi)) ) + (lambd/2) * np.dot(w.T, w)))\n",
    "    return J\n",
    "cost(np.array([0.99, 0.001, 0.99]), np.array([0.99, 0.001, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_w(alpha, lambd, x, w, y, m, n):    \n",
    "    #We will use mini-batch gradient desecent\n",
    "    assert(x.shape == (m, n))\n",
    "    w = w.reshape(-1, 1)\n",
    "    temp = np.dot(x.T, sigmoid_function(x, w)- y.reshape(-1, 1))\n",
    "    w = w - (alpha/m)*temp - (lambd/(2*m)) * w - ((alpha*lambd)/m) * w \n",
    "    return w\n",
    "x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "w = np.array([-1, 2, 3])\n",
    "w = update_w(0.8, 0.2, x, w, np.array([1, 0, 1]), 3, 3)\n",
    "w.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, decision):\n",
    "    phis = sigmoid_function(X, w.reshape(-1, 1))\n",
    "    decisions =  [ decision_function(phi, decision) for phi in phis]\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(alpha, lambd, decision, num_iterations, mini_batch_size, X, y):\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]#The bias unit was already added to X\n",
    "\n",
    "    permutation = list(numpy.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_y = y[permutation]\n",
    "\n",
    "    number_batches = math.floor(m/mini_batch_size)\n",
    "    mini_batches = []\n",
    "\n",
    "    for k in range(0, number_batches):\n",
    "        mini_x = shuffled_X[(k*mini_batch_size):((k+1)*mini_batch_size), :]\n",
    "        mini_y = shuffled_y[(k*mini_batch_size):((k+1)*mini_batch_size)]\n",
    "        mini_batch = (mini_x, mini_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if (m % mini_batch_size) != 0:\n",
    "        mini_x = shuffled_X[((number_batches*mini_batch_size)):m, :]\n",
    "        mini_y = shuffled_y[((number_batches*mini_batch_size)):m]\n",
    "        mini_batch = (mini_x, mini_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    costs = []\n",
    "    w = np.zeros((n, 1))#initialize w to zeros\n",
    "    for i in range(0, num_iterations):\n",
    "        for batch in mini_batches:\n",
    "            X_t, y_t = batch \n",
    "            #print(X_t.shape)\n",
    "            #print(y_t.shape)\n",
    "            w = update_w(alpha, lambd, X_t, w, y_t, X_t.shape[0], X_t.shape[1])\n",
    "            #print(w.shape)\n",
    "            #print(X_t.shape)\n",
    "            phi = sigmoid_function(X_t, w.reshape(-1, 1))\n",
    "            #print(phi.shape)\n",
    "            #print(y_t.shape)\n",
    "            #print(phi.shape)\n",
    "            #print(phi, '\\n')\n",
    "            cost = cost_function(phi.reshape(-1, 1),  y_t.reshape(-1, 1), w, lambd)\n",
    "        \n",
    "        if (i % 10) == 0:\n",
    "            print(f\"Cost is: {cost}\")\n",
    "            costs.append(cost)\n",
    "\n",
    "    decisions =  predict(X, w, decision)\n",
    "\n",
    "    return (w, decisions, costs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Without Regularization\n",
    "X_train = training_data.to_numpy()\n",
    "w, decisions, costs = model(alpha=1, lambd=0, decision=0.6, num_iterations=100, mini_batch_size=50, X=X_train[:, 0:-1], y=X_train[:, -1])\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(X_train[:, -1], decisions))\n",
    "print(f\"precision:{sklearn.metrics.precision_score(X_train[:, -1], decisions):0.3f}, recall:{sklearn.metrics.recall_score(X_train[:, -1], decisions):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = test_data.to_numpy()\n",
    "decisions = predict(X_test[:, 0:-1], w, 0.6)\n",
    "print(\"Performance on the test set\")\n",
    "print(sklearn.metrics.confusion_matrix(X_test[:, -1], decisions))\n",
    "print(f\"precision:{sklearn.metrics.precision_score(X_test[:, -1], decisions):0.3f}, recall:{sklearn.metrics.recall_score(X_test[:, -1], decisions):0.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#With Regularization\n",
    "X_train = training_data.to_numpy()\n",
    "w, decisions, costs = model(alpha=1, lambd=0.2, decision=0.6, num_iterations=100, mini_batch_size=50, X=X_train[:, 0:-1], y=X_train[:, -1])\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(X_train[:, -1], decisions))\n",
    "print(f\"precision:{sklearn.metrics.precision_score(X_train[:, -1], decisions):0.3f}, recall:{sklearn.metrics.recall_score(X_train[:, -1], decisions):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = test_data.to_numpy()\n",
    "decisions = predict(X_test[:, 0:-1], w, decision=0.6)\n",
    "print(\"Performance on the test set\")\n",
    "print(sklearn.metrics.confusion_matrix(X_test[:, -1], decisions))\n",
    "print(f\"precision:{sklearn.metrics.precision_score(X_test[:, -1], decisions):0.3f}, recall:{sklearn.metrics.recall_score(X_test[:, -1], decisions):0.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 1, chapter 2 and Chapter 4 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 1: (https://www.youtube.com/watch?v=UzxYlbK2c7E)\n",
    "* Andrew Ng, Lec 2: (https://www.youtube.com/watch?v=5u4G23_OohI)\n",
    "* Andrew Ng, Lec 3: (https://www.youtube.com/watch?v=HZ4cvaztQEs)\n",
    "* Andrew Ng, Lec 4: (https://www.youtube.com/watch?v=nLKOQfKLUks)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}