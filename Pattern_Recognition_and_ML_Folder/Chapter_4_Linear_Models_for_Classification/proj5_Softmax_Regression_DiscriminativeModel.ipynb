{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Discriminative Models\n",
    "### Multinomial Distribution\n",
    "Before getting into Softmax regression, we need to define what is a nultinomial distribution. A discrete random variable is said to have a multinomial distribution when it have multiple levels instead of 2 levels as was the case in binomial distribution. This discrete r.v is usually encoded in what is known as a one hot encoder in which this vector would have 1 at the index that correspond to the specified level and the rest of the entries would be zeros. In which each level is represented by a an index in this vector. So, the probability of occurance of a single observation is given by the following $p(\\underline{x}|\\underline{\\mu}=\\prod_{k=1}^{K}\\mu_k^{x_k}$. And to be be valid distribution the normalization axiom should be satisfied, hence, $\\sum_{k=1}^{K}\\mu_k=1$. For a dataset that are coonstructed from N observation is represented as follows:-\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&p(D|x;\\mu)=\\prod_{n=1}^{N}\\prod_{k=1}^K\\mu_k^{x_k}=\\prod_{k=1}^{K}\\prod_{n=1}^{N}\\mu_k^{x_k};\\ by \\ using\\ x^ax^b=x^{a+b}\\\\\n",
    "&p(D|x;\\mu)=\\prod_{k=1}^{K}\\mu_k{\\sum_{n=1}^{N}x_{nk}};\\ let\\ \\sum_{n=1}^{N}x_{nk}=m_k\\\\\n",
    "&p(D|x;\\mu)=\\prod_{k=1}^{K}\\mu_k^{m_k}\\\\\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "In order the best value for the $\\mu^{'s}$, we need to maximize the log-likelihood of the conditional distribution while taking care of the constrain on $||\\mu||_1=1$. This can be done by using lagrangian, which can be derived as follows:-\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&\\nabla_{mu_k}L(\\mu, \\lambda)=\\nabla_{\\mu_k}\\Big( \\sum_{k=1}{K}m_klog(\\mu_k)+\\lambda(\\sum_{i}\\mu_i -1) \\Big)=0\\rightarrow\\mu_k=\\frac{-m_k}{\\lambda}\\\\\n",
    "&\\sum_{k}\\mu_k=\\sum_{k}\\frac{-m_k}{\\lambda}=\\sum_{k}\\frac{-\\sum_{n}x_{nk}}{\\lambda}=1\\rightarrow \\lambda=-N\\\\\n",
    "&\\mu_k=\\frac{\\sum_{n}x_{nk}}{N}\\\\\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "As can be seen from the last equation, it is clearly sufficient to estimate the probability of each class target as the fraction of observation in those classes, hence, the name sufficient statistics that is used for $m_k$\n",
    "\n",
    "Therefore, the best value of each $\\mu$ is described by the proportion of number of observation of each class with respect to total number of observation. The multinomial distribution is discribed by the following equation, in which the multinomial coefficent(N choose the $\\mu^s$) is used for normalization purposes.\n",
    "$$\n",
    "\\begin{align*}\n",
    "Multi(\\underline{x}|N, \\mu^{'s}, m^{'s})=\n",
    "\\begin{pmatrix}\n",
    "&N\\\\\n",
    "&m1,m2,...,mk\n",
    "\\end{pmatrix}\n",
    "\\prod_{k=1}^{K}\\mu_k^{x_k}\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "### Softmax regression\n",
    "As was shown in logistic regression, we want to describe the conditional distribution of the target class given the input. Softmax regression will deal with datasets that have more than one class, in which each observation is descibed by the following formula:- $p(\\underline{t}|x;\\mu_1,...,\\mu_k)=\\prod_{k=1}^{K}\\mu_k^{t_k}$. In which t is hot encoded by 1 at the index that correspond to the class number, in which it act like an indicator function. Due to the constrain of $||\\mu||_1=1$ then knowing the mu's of class 1 to K-1 will be sufficient to know $\\mu_k$, hence, this class will be considered as the reference class and it will have any parameters such was the case for the negative class in logistic regression. As we did for logistic regression, we need to transform the distribution of the conditional distribution to the expoonential family, in order to deal with this probelm in the GLM's sense, and this will end up resulting to what is known as the softmax formula.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&p(t|x;\\eta)=h(t)g(\\eta)exp(\\eta^Tu(t))\\\\\n",
    "&p(\\underline{t}|x;\\eta)=exp(log(p(\\underline{t}|x;\\mu)))=exp(\\sum_{k=1}^{K}t_klog(\\mu_k));\\ by\\ \\sum_{i}\\mu_i=1\\rightarrow t_K = 1- \\sum_{i=1}^{K-1}t_i\\ because\\ at\\ least\\ 1\\ of\\ them\\ is\\ 1\\\\\n",
    "&p(\\underline{t}|x;\\eta)=exp(\\sum_{i=1}^{K-1}t_i\\mu_i + (1 -\\sum_{i=1}^{K-1}t_i)log(\\mu_K));\\ by\\ log(\\frac{a}{b})=log(a)-log(b)\\\\\n",
    "&p(\\underline{t}|x;\\eta)=\\mu_kexp(\\sum_{i=1}^{K-1}log(\\frac{\\mu_i}{\\mu_K}))\\\\\n",
    "\n",
    "&\\eta=\n",
    "\\begin{pmatrix}\n",
    "&log(\\frac{\\mu_1}{\\mu_k})\\\\\n",
    "&...\\\\\n",
    "&log(\\frac{\\mu_{K-1}}{\\mu_k})\n",
    "\\end{pmatrix}\\\\\n",
    "\n",
    "&u(t)=(t_1, t_2,..., t_{K-1})^T\\\\\n",
    "&Canonical\\ Link\\ function \\rightarrow\\ \\eta_i=log(\\frac{\\mu_i}{\\mu_K})=w_i^T\\underline{x}\\\\\n",
    "&\\mu_i=\\mu_Kexp(w_i^Tx);\\ by\\ summing\\ across\\ all\\ \\mu^{'}s\\rightarrow\\ \\mu_k=\\frac{1}{\\sum_{i}^{K}exp(w_i^Tx)}; w_k=\\underline{0}\\\\\n",
    "&\\therefore \\mu_i=\\frac{exp(w_i^Tx)}{1+\\sum_{k=1}^{K-1}exp(w_i^Tx)}\\\\\n",
    "&\\because p(y=1|x;\\eta)=\\mu_1\\ ; E[u(t_1)|x;\\eta]=E[1(t =1)|x;\\eta]=0(p(1(y \\neq 1)|x;\\eta))+p(1(y=1)|x;\\eta))=\\mu_1\\\\\n",
    "&\\therefore E[u(y)|x;\\eta]=(\\mu_1,..,\\mu_{K-1})^T\\\\\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Estimating the Parameters \n",
    "We will use gradient descent to minimize the cross entropy cost function, in which we need to minimize it K-1 times with respect to the parameters of each class.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&\\nabla_{w_i}\\ell(w1, w2, ..., w_{K-1}) = -\\nabla_{w_i}\\sum_{n=1}^{N}\\sum_{k=1}^{K}t_{nk}log(\\frac{exp(\\eta_{nk})}{1+\\sum_{j=1}^{K-1}exp(\\eta_{nj})}); by\\ \\frac{d logx}{dx} =(\\frac{1}{x})*1\\\\\n",
    "\n",
    "&\\nabla_{w_1}\\ell(w_1, w_2, ..., w_{K-1})=-\\sum_{n=1}^{N}t_{n1} \\frac{\\sum_{j}exp(w_j^Tx_n)}{exp(w_1^Tx_n)} \n",
    "\\Big(\n",
    "     \\frac{x\\ exp(w_1^Tx_n)\\sum_{j}exp(w_j^Tx_n)  -  x\\ exp(w_1^Tx_n)exp(w_1^Tx_n)}\n",
    "     {(\\sum_{j}exp(w_j^Tx_n) )^2}\\Big)\n",
    "     +t_{n2} \\big(\n",
    "          \\frac{-x\\ exp(w_2^Tx)exp(w_1^Tx)}\n",
    "          {exp(w_2^tx)\\sum_{j}exp(w_j^Tx)} \n",
    "           \\big) + ... \n",
    " ;\\\\\n",
    "\n",
    "\n",
    "&\\nabla_{w_1}\\ell(w1, w2, ..., w_{K-1})=-\\sum_{n=1}^{N}(x_nt_{n1}(1-\\mu_1)-x_nt_{n2}\\mu_1-x_nt_{n3}\\mu_1...)=-\\sum_{n=1}^{N}x_n(t_{n1} - \\mu_1(t_{n1}+t_{n2} + ...));\\ by\\ the\\ principle\\ of\\ one\\ hot\\ encoded\\\\\n",
    "\n",
    "& \\nabla_{w_1}\\ell(w1, w2, ..., w_{K-1}) =-\\sum_{n=1}^{N}(t_{n1} -\\mu_1)x_n\\\\\n",
    "\n",
    "&w_1^{t+1}=w_1^{t}+\\alpha\\nabla_{w_1^{t}}\\ell(w_1^t, w_2, ..., w_{K-1})\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "As can be seen from the last equation, we arrived to the same exact equation for gradient descent step that was seen in logistic regression and linear regression. \n",
    "\n",
    "### Prediction\n",
    "We will make K prediction in which the class that would be chosen is the class that gave the highest probability.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "&p(y=1|x;\\eta)=\\mu_1=\\frac{exp(w_1^Tx)}{1+\\sum_{k=1}^{K-1}exp(w_k^Tx)}\\\\\n",
    "&...\\\\\n",
    "&p(y=K|x;\\eta)=\\mu_K=1-\\sum_{i=1}^{K-1}\\frac{exp(w_k^Tx)}{1+\\sum_{k=1}^{K-1}exp(w_k^Tx)}; from\\ ||\\mu||_1=1 \\\\\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(112, 5)\n(38, 5)\n"
    }
   ],
   "source": [
    "X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "#X, y = sklearn.datasets.load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "X_train = standard.fit_transform(X_train)\n",
    "training_data = np.c_[X_train, y_train]#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "\n",
    "X_test = standard.transform(X_test)\n",
    "test_data = np.c_[X_test, y_test]\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "X_train, y_train = (training_data[:, 0:-1], training_data[:, -1])\n",
    "X_test, y_test = (test_data[:, 0:-1], test_data[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(object):\n",
    "\n",
    "    def __init__(self, X, y, k, learning_rate, num_iteration):\n",
    "        self.num_iteration = num_iteration\n",
    "        self.learning_rate = learning_rate\n",
    "        self.K = k\n",
    "        self.m = X.shape[0]\n",
    "        self.n = X.shape[1] + 1 #Need to add the bias unit\n",
    "        self.W = {}\n",
    "        self._initialize_parameters(self.n, self.K)\n",
    "        self.X_train = np.c_[np.ones((self.m, 1)), X]\n",
    "        self.y_train = sklearn.preprocessing.OneHotEncoder().fit_transform(y.reshape(-1, 1)).toarray()\n",
    "        \n",
    "    def _initialize_parameters(self, n, K):\n",
    "        for k in range(1, K + 1):\n",
    "            self.W[str(k)] = np.zeros((n, 1)) #We willn't change w[K]\n",
    "\n",
    "    def softmax_function(self, w, x):\n",
    "        assert(w.shape == x.shape)\n",
    "        t1 = np.exp(np.dot(w.T, x))\n",
    "        keys = list(self.W.keys())\n",
    "        t2 = np.sum(list(map(lambda k: np.exp(np.dot(self.W[k].T, x)), keys)))\n",
    "        return np.divide(t1, t2)\n",
    "\n",
    "    #Needs to be made pythonian \n",
    "    def fit(self):\n",
    "        for iter in range(0, self.num_iteration):\n",
    "            for k in range(1, self.K):\n",
    "                for n in range(0, self.m):\n",
    "                    self.W[str(k)] = self.W[str(k)].reshape(-1, 1) + self.learning_rate * np.multiply(self.y_train[n, k-1] - self.softmax_function(self.W[str(k)].reshape(-1, 1), self.X_train[n, :].reshape(-1, 1)) , self.X_train[n, :].reshape(-1, 1))\n",
    "        \n",
    "        return self.W\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.zeros((X.shape[0], self.K))\n",
    "        for k in range(1, self.K):\n",
    "            prediction[:, k-1] = list(map(lambda x: self.softmax_function(self.W[str(k)].reshape(-1, 1), x.reshape(-1, 1)), X))\n",
    "\n",
    "        prediction[:, self.K-1] = 1 - np.sum(prediction[:, 0:self.K-1], axis=1)#sum across the columns\n",
    "        return np.argmax(prediction, axis=1)#across the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the training set\n[[35  0  0]\n [ 0 36  3]\n [ 0  1 37]]\n"
    }
   ],
   "source": [
    "model = SoftmaxRegression(X_train, y_train, 3, 0.1, 100)\n",
    "W = model.fit()\n",
    "pred = model.predict(np.c_[np.ones((X_train.shape[0],1)), X_train])\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, pred))\n",
    "#print(f\"precision:{sklearn.metrics.precision_score(y_train, prediction):0.3f}, recall:{sklearn.metrics.recall_score(y_train, prediction):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the test set\n[[15  0  0]\n [ 0 11  0]\n [ 0  0 12]]\n"
    }
   ],
   "source": [
    "prediction = model.predict( np.c_[np.ones((X_test.shape[0], 1)), X_test] )\n",
    "print(\"Performance on the test set\")\n",
    "print(sklearn.metrics.confusion_matrix(prediction, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Performance on the training set\n[[35  0  0]\n [ 0 38  1]\n [ 0  1 37]]\n"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty='none')\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_train)\n",
    "print(\"Performance on the training set\")\n",
    "print(sklearn.metrics.confusion_matrix(y_train, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 1, chapter 2 and Chapter 4 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 1: (https://www.youtube.com/watch?v=UzxYlbK2c7E)\n",
    "* Andrew Ng, Lec 2: (https://www.youtube.com/watch?v=5u4G23_OohI)\n",
    "* Andrew Ng, Lec 3: (https://www.youtube.com/watch?v=HZ4cvaztQEs)\n",
    "* Andrew Ng, Lec 4: (https://www.youtube.com/watch?v=nLKOQfKLUks)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}