{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis Model\n",
    "### Assumptions about the Latent Variables and Model\n",
    "### Constructing the Joint Distribution of Observed RV and Latent RV\n",
    "### The Incomplete Log-likelihood with discussion about Jensen's Inequality\n",
    "### Thc Complete Log-likelihood\n",
    "#### Q-step\n",
    "#### E-step\n",
    "##### Maximizing w.r.t the mean of the observed input space\n",
    "##### Maximizing w.r.t the loading factor matrix\n",
    "$\\Lambda$\n",
    "##### Maximizing w.r.t to the covariance matrix of the uncertainity in the joint space of observed RV and latent variable \n",
    "$\\psi$\n",
    "#### Convergence Test\n",
    "#### Interpretation of the Loading Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import numpy.random\n",
    "import math\n",
    "import sklearn.metrics\n",
    "import sklearn.decomposition\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(379, 14)\n(127, 14)\n"
    }
   ],
   "source": [
    "X, y = sklearn.datasets.load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "standard = sklearn.preprocessing.StandardScaler()\n",
    "#X_train = standard.fit_transform(X_train)\n",
    "#y_train = standard.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "training_data = np.c_[X_train, y_train]#All of the features are continuous, so, no need to use one-hot encoder and we can directly standard normalize the features of the data set\n",
    "\n",
    "#X_test = standard.transform(X_test)\n",
    "#y_test = standard.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "test_data = np.c_[X_test, y_test]\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorAnalysisModel(object):\n",
    "\n",
    "    def __init__(self, X_train, G, initialization=\"random\"):\n",
    "        self.m = X_train.shape[0]\n",
    "        self.n = X_train.shape[1]\n",
    "        self.G = G\n",
    "        self.X_train = X_train\n",
    "        if initialization == \"random\":\n",
    "            self.mean_x = (1/self.m) * (np.sum(self.X_train, axis=0)).reshape(-1, 1)#sum across all of the observations\n",
    "            self.cov_x = np.cov(self.X_train.T)#I am just making sure it would have full rank, which is the objective of FAM\n",
    "            self.loading_factor = np.zeros((self.n, self.G))\n",
    "            np.fill_diagonal(self.loading_factor, 5)\n",
    "            print(np.linalg.matrix_rank(self.loading_factor))\n",
    "            self.cov_uncertainity = np.eye(self.n) * numpy.random.randn()\n",
    "            self.mean_z_given_x = np.zeros((self.m, self.G))\n",
    "            self.cov_z_given_x = np.eye(self.G) * numpy.random.randn()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def computing_inv_cov_x(self):\n",
    "        #Using Woodbury Identity\n",
    "        inv_psi = np.linalg.inv(self.cov_uncertainity)\n",
    "        t1 = np.linalg.pinv( np.add(np.eye(self.G), np.dot(self.loading_factor.T, np.dot(inv_psi, self.loading_factor))) )\n",
    "        t2 = np.dot(self.loading_factor, np.dot(t1, self.loading_factor.T))\n",
    "        t3 = np.dot(inv_psi, np.dot(t2, inv_psi))\n",
    "        assert(t3.shape == inv_psi.shape)\n",
    "\n",
    "        return np.subtract(inv_psi, t3)\n",
    "        #return np.linalg.inv(np.add(self.cov_uncertainity, np.dot(self.loading_factor, self.loading_factor.T)))\n",
    "\n",
    "    def E_step(self):\n",
    "        #Estimate the covariance and mean of the conditional latent space\n",
    "        temp = np.zeros((self.m, self.G))\n",
    "        for i in range(0, self.m):\n",
    "            temp[i, :] = (np.dot(self.loading_factor.T, np.dot(self.computing_inv_cov_x(), np.subtract(self.X_train[i, :].reshape(-1, 1), self.mean_x.reshape(-1, 1)).reshape(-1, 1)) ) ).reshape(1, -1)\n",
    "\n",
    "        self.mean_z_given_x = temp\n",
    "        self.cov_z_given_x = np.subtract( np.eye(self.G), np.dot( self.loading_factor.T,  np.dot(self.computing_inv_cov_x(), self.loading_factor)  ))\n",
    "        assert(self.cov_z_given_x.shape == (self.G, self.G))\n",
    "    \n",
    "    def M_step(self):\n",
    "        temp_LH = np.zeros((self.n, self.G))\n",
    "        temp_RH = np.zeros((self.G, self.G))\n",
    "\n",
    "        for i in range(0, self.m):\n",
    "            #print((self.X_train[i, :].reshape(-1, 1) - self.mean_x.reshape(-1, 1)).shape)\n",
    "            temp_LH += np.dot((self.X_train[i, :].reshape(-1, 1) - self.mean_x.reshape(-1, 1)).reshape(-1, 1), self.mean_z_given_x[i, :].reshape(1, -1))\n",
    "            #print(temp_LH.shape)\n",
    "            temp_RH += np.add(self.cov_z_given_x, np.dot(self.mean_z_given_x[i, :].reshape(-1, 1), self.mean_z_given_x[i, :].reshape(1, -1)))  \n",
    "\n",
    "        loading_factor_new = np.dot(temp_LH, np.linalg.inv(temp_RH))\n",
    "        #print(loading_factor_new.shape)\n",
    "        #S = np.zeros((self.n, self.n))\n",
    "        phi = np.zeros((self.n, self.n))\n",
    "        for i in range(0, self.m):    \n",
    "            phi += np.dot(loading_factor_new, np.dot(self.mean_z_given_x[i, :].reshape(-1, 1), (self.X_train[i, :].reshape(-1, 1) - self.mean_x.reshape(-1, 1)).reshape(1, -1)))\n",
    "            \n",
    "            \"\"\"phi += ( np.dot(self.X_train[i, :].reshape(-1, 1), self.X_train[i, :].reshape(1, -1))\n",
    "             - np.dot(np.dot(self.X_train[i, :].reshape(-1, 1), self.mean_z_given_x[i, :].reshape(1, -1)), loading_factor_new.T)- np.dot(loading_factor_new, np.dot(self.mean_z_given_x[i, :].reshape(-1, 1), self.X_train[i, :].reshape(1, -1)))\n",
    "               + np.dot(loading_factor_new, np.dot( np.add(self.cov_z_given_x, np.dot(self.mean_z_given_x[i, :].reshape(-1, 1), self.mean_z_given_x[i, :].reshape(1, -1)) ), loading_factor_new.T) ) )\"\"\"\n",
    "            #phi += np.dot(self.X_train[i, :].reshape(-1, 1), self.X_train[i, :].reshape(1, -1)) - np.dot(loading_factor_new, np.dot(self.mean_z_given_x[i, :].reshape(-1, 1), self.X_train[i, :].reshape(1, -1)))\n",
    "\n",
    "            \"\"\"S += np.dot((self.X_train[i, :].reshape(-1, 1) - self.mean_x.reshape(-1, 1)).reshape(-1, 1), (self.X_train[i, :].reshape(-1, 1) - self.mean_x.reshape(-1, 1)).reshape(1, -1))\"\"\"\n",
    "\n",
    "        phi = np.subtract(self.cov_x, (1/self.m) * phi) \n",
    "        temp = np.eye(self.n)\n",
    "        assert(temp.shape == phi.shape)\n",
    "        #self.cov_uncertainity = (1/self.m) * np.multiply(temp, phi)\n",
    "        self.cov_uncertainity =  np.multiply(temp, phi)\n",
    "\n",
    "        self.loading_factor = loading_factor_new\n",
    "\n",
    "    def computing_log_likelihood(self):\n",
    "        temp = 0\n",
    "        for i in range(0, self.m):\n",
    "\n",
    "            temp += np.log( \n",
    "            (1/np.sqrt(np.linalg.det(2*np.pi*self.computing_inv_cov_x()))) *\n",
    "            np.exp(\n",
    "            (-1/2) * np.dot((self.X_train[i, :].reshape(-1, 1) - self.mean_x.reshape(-1, 1)).T, \n",
    "            np.dot(self.computing_inv_cov_x(), self.X_train[i, :].reshape(-1, 1) - self.mean_x.reshape(-1, 1)))\n",
    "            )\n",
    "            )\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def fit(self, max_iterations, eps=1e-3):\n",
    "        convergence_test = True\n",
    "        count = 0 \n",
    "        while((convergence_test == True) and (count < max_iterations)):\n",
    "            self.E_step()#Update the soft latent values            \n",
    "            log_likelihood_t = self.computing_log_likelihood()\n",
    "            self.M_step()#Update the parameters of the condtional distribution of x given z\n",
    "            log_likelihood_t_future = self.computing_log_likelihood()\n",
    "            print(f\"Number of iteration:{count}, max_iteration:{max_iterations}, past:{log_likelihood_t}, future:{log_likelihood_t_future}\")\n",
    "            count = count + 1\n",
    "            if( (log_likelihood_t_future - log_likelihood_t) < eps and (count > 10)):\n",
    "                print(\"We converged to the optimal value for the log-likelihood\")\n",
    "                convergence_test =False #We reached the parameters that maximize the log-likelihood, no adancement in the log-likelihood\n",
    "        self.cov_x = np.add(self.cov_uncertainity, np.dot(self.loading_factor, self.loading_factor.T))#Update covariance matrix of the project input space\n",
    "        return self.mean_x, self.loading_factor, self.cov_uncertainity\n",
    "    \n",
    "    def transform(self, X):\n",
    "        temp = []\n",
    "        print(np.linalg.matrix_rank(self.loading_factor))\n",
    "        assert(np.linalg.matrix_rank(self.loading_factor) == self.G)\n",
    "        projection= np.linalg.inv( np.dot(self.loading_factor.T, self.loading_factor))\n",
    "        assert(projection.shape == (self.G, self.G))\n",
    "        for i in range(0, X.shape[0]):\n",
    "            temp.append(np.dot(projection, np.dot(self.loading_factor.T, (X[i, :].reshape(-1, 1) - self.mean_x).reshape(-1, 1) )))\n",
    "        \n",
    "        return np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4\nNumber of iteration:0, max_iteration:1000, past:[[-inf]], future:[[-3811.48699451]]\nNumber of iteration:1, max_iteration:1000, past:[[-3811.48699451]], future:[[-3528.15078223]]\nNumber of iteration:2, max_iteration:1000, past:[[-3528.15078223]], future:[[-3001.21476933]]\nNumber of iteration:3, max_iteration:1000, past:[[-3001.21476933]], future:[[-2441.33479454]]\nNumber of iteration:4, max_iteration:1000, past:[[-2441.33479454]], future:[[-1959.11467202]]\nNumber of iteration:5, max_iteration:1000, past:[[-1959.11467202]], future:[[-1577.69708787]]\nNumber of iteration:6, max_iteration:1000, past:[[-1577.69708787]], future:[[-1287.1080311]]\nNumber of iteration:7, max_iteration:1000, past:[[-1287.1080311]], future:[[-1065.98576967]]\nNumber of iteration:8, max_iteration:1000, past:[[-1065.98576967]], future:[[-893.15983306]]\nNumber of iteration:9, max_iteration:1000, past:[[-893.15983306]], future:[[-754.14265641]]\nNumber of iteration:10, max_iteration:1000, past:[[-754.14265641]], future:[[-640.38376854]]\nNumber of iteration:11, max_iteration:1000, past:[[-640.38376854]], future:[[-547.56925258]]\nNumber of iteration:12, max_iteration:1000, past:[[-547.56925258]], future:[[-473.48239699]]\nNumber of iteration:13, max_iteration:1000, past:[[-473.48239699]], future:[[-416.19522391]]\nNumber of iteration:14, max_iteration:1000, past:[[-416.19522391]], future:[[-373.82266996]]\nNumber of iteration:15, max_iteration:1000, past:[[-373.82266996]], future:[[-343.8725582]]\nNumber of iteration:16, max_iteration:1000, past:[[-343.8725582]], future:[[-322.90636732]]\nNumber of iteration:17, max_iteration:1000, past:[[-322.90636732]], future:[[-307.07758484]]\nNumber of iteration:18, max_iteration:1000, past:[[-307.07758484]], future:[[-293.20157447]]\nNumber of iteration:19, max_iteration:1000, past:[[-293.20157447]], future:[[-279.63789964]]\nNumber of iteration:20, max_iteration:1000, past:[[-279.63789964]], future:[[-266.13106844]]\nNumber of iteration:21, max_iteration:1000, past:[[-266.13106844]], future:[[-253.04919208]]\nNumber of iteration:22, max_iteration:1000, past:[[-253.04919208]], future:[[-240.80492163]]\nNumber of iteration:23, max_iteration:1000, past:[[-240.80492163]], future:[[-229.64213138]]\nNumber of iteration:24, max_iteration:1000, past:[[-229.64213138]], future:[[-219.6326723]]\nNumber of iteration:25, max_iteration:1000, past:[[-219.6326723]], future:[[-210.73557107]]\nNumber of iteration:26, max_iteration:1000, past:[[-210.73557107]], future:[[-202.85302955]]\nNumber of iteration:27, max_iteration:1000, past:[[-202.85302955]], future:[[-195.86737491]]\nNumber of iteration:28, max_iteration:1000, past:[[-195.86737491]], future:[[-189.66125361]]\nNumber of iteration:29, max_iteration:1000, past:[[-189.66125361]], future:[[-184.12690513]]\nNumber of iteration:30, max_iteration:1000, past:[[-184.12690513]], future:[[-179.16933858]]\nNumber of iteration:31, max_iteration:1000, past:[[-179.16933858]], future:[[-174.70653779]]\nNumber of iteration:32, max_iteration:1000, past:[[-174.70653779]], future:[[-170.66849753]]\nNumber of iteration:33, max_iteration:1000, past:[[-170.66849753]], future:[[-166.99604082]]\nNumber of iteration:34, max_iteration:1000, past:[[-166.99604082]], future:[[-163.63983957]]\nNumber of iteration:35, max_iteration:1000, past:[[-163.63983957]], future:[[-160.55970817]]\nNumber of iteration:36, max_iteration:1000, past:[[-160.55970817]], future:[[-157.72397187]]\nNumber of iteration:37, max_iteration:1000, past:[[-157.72397187]], future:[[-155.10852327]]\nNumber of iteration:38, max_iteration:1000, past:[[-155.10852327]], future:[[-152.69515342]]\nNumber of iteration:39, max_iteration:1000, past:[[-152.69515342]], future:[[-150.46899414]]\nNumber of iteration:40, max_iteration:1000, past:[[-150.46899414]], future:[[-148.41545237]]\nNumber of iteration:41, max_iteration:1000, past:[[-148.41545237]], future:[[-146.51761635]]\nNumber of iteration:42, max_iteration:1000, past:[[-146.51761635]], future:[[-144.75528988]]\nNumber of iteration:43, max_iteration:1000, past:[[-144.75528988]], future:[[-143.10621499]]\nNumber of iteration:44, max_iteration:1000, past:[[-143.10621499]], future:[[-141.54891129]]\nNumber of iteration:45, max_iteration:1000, past:[[-141.54891129]], future:[[-140.06569214]]\nNumber of iteration:46, max_iteration:1000, past:[[-140.06569214]], future:[[-138.64446856]]\nNumber of iteration:47, max_iteration:1000, past:[[-138.64446856]], future:[[-137.27879859]]\nNumber of iteration:48, max_iteration:1000, past:[[-137.27879859]], future:[[-135.96657273]]\nNumber of iteration:49, max_iteration:1000, past:[[-135.96657273]], future:[[-134.70816689]]\nNumber of iteration:50, max_iteration:1000, past:[[-134.70816689]], future:[[-133.50478579]]\nNumber of iteration:51, max_iteration:1000, past:[[-133.50478579]], future:[[-132.35735613]]\nNumber of iteration:52, max_iteration:1000, past:[[-132.35735613]], future:[[-131.26599747]]\nNumber of iteration:53, max_iteration:1000, past:[[-131.26599747]], future:[[-130.22992221]]\nNumber of iteration:54, max_iteration:1000, past:[[-130.22992221]], future:[[-129.24757744]]\nNumber of iteration:55, max_iteration:1000, past:[[-129.24757744]], future:[[-128.31687828]]\nNumber of iteration:56, max_iteration:1000, past:[[-128.31687828]], future:[[-127.43543959]]\nNumber of iteration:57, max_iteration:1000, past:[[-127.43543959]], future:[[-126.60076211]]\nNumber of iteration:58, max_iteration:1000, past:[[-126.60076211]], future:[[-125.8103618]]\nNumber of iteration:59, max_iteration:1000, past:[[-125.8103618]], future:[[-125.06184795]]\nNumber of iteration:60, max_iteration:1000, past:[[-125.06184795]], future:[[-124.3529624]]\nNumber of iteration:61, max_iteration:1000, past:[[-124.3529624]], future:[[-123.68159262]]\nNumber of iteration:62, max_iteration:1000, past:[[-123.68159262]], future:[[-123.04576918]]\nNumber of iteration:63, max_iteration:1000, past:[[-123.04576918]], future:[[-122.44365544]]\nNumber of iteration:64, max_iteration:1000, past:[[-122.44365544]], future:[[-121.87353426]]\nNumber of iteration:65, max_iteration:1000, past:[[-121.87353426]], future:[[-121.33379475]]\nNumber of iteration:66, max_iteration:1000, past:[[-121.33379475]], future:[[-120.8229205]]\nNumber of iteration:67, max_iteration:1000, past:[[-120.8229205]], future:[[-120.33947972]]\nNumber of iteration:68, max_iteration:1000, past:[[-120.33947972]], future:[[-119.88211736]]\nNumber of iteration:69, max_iteration:1000, past:[[-119.88211736]], future:[[-119.44954897]]\nNumber of iteration:70, max_iteration:1000, past:[[-119.44954897]], future:[[-119.04055595]]\nNumber of iteration:71, max_iteration:1000, past:[[-119.04055595]], future:[[-118.65398178]]\nNumber of iteration:72, max_iteration:1000, past:[[-118.65398178]], future:[[-118.28872911]]\nNumber of iteration:73, max_iteration:1000, past:[[-118.28872911]], future:[[-117.94375726]]\nNumber of iteration:74, max_iteration:1000, past:[[-117.94375726]], future:[[-117.61808023]]\nNumber of iteration:75, max_iteration:1000, past:[[-117.61808023]], future:[[-117.31076478]]\nNumber of iteration:76, max_iteration:1000, past:[[-117.31076478]], future:[[-117.02092887]]\nNumber of iteration:77, max_iteration:1000, past:[[-117.02092887]], future:[[-116.74774014]]\nNumber of iteration:78, max_iteration:1000, past:[[-116.74774014]], future:[[-116.49041449]]\nNumber of iteration:79, max_iteration:1000, past:[[-116.49041449]], future:[[-116.24821485]]\nNumber of iteration:80, max_iteration:1000, past:[[-116.24821485]], future:[[-116.02045005]]\nNumber of iteration:81, max_iteration:1000, past:[[-116.02045005]], future:[[-115.80647385]]\nNumber of iteration:82, max_iteration:1000, past:[[-115.80647385]], future:[[-115.60568413]]\nNumber of iteration:83, max_iteration:1000, past:[[-115.60568413]], future:[[-115.41752232]]\nNumber of iteration:84, max_iteration:1000, past:[[-115.41752232]], future:[[-115.24147308]]\nNumber of iteration:85, max_iteration:1000, past:[[-115.24147308]], future:[[-115.07706425]]\nNumber of iteration:86, max_iteration:1000, past:[[-115.07706425]], future:[[-114.92386719]]\nNumber of iteration:87, max_iteration:1000, past:[[-114.92386719]], future:[[-114.78149752]]\nNumber of iteration:88, max_iteration:1000, past:[[-114.78149752]], future:[[-114.64961637]]\nNumber of iteration:89, max_iteration:1000, past:[[-114.64961637]], future:[[-114.52793222]]\nNumber of iteration:90, max_iteration:1000, past:[[-114.52793222]], future:[[-114.41620349]]\nNumber of iteration:91, max_iteration:1000, past:[[-114.41620349]], future:[[-114.31424204]]\nNumber of iteration:92, max_iteration:1000, past:[[-114.31424204]], future:[[-114.2219177]]\nNumber of iteration:93, max_iteration:1000, past:[[-114.2219177]], future:[[-114.13916424]]\nNumber of iteration:94, max_iteration:1000, past:[[-114.13916424]], future:[[-114.06598701]]\nNumber of iteration:95, max_iteration:1000, past:[[-114.06598701]], future:[[-114.00247267]]\nNumber of iteration:96, max_iteration:1000, past:[[-114.00247267]], future:[[-113.94880169]]\nNumber of iteration:97, max_iteration:1000, past:[[-113.94880169]], future:[[-113.90526428]]\nNumber of iteration:98, max_iteration:1000, past:[[-113.90526428]], future:[[-113.87228066]]\nNumber of iteration:99, max_iteration:1000, past:[[-113.87228066]], future:[[-113.85042708]]\nNumber of iteration:100, max_iteration:1000, past:[[-113.85042708]], future:[[-113.84046907]]\nNumber of iteration:101, max_iteration:1000, past:[[-113.84046907]], future:[[-113.84340423]]\nWe converged to the optimal value for the log-likelihood\n"
    }
   ],
   "source": [
    "###############################################\n",
    "#There is a good chance that there is something wrong in the results, need to discover what is the issue in my code. I suspect there is something wrong in my initialization phase.\n",
    "###############################################\n",
    "q =4\n",
    "model = FactorAnalysisModel(X_train, G=q, initialization=\"random\")\n",
    "mean_x, loading_factor, cov_uncertainity = model.fit(max_iterations=1000)\n",
    "#loading_factor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-1.75843591],\n       [ 5.68960239],\n       [ 0.56836204],\n       [ 3.6240142 ]])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "orthogonal_projection_data_onto_qSubspace = model.transform(X_train)\n",
    "orthogonal_projection_data_onto_qSubspace[10, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            unobs_var_0  unobs_var_1  unobs_var_2  unobs_var_3\nobs_var_0      6.667845    -0.248543     2.159935    -0.088554\nobs_var_1     -2.647139    14.966404    -9.586004    -2.619961\nobs_var_2      1.478685    -2.181959     5.319660     0.517780\nobs_var_3     -0.023463     0.011112     0.034200     0.046844\nobs_var_4      0.028085    -0.027067     0.093971     0.038972\nobs_var_5     -0.096975     0.249967    -0.182682     0.093385\nobs_var_6      5.986790   -12.719391    16.082667     8.429384\nobs_var_7     -0.497238     0.918836    -1.377466    -0.707042\nobs_var_8      4.725736     0.543958     5.996970    -2.475967\nobs_var_9     76.872313    12.257698   134.049849   -44.607113\nobs_var_10     0.598526    -1.102851     0.552408    -1.426548\nobs_var_11   -22.250209    -0.639257   -31.692287     1.055153\nobs_var_12     2.272330    -2.615530     3.448841     0.117806",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unobs_var_0</th>\n      <th>unobs_var_1</th>\n      <th>unobs_var_2</th>\n      <th>unobs_var_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>obs_var_0</th>\n      <td>6.667845</td>\n      <td>-0.248543</td>\n      <td>2.159935</td>\n      <td>-0.088554</td>\n    </tr>\n    <tr>\n      <th>obs_var_1</th>\n      <td>-2.647139</td>\n      <td>14.966404</td>\n      <td>-9.586004</td>\n      <td>-2.619961</td>\n    </tr>\n    <tr>\n      <th>obs_var_2</th>\n      <td>1.478685</td>\n      <td>-2.181959</td>\n      <td>5.319660</td>\n      <td>0.517780</td>\n    </tr>\n    <tr>\n      <th>obs_var_3</th>\n      <td>-0.023463</td>\n      <td>0.011112</td>\n      <td>0.034200</td>\n      <td>0.046844</td>\n    </tr>\n    <tr>\n      <th>obs_var_4</th>\n      <td>0.028085</td>\n      <td>-0.027067</td>\n      <td>0.093971</td>\n      <td>0.038972</td>\n    </tr>\n    <tr>\n      <th>obs_var_5</th>\n      <td>-0.096975</td>\n      <td>0.249967</td>\n      <td>-0.182682</td>\n      <td>0.093385</td>\n    </tr>\n    <tr>\n      <th>obs_var_6</th>\n      <td>5.986790</td>\n      <td>-12.719391</td>\n      <td>16.082667</td>\n      <td>8.429384</td>\n    </tr>\n    <tr>\n      <th>obs_var_7</th>\n      <td>-0.497238</td>\n      <td>0.918836</td>\n      <td>-1.377466</td>\n      <td>-0.707042</td>\n    </tr>\n    <tr>\n      <th>obs_var_8</th>\n      <td>4.725736</td>\n      <td>0.543958</td>\n      <td>5.996970</td>\n      <td>-2.475967</td>\n    </tr>\n    <tr>\n      <th>obs_var_9</th>\n      <td>76.872313</td>\n      <td>12.257698</td>\n      <td>134.049849</td>\n      <td>-44.607113</td>\n    </tr>\n    <tr>\n      <th>obs_var_10</th>\n      <td>0.598526</td>\n      <td>-1.102851</td>\n      <td>0.552408</td>\n      <td>-1.426548</td>\n    </tr>\n    <tr>\n      <th>obs_var_11</th>\n      <td>-22.250209</td>\n      <td>-0.639257</td>\n      <td>-31.692287</td>\n      <td>1.055153</td>\n    </tr>\n    <tr>\n      <th>obs_var_12</th>\n      <td>2.272330</td>\n      <td>-2.615530</td>\n      <td>3.448841</td>\n      <td>0.117806</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "empty_dictionary = {}\n",
    "for i in range(0, q):\n",
    "    empty_dictionary[\"unobs_var_\" + str(i)] = loading_factor[:, i].tolist()\n",
    "pd.DataFrame(data=empty_dictionary, index=[\"obs_var_\"+str(i) for i in range(0, X_train.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-1.23236617  0.30758369 -1.43656498 -0.70399144]\n[4.48235828e+01 1.11846806e+00 1.72547118e+01 6.97198149e-02\n 4.78947510e-03 4.52447706e-01 1.46635416e+00 1.37973917e+00\n 1.24118103e+01 1.07692721e+00 3.62546505e+00 1.00394704e+00\n 3.02486304e+01]\n"
    }
   ],
   "source": [
    "model = sklearn.decomposition.FactorAnalysis(n_components=4, random_state=0)\n",
    "model.fit(X_train)\n",
    "loading_factors = model.components_\n",
    "loglike = model.loglike_\n",
    "mean = model.mean_\n",
    "transformed = model.transform(X_train)\n",
    "print(transformed[0, :])\n",
    "print(model.noise_variance_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             0          1          2          3\n0     4.882882  -0.166284  -0.365222  -0.335824\n1    -7.309185  -0.301091  19.282264 -10.269288\n2     4.844053  -0.198577  -2.729536  -0.113635\n3     0.000272  -0.011576  -0.023317  -0.016270\n4     0.078058   0.000937  -0.054616  -0.016891\n5    -0.205509   0.029813   0.126979  -0.073904\n6    13.000962  -0.133949 -21.088251 -12.256100\n7    -1.120167   0.003131   1.344535   0.095691\n8     7.784843  -0.783732  -0.081635   0.388655\n9   164.299462 -22.455679   1.897111   0.265951\n10    1.010751  -0.216249  -0.395810   0.511329\n11  -46.471126 -74.008198  -0.580302  -0.023423\n12    3.743085   0.274917  -2.437573  -0.823448",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.882882</td>\n      <td>-0.166284</td>\n      <td>-0.365222</td>\n      <td>-0.335824</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-7.309185</td>\n      <td>-0.301091</td>\n      <td>19.282264</td>\n      <td>-10.269288</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.844053</td>\n      <td>-0.198577</td>\n      <td>-2.729536</td>\n      <td>-0.113635</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000272</td>\n      <td>-0.011576</td>\n      <td>-0.023317</td>\n      <td>-0.016270</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.078058</td>\n      <td>0.000937</td>\n      <td>-0.054616</td>\n      <td>-0.016891</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.205509</td>\n      <td>0.029813</td>\n      <td>0.126979</td>\n      <td>-0.073904</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>13.000962</td>\n      <td>-0.133949</td>\n      <td>-21.088251</td>\n      <td>-12.256100</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-1.120167</td>\n      <td>0.003131</td>\n      <td>1.344535</td>\n      <td>0.095691</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7.784843</td>\n      <td>-0.783732</td>\n      <td>-0.081635</td>\n      <td>0.388655</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>164.299462</td>\n      <td>-22.455679</td>\n      <td>1.897111</td>\n      <td>0.265951</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1.010751</td>\n      <td>-0.216249</td>\n      <td>-0.395810</td>\n      <td>0.511329</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-46.471126</td>\n      <td>-74.008198</td>\n      <td>-0.580302</td>\n      <td>-0.023423</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>3.743085</td>\n      <td>0.274917</td>\n      <td>-2.437573</td>\n      <td>-0.823448</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "pd.DataFrame(loading_factors.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(4, 13)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-1.23291283,  0.30700279, -1.40285944, -0.68994645])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "temp = np.dot( np.linalg.inv(np.dot(loading_factors, loading_factors.T)), loading_factors)\n",
    "print(temp.shape)\n",
    "np.dot(temp, X_train[0,:] -mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[-31992.37941942195,\n -13931.879036722328,\n -13931.774530415927,\n -13931.7732158142]"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "* Chapter 2 and Chapter 12 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n",
    "* Andrew Ng, Lec 13: (https://www.youtube.com/watch?v=LBtuYU-HfUg)\n",
    "* Andrew Ng, Lec 14: (https://www.youtube.com/watch?v=ey2PE5xi9-A)\n",
    "* McNicholas, P.D. (2016). Mixture Model-Based Classification. Boca Raton: Chapman &\n",
    "Hall/CRC Press.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bitmyenv64venv0776e80e1d964a309141464fb4ff9d0d",
   "display_name": "Python 3.8.0 64-bit ('my_env64': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}